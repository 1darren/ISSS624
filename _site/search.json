[
  {
    "objectID": "Hands-on_Ex03/Hands-on_Ex03.html",
    "href": "Hands-on_Ex03/Hands-on_Ex03.html",
    "title": "Hands-on Exercise 3: Processing & Visualizing Flow Data",
    "section": "",
    "text": "On this page, I address Hands-On Exercise for Lesson 03:"
  },
  {
    "objectID": "Hands-on_Ex03/Hands-on_Ex03.html#import-packages",
    "href": "Hands-on_Ex03/Hands-on_Ex03.html#import-packages",
    "title": "Hands-on Exercise 3: Processing & Visualizing Flow Data",
    "section": "1.2 Import packages",
    "text": "1.2 Import packages\n\n\nshow code\npacman::p_load(tmap, sf, DT, stplanr,\n               performance,\n               ggpubr, tidyverse)\n\n\n\nsf, tidyverse, tmap from previous\nNEW:\n\nDT -\nstplanar -\nperformance -\nggpubr -"
  },
  {
    "objectID": "Hands-on_Ex03/Hands-on_Ex03.html#preparing-flow-data",
    "href": "Hands-on_Ex03/Hands-on_Ex03.html#preparing-flow-data",
    "title": "Hands-on Exercise 3: Processing & Visualizing Flow Data",
    "section": "1.3 Preparing Flow Data",
    "text": "1.3 Preparing Flow Data\n\n1.3.1 Importing O/D Data\n\n/data/geospatial/Hunan.###: This csv is “Passenger Volume by Origin Destination Bus Stops”, via LTA Datamall\n\nConvert ORIGIN_PT_CODE and DESTINATION_PT_CODE from numeric to character datatype\n\n\n\n\nshow code\nodbus &lt;- read_csv(\"data/aspatial/origin_destination_bus_202310.csv\")\n\n\nRows: 5694297 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): YEAR_MONTH, DAY_TYPE, PT_TYPE, ORIGIN_PT_CODE, DESTINATION_PT_CODE\ndbl (2): TIME_PER_HOUR, TOTAL_TRIPS\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nshow code\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE) \n\nglimpse(odbus)\n\n\nRows: 5,694,297\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2023-10\", \"2023-10\", \"2023-10\", \"2023-10\", \"2023-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKENDS/HOLIDAY\", \"WEEKDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;dbl&gt; 16, 16, 14, 14, 17, 17, 17, 7, 14, 14, 10, 20, 20,…\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      &lt;fct&gt; 04168, 04168, 80119, 80119, 44069, 20281, 20281, 1…\n$ DESTINATION_PT_CODE &lt;fct&gt; 10051, 10051, 90079, 90079, 17229, 20141, 20141, 1…\n$ TOTAL_TRIPS         &lt;dbl&gt; 3, 5, 3, 5, 4, 1, 24, 2, 1, 7, 3, 2, 5, 1, 1, 1, 1…\n\n\n\n\n1.3.2 Extracting study data (Weekday Morning Peak)\n\nuse filter() to select rows by:\n\nfilter for \"WEEKDAY\" using $DAY_TYPE column\nfilter for tap-on between 0600 and 0900 using $TIME_PER_HOUR column\n\ngroup_by identifies O/D flows by bus stop codes\n\nsummarise() aggregates $TOTAL_TRIPS into $TRIPS\n\nwrite_rds() as R data object for future use\n\nto load, read_rds()`\n\n\n\n\nshow code\nodbus6_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE,\n           DESTINATION_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\n`summarise()` has grouped output by 'ORIGIN_PT_CODE'. You can override using\nthe `.groups` argument.\n\n\nshow code\n## save for future use:\nwrite_rds(odbus6_9, \"data/rds/odbus6_9.rds\")\ndatatable(head(odbus6_9, 5))"
  },
  {
    "objectID": "Hands-on_Ex03/Hands-on_Ex03.html#working-with-geospatial-data",
    "href": "Hands-on_Ex03/Hands-on_Ex03.html#working-with-geospatial-data",
    "title": "Hands-on Exercise 3: Processing & Visualizing Flow Data",
    "section": "1.4 Working with Geospatial Data",
    "text": "1.4 Working with Geospatial Data\n\n1.4.1 Import geospatial data\n\nuse st_read() to import BusStop & MPSZ-2019 as sf dataframe;\n\nst_transform() to transform projection to SVY21 / crs3414\n\n\n\n\nshow code\nbusstop &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"BusStop\") %&gt;%\n  st_transform(crs = 3414)\n\n\nReading layer `BusStop' from data source \n  `C:\\1darren\\ISSS624\\Hands-on_Ex03\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\nshow code\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\n\nReading layer `MPSZ-2019' from data source \n  `C:\\1darren\\ISSS624\\Hands-on_Ex03\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\npreview mpsz:\n\n\n\nshow code\nmpsz\n\n\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n                 SUBZONE_N SUBZONE_C       PLN_AREA_N PLN_AREA_C       REGION_N\n1              MARINA EAST    MESZ01      MARINA EAST         ME CENTRAL REGION\n2         INSTITUTION HILL    RVSZ05     RIVER VALLEY         RV CENTRAL REGION\n3           ROBERTSON QUAY    SRSZ01  SINGAPORE RIVER         SR CENTRAL REGION\n4  JURONG ISLAND AND BUKOM    WISZ01  WESTERN ISLANDS         WI    WEST REGION\n5             FORT CANNING    MUSZ02           MUSEUM         MU CENTRAL REGION\n6         MARINA EAST (MP)    MPSZ05    MARINE PARADE         MP CENTRAL REGION\n7                   SUDONG    WISZ03  WESTERN ISLANDS         WI    WEST REGION\n8                  SEMAKAU    WISZ02  WESTERN ISLANDS         WI    WEST REGION\n9           SOUTHERN GROUP    SISZ02 SOUTHERN ISLANDS         SI CENTRAL REGION\n10                 SENTOSA    SISZ01 SOUTHERN ISLANDS         SI CENTRAL REGION\n   REGION_C                       geometry\n1        CR MULTIPOLYGON (((33222.98 29...\n2        CR MULTIPOLYGON (((28481.45 30...\n3        CR MULTIPOLYGON (((28087.34 30...\n4        WR MULTIPOLYGON (((14557.7 304...\n5        CR MULTIPOLYGON (((29542.53 31...\n6        CR MULTIPOLYGON (((35279.55 30...\n7        WR MULTIPOLYGON (((15772.59 21...\n8        WR MULTIPOLYGON (((19843.41 21...\n9        CR MULTIPOLYGON (((30870.53 22...\n10       CR MULTIPOLYGON (((26879.04 26..."
  },
  {
    "objectID": "Hands-on_Ex03/Hands-on_Ex03.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex03/Hands-on_Ex03.html#geospatial-data-wrangling",
    "title": "Hands-on Exercise 3: Processing & Visualizing Flow Data",
    "section": "1.5. Geospatial Data Wrangling",
    "text": "1.5. Geospatial Data Wrangling\n\nst_intersection() to find find overlap of point (e.g. busstop) and polygon (e.g. mpsz) shapes, creating an sf object output\n\nas part of this, non-SG bus stops are dropped as they fall outside SG/mpsz boundary\nuse select() to only retain two columns, $BUS_STOP_N and $SUBZONE_C\nst_drop_geometry() keeps it a simple dataframe instead of geom sf\n\ndatatable() seems much more useful than kable()\n\n\n\nshow code\nbusstop_mpsz &lt;- st_intersection(busstop, mpsz) %&gt;%\n  select(BUS_STOP_N, SUBZONE_C) %&gt;%\n  st_drop_geometry()\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\nshow code\nwrite_rds(busstop_mpsz, \"data/rds/busstop_mpsz.rds\")  \ndatatable(busstop_mpsz)\n\n\n\n\n\n\n\n\nnow we combine planning subzone code to O/D dataset\n\nthis specifically creates the $ORIGIN_SZ column, we’ll need to repeat this for the destination subzone col later\n\n\n\n\nshow code\nod_data = left_join(odbus6_9 , busstop_mpsz,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         ORIGIN_SZ = SUBZONE_C,\n         DESTIN_BS = DESTINATION_PT_CODE)\n\n\nWarning in left_join(odbus6_9, busstop_mpsz, by = c(ORIGIN_PT_CODE = \"BUS_STOP_N\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 25632 of `x` matches multiple rows in `y`.\nℹ Row 673 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\nWe received warning:\n\nWarning: Detected an unexpected many-to-many relationship between `x` and `y`.\n\nFor ‘x’, makes sense, as we have multiple repeats of ORIGIN_PT_CODE (with different DESTINATION_PT_CODE pairs)\nFor ‘y’, this is not surprising; we saw duplicated bus stops in busstop dataset.\n\n\n\nshow code\nduplicate &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\ncat(\"Before cleaning, nrows of duplicate: \", paste0(nrow(duplicate)))\n\n\nBefore cleaning, nrows of duplicate:  1186\n\n\nshow code\nod_data &lt;- unique(od_data)\n\nduplicate2 &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\ncat(\"\\nAfter cleaning, nrows of duplicate: \", paste0(nrow(duplicate2)))\n\n\n\nAfter cleaning, nrows of duplicate:  0\n\n\n\non this second step, we create $DESTIN_SZ column, repeating from the $ORIGIN_SZ step earlier\n\n\n\nshow code\nod_data &lt;- left_join(od_data , busstop_mpsz,\n            by = c(\"DESTIN_BS\" = \"BUS_STOP_N\")) \n\n\nWarning in left_join(od_data, busstop_mpsz, by = c(DESTIN_BS = \"BUS_STOP_N\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 167 of `x` matches multiple rows in `y`.\nℹ Row 672 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\nshow code\nduplicate &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\n\ncat(\"\\nBefore cleaning, nrows of duplicate: \", paste0(nrow(duplicate)))\n\n\n\nBefore cleaning, nrows of duplicate:  1350\n\n\nshow code\nod_data &lt;- unique(od_data)\n\n\n\nduplicate2 &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\ncat(\"\\nAfter cleaning, nrows of duplicate: \", paste0(nrow(duplicate2)), \"\\n\")\n\n\n\nAfter cleaning, nrows of duplicate:  0 \n\n\nshow code\nod_data &lt;- od_data %&gt;%\n  rename(DESTIN_SZ = SUBZONE_C) %&gt;%\n  drop_na() %&gt;%\n  group_by(ORIGIN_SZ, DESTIN_SZ) %&gt;%\n  summarise(MORNING_PEAK = sum(TRIPS))\n\n\n`summarise()` has grouped output by 'ORIGIN_SZ'. You can override using the\n`.groups` argument.\n\n\nshow code\nwrite_rds(od_data, \"data/rds/od_data.rds\")\n# uncomment to load od_data\n# od_data &lt;- read_rds(\"data/rds/od_data.rds\")\n\n\n\nMore repeated destination bus stops (1350) than origin bus stops (1186)"
  },
  {
    "objectID": "Hands-on_Ex03/Hands-on_Ex03.html#visualizing-spatial-interaction",
    "href": "Hands-on_Ex03/Hands-on_Ex03.html#visualizing-spatial-interaction",
    "title": "Hands-on Exercise 3: Processing & Visualizing Flow Data",
    "section": "1.6 Visualizing Spatial Interaction",
    "text": "1.6 Visualizing Spatial Interaction\n\nGoal is to visualize inter-mpsz zonal flows\n\n\n1.6.1 Removing intra-zonal flows\n\n\nshow code\nod_data1 &lt;- od_data[od_data$ORIGIN_SZ!=od_data$DESTIN_SZ,]\nglimpse(od_data1)\n\n\nRows: 20,787\nColumns: 3\nGroups: ORIGIN_SZ [310]\n$ ORIGIN_SZ    &lt;chr&gt; \"AMSZ01\", \"AMSZ01\", \"AMSZ01\", \"AMSZ01\", \"AMSZ01\", \"AMSZ01…\n$ DESTIN_SZ    &lt;chr&gt; \"AMSZ02\", \"AMSZ03\", \"AMSZ04\", \"AMSZ05\", \"AMSZ06\", \"AMSZ07…\n$ MORNING_PEAK &lt;dbl&gt; 10591, 14980, 3106, 7734, 2306, 1824, 2734, 2300, 164, 93…\n\n\n\n\n1.6.2 Creating desire lines\n\nAptly named “od2line” function for converting od to line object\n\nfrom documentation, first two columns of flow dataframe needs to correspond to first column of zones dataframe\n\n\n\n\nshow code\nflowLine &lt;- od2line(flow = od_data1, \n                    zones = mpsz,\n                    zone_code = \"SUBZONE_C\")\n\n\nCreating centroids representing desire line start and end points.\n\n\n\n\n1.6.3 Visualizing desire lines\n\nuse tm_shape() + tm_lines to create flow lines:\n\nCOMMENTED OUT: This takes too long to run, and results in a black blob of lines that are hard to read. 1000+ lines are drawn to be rendered, which is\n\n\n\n\nshow code\n# tm_shape(mpsz) +\n#   tm_polygons() +\n# flowLine %&gt;%  \n# tm_shape() +\n#   tm_lines(lwd = \"MORNING_PEAK\",\n#            style = \"quantile\",\n#            scale = c(0.1, 1, 3, 5, 7, 10),\n#            n = 6,\n#            alpha = 0.3)\n\n\n\nVisualizing only flows with trips &gt; 5000\nI increased the alpha to 0.5 for greater visibility\n\n\n\nshow code\ntm_shape(mpsz) +\n  tm_polygons() +\nflowLine %&gt;%  \n  filter(MORNING_PEAK &gt;= 5000) %&gt;%\ntm_shape() +\n  tm_lines(lwd = \"MORNING_PEAK\",\n           style = \"quantile\",\n           # scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.5)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "",
    "text": "This function calls pacman to load sf, tidyverse, tmap packages; - tmap : For thematic mapping - sf : for geospatial data handling - tidyverse : for non-spatial data handling\n\npacman::p_load(tmap, sf, tidyverse)\n\n\n\n\n\n\n\nAdditional Notes\n\n\n\n\n\n\nIn the ‘old days’, the library-import would have been written thus:\n\nlibrary(tmap)\nlibrary(sf)\nlibrary(tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#getting-started---import-packages",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#getting-started---import-packages",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "",
    "text": "This function calls pacman to load sf, tidyverse, tmap packages; - tmap : For thematic mapping - sf : for geospatial data handling - tidyverse : for non-spatial data handling\n\npacman::p_load(tmap, sf, tidyverse)\n\n\n\n\n\n\n\nAdditional Notes\n\n\n\n\n\n\nIn the ‘old days’, the library-import would have been written thus:\n\nlibrary(tmap)\nlibrary(sf)\nlibrary(tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#preparing-the-flow-data",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#preparing-the-flow-data",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "Preparing the Flow Data",
    "text": "Preparing the Flow Data\n\nImporting the Origin/Destination (O/D) Data\nFirstly, we will import the Passenger volume by Origin/Destination Bus Stops dataset downloaded by LTA Datamall by using read_csv() of readr package.\n\n\nShow the code\nodbus &lt;- read_csv(\"data/aspatial/origin_destination_bus_202308.csv\")\n\n\n\n\n\n\n\n\nLearneR notes\n\n\n\n\n\n\nNB: Here, we use relative paths, which is helpful for Rstudio, HTML rendering etc\n\nThis works because current file (“In-class_Ex1.qmd”) is in the same subdirectory level as /data file\nIn the “Olde Style”, we would specify data directories\n\nIn “Environment” window, clicking white triangle beside ODBUS gives an .info() of the odbus dataframe; easier than using str (structure) command\n\nCan also open folder in R to review for quick inspection;\nRemember to close after use to avoid\n\n\n\n\n\n\n\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE)\n\n\n\n\n\n\n\nLearneR notes\n\n\n\n\n\n\nCommand changes two columns into factor datatype; categorical variable stored as integer\n\n\n\n\n\norigtrip_7_9 &lt;- odbus %&gt;% \n  filter(DAY_TYPE == \"WEEKDAY\")  %&gt;% \n  filter(TIME_PER_HOUR &gt;= 7 & TIME_PER_HOUR &lt;= 9)  %&gt;% \n  group_by(ORIGIN_PT_CODE)  %&gt;% \n  summarise(TRIPS = sum(TOTAL_TRIPS))\norigtrip_7_9\n\n# A tibble: 5,015 × 2\n   ORIGIN_PT_CODE TRIPS\n   &lt;fct&gt;          &lt;dbl&gt;\n 1 01012           1617\n 2 01013            813\n 3 01019           1620\n 4 01029           2383\n 5 01039           2727\n 6 01059           1415\n 7 01109            115\n 8 01112           7675\n 9 01113           8074\n10 01119           3913\n# ℹ 5,005 more rows\n\n\n\n\n\n\n\n\nLearneR notes\n\n\n\n\n\n\n%&gt;% Pipe function; chains together several operations in sequence to allow for multi-filter\n\nCode filters from odbus to …\n\n\nFilter by WEEKDAY\nTIME_PER_HOUR between 7 to 9 inclusive (eg 0700 to 0959)\nCreates a table, grouped by ORIGIN_PT_CODE…\n…with a fourth summary column, TRIPS, created as a sum of TOTAL_TRIPS"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#extracting-the-study-data",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#extracting-the-study-data",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "Extracting the Study Data",
    "text": "Extracting the Study Data\n\nbusstop &lt;- st_read(dsn = \"data/geospatial\", \n                   layer = \"BusStop\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `BusStop' from data source \n  `C:\\1darren\\ISSS624\\In-class_Ex\\In-class_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\1darren\\ISSS624\\In-class_Ex\\In-class_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\nBUSSTOP$GEOMETRY: Binary Large Object Field; defines coordinate for each bus-stop\n\nTypical cell of dataframe is either INT or CHAR\nBLOF allows to store a list; POINT(lat, long)\n\nMPSZ$GEOMETRYis instead a multipolygon; a list of 300 x 2 coordinates\nIMPORTANT: There are two different coordinates system;\n\nMost GML, KML, data.gov are in wgs84, decimal degree data format\nWe should change to svy21\nCRS: coordinate system of Singapore; cahnge to metres\n\nNow we can join odbus (bus stop code) and busstop (coordinates) and also master plan subzone (mpsz) ## Working with Geospatial Data"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#importing-geospatial-data",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#importing-geospatial-data",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "Importing Geospatial Data",
    "text": "Importing Geospatial Data"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#geospatial-data-wrangling",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "Geospatial Data Wrangling",
    "text": "Geospatial Data Wrangling"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#combining-bus-stop-mpsz",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#combining-bus-stop-mpsz",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "Combining Bus Stop & MPSZ",
    "text": "Combining Bus Stop & MPSZ"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#visualizing-the-geospatial-data",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#visualizing-the-geospatial-data",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "Visualizing the Geospatial Data",
    "text": "Visualizing the Geospatial Data"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#viewing-the-subzone-spatial-file",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#viewing-the-subzone-spatial-file",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "Viewing the Subzone Spatial File",
    "text": "Viewing the Subzone Spatial File"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#personal-notes",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#personal-notes",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "Personal Notes",
    "text": "Personal Notes\n\nODBus Structure\n\n7 columns;\ntime_per_hour: hour of travel\norigin_pt_code, destination_pt_code: per bus-trip; change-bus counted as 2 trips\ntotal_trips: number of passengers moving between startpoint/endpoint, during that hour\nIssue: no explicit location, only bus stop code; can we reconcile bus-stop-code with geospatial data?\nyes:"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_GLSA.html",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_GLSA.html",
    "title": "In-class Exercise 2: Spatial",
    "section": "",
    "text": "This function calls pacman to load sf, tidyverse, tmap, knitr packages;\n\ntmap : For thematic mapping; powerful mapping package\nsf : for geospatial data handling, but also geoprocessing: buffer, point-in-polygon count, etc\n\nbatch processing over GIS packages; can handle tibble format\n\nsfdep : creates space-time cube, EHSA; replaces spdep\ntidyverse : for non-spatial data handling; commonly used R package\nknitr : generates html table\n\n\npacman::p_load(tmap, sf, sfdep, tidyverse, knitr)\n\n\n\n\n\nHunan: geospatial dataset in ESRI shapefile format\n\nuse of st_read() to import assf data.frame\n\n$geometry column is actually a list inside the df cell; that’s the power of the tibble dataframe\n“features” of simple features refers to geometric features eg point line curve etc\n\nnote projection is WGS84; see `88\n\nhunan2012: attribute format in csv format\n\nuse of read_csv() astbl_df data.frame\n\n!IMPORTANT! to retain geometry, you must left join to the sf dataframe (eg you can also hunan2012 right join hunan)\n\nwithout sf dataframe, normal tibble dataframe will drop the geometry column\n\n\n\n\nshow code\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `C:\\1darren\\ISSS624\\In-class_Ex\\In-class_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nshow code\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\nhunan_GDPPC &lt;- left_join(hunan,hunan2012)%&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\n\nshow code\n#qtm(hunan, \"GDPPC\") +\n#  tm_layout(main.title = \"GDPPC\", main.title.position = \"right\")\n\n\ntm_shape(hunan_GDPPC) +\n  tm_fill(col = \"GDPPC\", \n          style = \"pretty\",\n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC\",\n            inner.margins = c(0.1, 0.1, 0.1, 0.1),\n            outer.margins = c(0.1, 0.1, 0.1, 0.1)\n            ) + \n  tm_grid(alpha = ) \n\n\n\n\n\n\n\n\n\n\nmutate is function that creates new column from previous column datas\n\nst_contiguity creates nb neighbour matrix (QUEEN contiguity, by default)\nst_weights creates row-standardised weights (style=\"W\") from nb object\nOne-step function using sfdep; a wrapper for spdep but writes output into sf dataframe\n\n\n\n\nshow code\nwm_q &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry), \n         wt = st_weights(nb, style = \"W\"),\n         .before = 1)\n\n\n\n\n\n\nbelow is “old_style”\n\n\n\nshow code\n# moran_i = global_moran(\n#   hunan_GDPPC$GDPPC,\n#   hunan_GDPPC$nb,\n#   hunan_GDPPC$wt\n# )\n# glimpse(moran_i)\n\n\n\n\n\n\nMonte Carlo: simulation more accurate, calculate Local Moran’s I using\nunnest is needed to turn the output of local_moran into individual columns in lisa\n\nlocal_moran will create a group table; a separate list of columns that’s hard to read\n\nseveral high/low options for Moran’s I\n\n$mean is default;\n$median can be used if distribution is highly skewed (eg skew high == biased to right)\n\n\n\n\nshow code\n# \n# lisa &lt;- wm_q %&gt;%\n#   mutate(localmoran = local_moran(GDPPC, nb, wt, nsim=99), \n#          .before = 1) %&gt;%\n#   unnest(localmoran)\n# glimpse(lisa)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_GLSA.html#getting-started---import-packages",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_GLSA.html#getting-started---import-packages",
    "title": "In-class Exercise 2: Spatial",
    "section": "",
    "text": "This function calls pacman to load sf, tidyverse, tmap, knitr packages;\n\ntmap : For thematic mapping; powerful mapping package\nsf : for geospatial data handling, but also geoprocessing: buffer, point-in-polygon count, etc\n\nbatch processing over GIS packages; can handle tibble format\n\nsfdep : creates space-time cube, EHSA; replaces spdep\ntidyverse : for non-spatial data handling; commonly used R package\nknitr : generates html table\n\n\npacman::p_load(tmap, sf, sfdep, tidyverse, knitr)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_GLSA.html#loading-the-data",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_GLSA.html#loading-the-data",
    "title": "In-class Exercise 2: Spatial",
    "section": "",
    "text": "Hunan: geospatial dataset in ESRI shapefile format\n\nuse of st_read() to import assf data.frame\n\n$geometry column is actually a list inside the df cell; that’s the power of the tibble dataframe\n“features” of simple features refers to geometric features eg point line curve etc\n\nnote projection is WGS84; see `88\n\nhunan2012: attribute format in csv format\n\nuse of read_csv() astbl_df data.frame\n\n!IMPORTANT! to retain geometry, you must left join to the sf dataframe (eg you can also hunan2012 right join hunan)\n\nwithout sf dataframe, normal tibble dataframe will drop the geometry column\n\n\n\n\nshow code\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `C:\\1darren\\ISSS624\\In-class_Ex\\In-class_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nshow code\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\nhunan_GDPPC &lt;- left_join(hunan,hunan2012)%&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\n\nshow code\n#qtm(hunan, \"GDPPC\") +\n#  tm_layout(main.title = \"GDPPC\", main.title.position = \"right\")\n\n\ntm_shape(hunan_GDPPC) +\n  tm_fill(col = \"GDPPC\", \n          style = \"pretty\",\n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC\",\n            inner.margins = c(0.1, 0.1, 0.1, 0.1),\n            outer.margins = c(0.1, 0.1, 0.1, 0.1)\n            ) + \n  tm_grid(alpha = )"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_GLSA.html#deriving-queen-contiguity-weights",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_GLSA.html#deriving-queen-contiguity-weights",
    "title": "In-class Exercise 2: Spatial",
    "section": "",
    "text": "mutate is function that creates new column from previous column datas\n\nst_contiguity creates nb neighbour matrix (QUEEN contiguity, by default)\nst_weights creates row-standardised weights (style=\"W\") from nb object\nOne-step function using sfdep; a wrapper for spdep but writes output into sf dataframe\n\n\n\n\nshow code\nwm_q &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry), \n         wt = st_weights(nb, style = \"W\"),\n         .before = 1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_GLSA.html#computing-global-morans-i",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_GLSA.html#computing-global-morans-i",
    "title": "In-class Exercise 2: Spatial",
    "section": "",
    "text": "below is “old_style”\n\n\n\nshow code\n# moran_i = global_moran(\n#   hunan_GDPPC$GDPPC,\n#   hunan_GDPPC$nb,\n#   hunan_GDPPC$wt\n# )\n# glimpse(moran_i)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_GLSA.html#computing-local-morans-i",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_GLSA.html#computing-local-morans-i",
    "title": "In-class Exercise 2: Spatial",
    "section": "",
    "text": "Monte Carlo: simulation more accurate, calculate Local Moran’s I using\nunnest is needed to turn the output of local_moran into individual columns in lisa\n\nlocal_moran will create a group table; a separate list of columns that’s hard to read\n\nseveral high/low options for Moran’s I\n\n$mean is default;\n$median can be used if distribution is highly skewed (eg skew high == biased to right)\n\n\n\n\nshow code\n# \n# lisa &lt;- wm_q %&gt;%\n#   mutate(localmoran = local_moran(GDPPC, nb, wt, nsim=99), \n#          .before = 1) %&gt;%\n#   unnest(localmoran)\n# glimpse(lisa)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_Spatial.html",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_Spatial.html",
    "title": "In-class Exercise 2: Spatial",
    "section": "",
    "text": "Hidden: In-Class Lecture notes\n\n\n\n\n\n\n“In real world, there is no concept of equality.” – Prof Kam\n### Dependency Ratio Plot\n\n-   First: check for statistical significance,\n-   Then, plot/test for null hypothesis == normally distributed\n\n-   In geospatial, we don't test for `normality`, we test for `spatial randomness` : whatever is observed in space is randomly distributed\n    -   Similar to default assumption that distributions are Normal, default assumption\n    -   Worth checking; not all\n\n-   `Dependency Ratio`: Number of non-working people / Number of working people\n    - High chance that dependency ratio is not randomly distributed;\n    - Socio-economic factors (eg private homes, \"good\" schools, workers)\n    - SES-locales can affect crime rates, electricity consumption, \n    =&gt; Concept of `Spatial Inequality`: This world is not equal\n\n\n### Tobler's First Law of Geography: \n\nEverything is related to everything else, but near things are more related than distant things\n\n\n### Spatial Weights\n\n- Spatial weights help to parse spatial dependency\n- Use of `adjacency` or `neighbours` as a frame to examine dependency\n    - Neighbourhood search: neighbours or not neighbours\n    \n- How do we qualify 'neighbourliness'?\n    - ADJACENCY - If vertex/edges touching, then neighbours\n    - DISTANCE  - If within certain distance, then neighbours\n        - binary, (1/0) neighbourhood stats\n    - INVERSE DISTANCE  - 1/distance\n        - gives higher weightage to nearer neighbours; Tobler's\n\n- In the real world, not everything is adjacency -- islands are not touching\n    - If using hex-based map, also \n    - Tobler's first law: not touching but also has direct effects\n- Use of hexagon binning: MODIFIED AREA UNIT\n    - Planning Subzone has huge irregularities e.g. large areas (deployment area), oddly shaped long areas etc\n    - Hexagons are one of most compact tessellating shapes\n    - Modern city design work uses grids -- rare, not very usual (eg Punggol) \n\n\n- QUEEN'S CONTIGUITY: \"So long as they touch\"; any point on edge is vertex\n\n- LAGGED CONTINUITY\n    - Lag 1: all adjacent neighbours\n    - Lag 2: adjacent neighbours' neighbours PLUS Lag 1;\n- All levels of lag include previous levels;\n    - Hence, see correllogram: influence gradually wanes over distance\n- EXAMPLE: Where to put Pizza Hut to minimise drive time, maximize food quality\n    - Look at real estate to identify largest expected market share, penetration\n    \n- If region is considered water or unnecessary, region can be dropped as rows\n    - Has to be excluded from consideration, otherwise will have geometric errors\n    - Will also mess with Moran's I calculations\n    \n    \n- Use of ROW-STANDARDISED WEIGHTS to avoid bias\n    - Since weights matrix is a matrix, diagonals are zero, symmetric about diagonal\n    - Also possible to calculate COLUMN-STANDARDISED METRICS with similar effect\n\n- GLOBAL Measure are for mathematical understanding;\n- LOCAL measures are more interesting\n\n- GDPPC in any regions will be unevenly spatially distributed\n    - Aside from capital, may have secondary city: second region of prosperity & development as growth centre\n    \n    \n### Spatial Dependency\n\n- When measuring rainfall, not every region has a rainfall station. How do we do this for all region?\n    - Use interpolation for uncovered regions + concept of Spatial Dependency\n    \n### Spatial Autocorrelation\n- Typical correlation compares 2 different features;\n- Spatial Autocorrelation compares one feature, of self vs neighbour\n    - Similar formula to correlation coefficient\n    \n- Spatial Autocorrelation tests (Moran's I, Geary's C) tries to reject the null hypothesis that \"things are randomly distributed in space\"\n    - Positive Spatial Autocorrelation: Pattern of clustering, e.g. high is found near to high \n    - Negative Spatial Autocorrelation does not mean randomness! Instead, it follows some ordered pattern (e.g. checkerboard -- high surrounds low, low surrounds high); searching for outliers where different from surroundings\n    - Spatial Autocorrelation suggests non-random patterns \n    \n- in Spatial Data, go directly to monte carlo simulation\n\n\n### LISA\n- Helps to identify clusters (high-high, low-low) and outliers (high-low, low-high) ==&gt; regions of positive/negative spatial autocorrelation\n    - hence quadrant 5, \"none\"\n- For all tests, we only highlight Spatial Autocorrelation AND high statistical significance\n\n- Why might region not be statistically significant?\n    - Not enough neighbours; test is uncertain\n    - Enough neighbours, but data just not stat-significant\n    \n    \n### Gi*\n- For Gi*, should always be distance-based;\n    - Moran's I, Geary's C can be contiguity or distance-based; but Getis-Ord should always be distance-based\n    - Evolved from work on dengue transmissions; hence distance is more valuable\n- Gi does not count itself, Gi* counts itself\n\n\n### Mann-Kendall \n- Useful for time-series data;\n    - Hunan GDPPC is single-instance, but most effects happen over space and time; thus, it's useful to consider time \n- A \"sign\" statistic:\n    - compares value at time j vs at time k;\n    - outputs either +1 if decreasing, -1 if increasing, or 0 if no change\n    - does not consider output, only test for monotonicity and \"interprets the relationship\"\n    - cannot have breaks in timeseries; eg if COVID, add datapoint of 0 to keep it going\n- Note: this is nonspatial, only statistical\n\n\n### Emerging Hotspot EHSA\n- Space & time analysis improvement over Mann-Kendall\n- Instead using Gi* instead of value\n    - Compares Gi* over time -- increasing or decreasing\n    \n- Need to transform data into SPACETIME CUBE\n    - x, y dimensions of attribute a\n    - z-dimension of time \n    - `sfdep` allows construction of spacetime cube"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_Spatial.html#in-class-lecture-notes",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_Spatial.html#in-class-lecture-notes",
    "title": "In-class Exercise 2: Spatial",
    "section": "",
    "text": "“In real world, there is no concept of equality.” – Prof Kam\n### Dependency Ratio Plot\n\n-   First: check for statistical significance,\n-   Then, plot/test for null hypothesis == normally distributed\n\n-   In geospatial, we don't test for `normality`, we test for `spatial randomness` : whatever is observed in space is randomly distributed\n    -   Similar to default assumption that distributions are Normal, default assumption\n    -   Worth checking; not all\n\n-   `Dependency Ratio`: Number of non-working people / Number of working people\n    - High chance that dependency ratio is not randomly distributed;\n    - Socio-economic factors (eg private homes, \"good\" schools, workers)\n    - SES-locales can affect crime rates, electricity consumption, \n    =&gt; Concept of `Spatial Inequality`: This world is not equal\n\n\n### Tobler's First Law of Geography: \n\nEverything is related to everything else, but near things are more related than distant things\n\n\n### Spatial Weights\n\n- Spatial weights help to parse spatial dependency\n- Use of `adjacency` or `neighbours` as a frame to examine dependency\n    - Neighbourhood search: neighbours or not neighbours\n    \n- How do we qualify 'neighbourliness'?\n    - ADJACENCY - If vertex/edges touching, then neighbours\n    - DISTANCE  - If within certain distance, then neighbours\n        - binary, (1/0) neighbourhood stats\n    - INVERSE DISTANCE  - 1/distance\n        - gives higher weightage to nearer neighbours; Tobler's\n\n- In the real world, not everything is adjacency -- islands are not touching\n    - If using hex-based map, also \n    - Tobler's first law: not touching but also has direct effects\n- Use of hexagon binning: MODIFIED AREA UNIT\n    - Planning Subzone has huge irregularities e.g. large areas (deployment area), oddly shaped long areas etc\n    - Hexagons are one of most compact tessellating shapes\n    - Modern city design work uses grids -- rare, not very usual (eg Punggol) \n\n\n- QUEEN'S CONTIGUITY: \"So long as they touch\"; any point on edge is vertex\n\n- LAGGED CONTINUITY\n    - Lag 1: all adjacent neighbours\n    - Lag 2: adjacent neighbours' neighbours PLUS Lag 1;\n- All levels of lag include previous levels;\n    - Hence, see correllogram: influence gradually wanes over distance\n- EXAMPLE: Where to put Pizza Hut to minimise drive time, maximize food quality\n    - Look at real estate to identify largest expected market share, penetration\n    \n- If region is considered water or unnecessary, region can be dropped as rows\n    - Has to be excluded from consideration, otherwise will have geometric errors\n    - Will also mess with Moran's I calculations\n    \n    \n- Use of ROW-STANDARDISED WEIGHTS to avoid bias\n    - Since weights matrix is a matrix, diagonals are zero, symmetric about diagonal\n    - Also possible to calculate COLUMN-STANDARDISED METRICS with similar effect\n\n- GLOBAL Measure are for mathematical understanding;\n- LOCAL measures are more interesting\n\n- GDPPC in any regions will be unevenly spatially distributed\n    - Aside from capital, may have secondary city: second region of prosperity & development as growth centre\n    \n    \n### Spatial Dependency\n\n- When measuring rainfall, not every region has a rainfall station. How do we do this for all region?\n    - Use interpolation for uncovered regions + concept of Spatial Dependency\n    \n### Spatial Autocorrelation\n- Typical correlation compares 2 different features;\n- Spatial Autocorrelation compares one feature, of self vs neighbour\n    - Similar formula to correlation coefficient\n    \n- Spatial Autocorrelation tests (Moran's I, Geary's C) tries to reject the null hypothesis that \"things are randomly distributed in space\"\n    - Positive Spatial Autocorrelation: Pattern of clustering, e.g. high is found near to high \n    - Negative Spatial Autocorrelation does not mean randomness! Instead, it follows some ordered pattern (e.g. checkerboard -- high surrounds low, low surrounds high); searching for outliers where different from surroundings\n    - Spatial Autocorrelation suggests non-random patterns \n    \n- in Spatial Data, go directly to monte carlo simulation\n\n\n### LISA\n- Helps to identify clusters (high-high, low-low) and outliers (high-low, low-high) ==&gt; regions of positive/negative spatial autocorrelation\n    - hence quadrant 5, \"none\"\n- For all tests, we only highlight Spatial Autocorrelation AND high statistical significance\n\n- Why might region not be statistically significant?\n    - Not enough neighbours; test is uncertain\n    - Enough neighbours, but data just not stat-significant\n    \n    \n### Gi*\n- For Gi*, should always be distance-based;\n    - Moran's I, Geary's C can be contiguity or distance-based; but Getis-Ord should always be distance-based\n    - Evolved from work on dengue transmissions; hence distance is more valuable\n- Gi does not count itself, Gi* counts itself\n\n\n### Mann-Kendall \n- Useful for time-series data;\n    - Hunan GDPPC is single-instance, but most effects happen over space and time; thus, it's useful to consider time \n- A \"sign\" statistic:\n    - compares value at time j vs at time k;\n    - outputs either +1 if decreasing, -1 if increasing, or 0 if no change\n    - does not consider output, only test for monotonicity and \"interprets the relationship\"\n    - cannot have breaks in timeseries; eg if COVID, add datapoint of 0 to keep it going\n- Note: this is nonspatial, only statistical\n\n\n### Emerging Hotspot EHSA\n- Space & time analysis improvement over Mann-Kendall\n- Instead using Gi* instead of value\n    - Compares Gi* over time -- increasing or decreasing\n    \n- Need to transform data into SPACETIME CUBE\n    - x, y dimensions of attribute a\n    - z-dimension of time \n    - `sfdep` allows construction of spacetime cube"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_Spatial.html#overview",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_Spatial.html#overview",
    "title": "In-class Exercise 2: Spatial",
    "section": "Overview",
    "text": "Overview\n\nSee also In-Class Exercise 2: GLSA and In-Class Exercise 2: EHSA"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_Spatial.html#getting-started---import-packages",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_Spatial.html#getting-started---import-packages",
    "title": "In-class Exercise 2: Spatial",
    "section": "Getting Started - Import packages",
    "text": "Getting Started - Import packages\nThis function calls pacman to load sf, tidyverse, tmap, knitr packages;\n\ntmap : For thematic mapping; powerful mapping package\nsf : for geospatial data handling, but also geoprocessing: buffer, point-in-polygon count, etc\n\nbatch processing over GIS packages; can handle tibble format\n\nsfdep : creates space-time cube, EHSA; replaces spdep\ntidyverse : for non-spatial data handling; commonly used R package\nknitr : generates html table\n\n\npacman::p_load(tmap, sf, sfdep, tidyverse, knitr)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_Spatial.html#loading-the-data",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_Spatial.html#loading-the-data",
    "title": "In-class Exercise 2: Spatial",
    "section": "Loading the data",
    "text": "Loading the data\n\nHunan: geospatial dataset in ESRI shapefile format\n\nuse of st_read() to import assf data.frame\n\n$geometry column is actually a list inside the df cell; that’s the power of the tibble dataframe\n“features” of simple features refers to geometric features eg point line curve etc\n\nnote projection is WGS84; see `88\n\nhunan2012: attribute format in csv format\n\nuse of read_csv() astbl_df data.frame\n\n!IMPORTANT! to retain geometry, you must left join to the sf dataframe (eg you can also hunan2012 right join hunan)\n\nwithout sf dataframe, normal tibble dataframe will drop the geometry column\n\n\n\n\nshow code\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `C:\\1darren\\ISSS624\\In-class_Ex\\In-class_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nshow code\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\nhunan_GDPPC &lt;- left_join(hunan,hunan2012)%&gt;%\n  select(1:4, 7, 15)\n\n\n\nPlot a chloropleth of GDPPC\n\n\nshow code\n#qtm(hunan, \"GDPPC\") +\n#  tm_layout(main.title = \"GDPPC\", main.title.position = \"right\")\n\n\ntm_shape(hunan_GDPPC) +\n  tm_fill(col = \"GDPPC\", \n          style = \"pretty\",\n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC\",\n            inner.margins = c(0.1, 0.1, 0.1, 0.1),\n            outer.margins = c(0.1, 0.1, 0.1, 0.1)\n            ) + \n  tm_grid(alpha = )"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_Spatial.html#deriving-queen-contiguity-weights",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_Spatial.html#deriving-queen-contiguity-weights",
    "title": "In-class Exercise 2: Spatial",
    "section": "Deriving QUEEN contiguity weights",
    "text": "Deriving QUEEN contiguity weights\n\nmutate is function that creates new column from previous column datas\n\nst_contiguity creates nb neighbour matrix (QUEEN contiguity, by default)\nst_weights creates row-standardised weights (style=\"W\") from nb object\nOne-step function using sfdep; a wrapper for spdep but writes output into sf dataframe\n\n\n\n\nshow code\nwm_q &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry), \n         wt = st_weights(nb, style = \"W\"),\n         .before = 1)"
  }
]