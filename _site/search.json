[
  {
    "objectID": "Hands-on_Ex03/Hands-on_Ex03.html",
    "href": "Hands-on_Ex03/Hands-on_Ex03.html",
    "title": "Hands-on Exercise 3: Processing & Visualizing Flow Data",
    "section": "",
    "text": "On this page, I address Hands-On Exercise for Lesson 03:"
  },
  {
    "objectID": "Hands-on_Ex03/Hands-on_Ex03.html#import-packages",
    "href": "Hands-on_Ex03/Hands-on_Ex03.html#import-packages",
    "title": "Hands-on Exercise 3: Processing & Visualizing Flow Data",
    "section": "1.2 Import packages",
    "text": "1.2 Import packages\n\n\nshow code\npacman::p_load(tmap, sf, DT, stplanr,\n               performance,\n               ggpubr, tidyverse)\n\n\n\nsf, tidyverse, tmap from previous\nNEW:\n\nDT -\nstplanar -\nperformance -\nggpubr -"
  },
  {
    "objectID": "Hands-on_Ex03/Hands-on_Ex03.html#preparing-flow-data",
    "href": "Hands-on_Ex03/Hands-on_Ex03.html#preparing-flow-data",
    "title": "Hands-on Exercise 3: Processing & Visualizing Flow Data",
    "section": "1.3 Preparing Flow Data",
    "text": "1.3 Preparing Flow Data\n\n1.3.1 Importing O/D Data\n\n/data/geospatial/Hunan.###: This csv is “Passenger Volume by Origin Destination Bus Stops”, via LTA Datamall\n\nConvert ORIGIN_PT_CODE and DESTINATION_PT_CODE from numeric to character datatype\n\n\n\n\nshow code\nodbus &lt;- read_csv(\"data/aspatial/origin_destination_bus_202310.csv\")\n\n\nRows: 5694297 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): YEAR_MONTH, DAY_TYPE, PT_TYPE, ORIGIN_PT_CODE, DESTINATION_PT_CODE\ndbl (2): TIME_PER_HOUR, TOTAL_TRIPS\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nshow code\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE) \n\nglimpse(odbus)\n\n\nRows: 5,694,297\nColumns: 7\n$ YEAR_MONTH          &lt;chr&gt; \"2023-10\", \"2023-10\", \"2023-10\", \"2023-10\", \"2023-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKENDS/HOLIDAY\", \"WEEKDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;dbl&gt; 16, 16, 14, 14, 17, 17, 17, 7, 14, 14, 10, 20, 20,…\n$ PT_TYPE             &lt;chr&gt; \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"BUS\", \"…\n$ ORIGIN_PT_CODE      &lt;fct&gt; 04168, 04168, 80119, 80119, 44069, 20281, 20281, 1…\n$ DESTINATION_PT_CODE &lt;fct&gt; 10051, 10051, 90079, 90079, 17229, 20141, 20141, 1…\n$ TOTAL_TRIPS         &lt;dbl&gt; 3, 5, 3, 5, 4, 1, 24, 2, 1, 7, 3, 2, 5, 1, 1, 1, 1…\n\n\n\n\n1.3.2 Extracting study data (Weekday Morning Peak)\n\nuse filter() to select rows by:\n\nfilter for \"WEEKDAY\" using $DAY_TYPE column\nfilter for tap-on between 0600 and 0900 using $TIME_PER_HOUR column\n\ngroup_by identifies O/D flows by bus stop codes\n\nsummarise() aggregates $TOTAL_TRIPS into $TRIPS\n\nwrite_rds() as R data object for future use\n\nto load, read_rds()`\n\n\n\n\nshow code\nodbus6_9 &lt;- odbus %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6 &\n           TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE,\n           DESTINATION_PT_CODE) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n\n`summarise()` has grouped output by 'ORIGIN_PT_CODE'. You can override using\nthe `.groups` argument.\n\n\nshow code\n## save for future use:\nwrite_rds(odbus6_9, \"data/rds/odbus6_9.rds\")\ndatatable(head(odbus6_9, 5))"
  },
  {
    "objectID": "Hands-on_Ex03/Hands-on_Ex03.html#working-with-geospatial-data",
    "href": "Hands-on_Ex03/Hands-on_Ex03.html#working-with-geospatial-data",
    "title": "Hands-on Exercise 3: Processing & Visualizing Flow Data",
    "section": "1.4 Working with Geospatial Data",
    "text": "1.4 Working with Geospatial Data\n\n1.4.1 Import geospatial data\n\nuse st_read() to import BusStop & MPSZ-2019 as sf dataframe;\n\nst_transform() to transform projection to SVY21 / crs3414\n\n\n\n\nshow code\nbusstop &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"BusStop\") %&gt;%\n  st_transform(crs = 3414)\n\n\nReading layer `BusStop' from data source \n  `C:\\1darren\\ISSS624\\Hands-on_Ex03\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\nshow code\nmpsz &lt;- st_read(dsn = \"data/geospatial\",\n                   layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\n\nReading layer `MPSZ-2019' from data source \n  `C:\\1darren\\ISSS624\\Hands-on_Ex03\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\npreview mpsz:\n\n\n\nshow code\nmpsz\n\n\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n                 SUBZONE_N SUBZONE_C       PLN_AREA_N PLN_AREA_C       REGION_N\n1              MARINA EAST    MESZ01      MARINA EAST         ME CENTRAL REGION\n2         INSTITUTION HILL    RVSZ05     RIVER VALLEY         RV CENTRAL REGION\n3           ROBERTSON QUAY    SRSZ01  SINGAPORE RIVER         SR CENTRAL REGION\n4  JURONG ISLAND AND BUKOM    WISZ01  WESTERN ISLANDS         WI    WEST REGION\n5             FORT CANNING    MUSZ02           MUSEUM         MU CENTRAL REGION\n6         MARINA EAST (MP)    MPSZ05    MARINE PARADE         MP CENTRAL REGION\n7                   SUDONG    WISZ03  WESTERN ISLANDS         WI    WEST REGION\n8                  SEMAKAU    WISZ02  WESTERN ISLANDS         WI    WEST REGION\n9           SOUTHERN GROUP    SISZ02 SOUTHERN ISLANDS         SI CENTRAL REGION\n10                 SENTOSA    SISZ01 SOUTHERN ISLANDS         SI CENTRAL REGION\n   REGION_C                       geometry\n1        CR MULTIPOLYGON (((33222.98 29...\n2        CR MULTIPOLYGON (((28481.45 30...\n3        CR MULTIPOLYGON (((28087.34 30...\n4        WR MULTIPOLYGON (((14557.7 304...\n5        CR MULTIPOLYGON (((29542.53 31...\n6        CR MULTIPOLYGON (((35279.55 30...\n7        WR MULTIPOLYGON (((15772.59 21...\n8        WR MULTIPOLYGON (((19843.41 21...\n9        CR MULTIPOLYGON (((30870.53 22...\n10       CR MULTIPOLYGON (((26879.04 26..."
  },
  {
    "objectID": "Hands-on_Ex03/Hands-on_Ex03.html#geospatial-data-wrangling",
    "href": "Hands-on_Ex03/Hands-on_Ex03.html#geospatial-data-wrangling",
    "title": "Hands-on Exercise 3: Processing & Visualizing Flow Data",
    "section": "1.5. Geospatial Data Wrangling",
    "text": "1.5. Geospatial Data Wrangling\n\nst_intersection() to find find overlap of point (e.g. busstop) and polygon (e.g. mpsz) shapes, creating an sf object output\n\nas part of this, non-SG bus stops are dropped as they fall outside SG/mpsz boundary\nuse select() to only retain two columns, $BUS_STOP_N and $SUBZONE_C\nst_drop_geometry() keeps it a simple dataframe instead of geom sf\n\ndatatable() seems much more useful than kable()\n\n\n\nshow code\nbusstop_mpsz &lt;- st_intersection(busstop, mpsz) %&gt;%\n  select(BUS_STOP_N, SUBZONE_C) %&gt;%\n  st_drop_geometry()\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\nshow code\nwrite_rds(busstop_mpsz, \"data/rds/busstop_mpsz.rds\")  \ndatatable(busstop_mpsz)\n\n\n\n\n\n\n\n\nnow we combine planning subzone code to O/D dataset\n\nthis specifically creates the $ORIGIN_SZ column, we’ll need to repeat this for the destination subzone col later\n\n\n\n\nshow code\nod_data = left_join(odbus6_9 , busstop_mpsz,\n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  rename(ORIGIN_BS = ORIGIN_PT_CODE,\n         ORIGIN_SZ = SUBZONE_C,\n         DESTIN_BS = DESTINATION_PT_CODE)\n\n\nWarning in left_join(odbus6_9, busstop_mpsz, by = c(ORIGIN_PT_CODE = \"BUS_STOP_N\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 25632 of `x` matches multiple rows in `y`.\nℹ Row 673 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\n\nWe received warning:\n\nWarning: Detected an unexpected many-to-many relationship between `x` and `y`.\n\nFor ‘x’, makes sense, as we have multiple repeats of ORIGIN_PT_CODE (with different DESTINATION_PT_CODE pairs)\nFor ‘y’, this is not surprising; we saw duplicated bus stops in busstop dataset.\n\n\n\nshow code\nduplicate &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\ncat(\"Before cleaning, nrows of duplicate: \", paste0(nrow(duplicate)))\n\n\nBefore cleaning, nrows of duplicate:  1186\n\n\nshow code\nod_data &lt;- unique(od_data)\n\nduplicate2 &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\ncat(\"\\nAfter cleaning, nrows of duplicate: \", paste0(nrow(duplicate2)))\n\n\n\nAfter cleaning, nrows of duplicate:  0\n\n\n\non this second step, we create $DESTIN_SZ column, repeating from the $ORIGIN_SZ step earlier\n\n\n\nshow code\nod_data &lt;- left_join(od_data , busstop_mpsz,\n            by = c(\"DESTIN_BS\" = \"BUS_STOP_N\")) \n\n\nWarning in left_join(od_data, busstop_mpsz, by = c(DESTIN_BS = \"BUS_STOP_N\")): Detected an unexpected many-to-many relationship between `x` and `y`.\nℹ Row 167 of `x` matches multiple rows in `y`.\nℹ Row 672 of `y` matches multiple rows in `x`.\nℹ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\nshow code\nduplicate &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\n\n\ncat(\"\\nBefore cleaning, nrows of duplicate: \", paste0(nrow(duplicate)))\n\n\n\nBefore cleaning, nrows of duplicate:  1350\n\n\nshow code\nod_data &lt;- unique(od_data)\n\n\n\nduplicate2 &lt;- od_data %&gt;%\n  group_by_all() %&gt;%\n  filter(n()&gt;1) %&gt;%\n  ungroup()\ncat(\"\\nAfter cleaning, nrows of duplicate: \", paste0(nrow(duplicate2)), \"\\n\")\n\n\n\nAfter cleaning, nrows of duplicate:  0 \n\n\nshow code\nod_data &lt;- od_data %&gt;%\n  rename(DESTIN_SZ = SUBZONE_C) %&gt;%\n  drop_na() %&gt;%\n  group_by(ORIGIN_SZ, DESTIN_SZ) %&gt;%\n  summarise(MORNING_PEAK = sum(TRIPS))\n\n\n`summarise()` has grouped output by 'ORIGIN_SZ'. You can override using the\n`.groups` argument.\n\n\nshow code\nwrite_rds(od_data, \"data/rds/od_data.rds\")\n# uncomment to load od_data\n# od_data &lt;- read_rds(\"data/rds/od_data.rds\")\n\n\n\nMore repeated destination bus stops (1350) than origin bus stops (1186)"
  },
  {
    "objectID": "Hands-on_Ex03/Hands-on_Ex03.html#visualizing-spatial-interaction",
    "href": "Hands-on_Ex03/Hands-on_Ex03.html#visualizing-spatial-interaction",
    "title": "Hands-on Exercise 3: Processing & Visualizing Flow Data",
    "section": "1.6 Visualizing Spatial Interaction",
    "text": "1.6 Visualizing Spatial Interaction\n\nGoal is to visualize inter-mpsz zonal flows\n\n\n1.6.1 Removing intra-zonal flows\n\n\nshow code\nod_data1 &lt;- od_data[od_data$ORIGIN_SZ!=od_data$DESTIN_SZ,]\nglimpse(od_data1)\n\n\nRows: 20,787\nColumns: 3\nGroups: ORIGIN_SZ [310]\n$ ORIGIN_SZ    &lt;chr&gt; \"AMSZ01\", \"AMSZ01\", \"AMSZ01\", \"AMSZ01\", \"AMSZ01\", \"AMSZ01…\n$ DESTIN_SZ    &lt;chr&gt; \"AMSZ02\", \"AMSZ03\", \"AMSZ04\", \"AMSZ05\", \"AMSZ06\", \"AMSZ07…\n$ MORNING_PEAK &lt;dbl&gt; 10591, 14980, 3106, 7734, 2306, 1824, 2734, 2300, 164, 93…\n\n\n\n\n1.6.2 Creating desire lines\n\nAptly named “od2line” function for converting od to line object\n\nfrom documentation, first two columns of flow dataframe needs to correspond to first column of zones dataframe\n\n\n\n\nshow code\nflowLine &lt;- od2line(flow = od_data1, \n                    zones = mpsz,\n                    zone_code = \"SUBZONE_C\")\n\n\nCreating centroids representing desire line start and end points.\n\n\n\n\n1.6.3 Visualizing desire lines\n\nuse tm_shape() + tm_lines to create flow lines:\n\nCOMMENTED OUT: This takes too long to run, and results in a black blob of lines that are hard to read. 1000+ lines are drawn to be rendered, which is\n\n\n\n\nshow code\n# tm_shape(mpsz) +\n#   tm_polygons() +\n# flowLine %&gt;%  \n# tm_shape() +\n#   tm_lines(lwd = \"MORNING_PEAK\",\n#            style = \"quantile\",\n#            scale = c(0.1, 1, 3, 5, 7, 10),\n#            n = 6,\n#            alpha = 0.3)\n\n\n\nVisualizing only flows with trips &gt; 5000\nI increased the alpha to 0.5 for greater visibility\n\n\n\nshow code\ntm_shape(mpsz) +\n  tm_polygons() +\nflowLine %&gt;%  \n  filter(MORNING_PEAK &gt;= 5000) %&gt;%\ntm_shape() +\n  tm_lines(lwd = \"MORNING_PEAK\",\n           style = \"quantile\",\n           # scale = c(0.1, 1, 3, 5, 7, 10),\n           n = 6,\n           alpha = 0.5)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "",
    "text": "This function calls pacman to load sf, tidyverse, tmap packages; - tmap : For thematic mapping - sf : for geospatial data handling - tidyverse : for non-spatial data handling\n\npacman::p_load(tmap, sf, tidyverse)\n\n\n\n\n\n\n\nAdditional Notes\n\n\n\n\n\n\nIn the ‘old days’, the library-import would have been written thus:\n\nlibrary(tmap)\nlibrary(sf)\nlibrary(tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#getting-started---import-packages",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#getting-started---import-packages",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "",
    "text": "This function calls pacman to load sf, tidyverse, tmap packages; - tmap : For thematic mapping - sf : for geospatial data handling - tidyverse : for non-spatial data handling\n\npacman::p_load(tmap, sf, tidyverse)\n\n\n\n\n\n\n\nAdditional Notes\n\n\n\n\n\n\nIn the ‘old days’, the library-import would have been written thus:\n\nlibrary(tmap)\nlibrary(sf)\nlibrary(tidyverse)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#preparing-the-flow-data",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#preparing-the-flow-data",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "Preparing the Flow Data",
    "text": "Preparing the Flow Data\n\nImporting the Origin/Destination (O/D) Data\nFirstly, we will import the Passenger volume by Origin/Destination Bus Stops dataset downloaded by LTA Datamall by using read_csv() of readr package.\n\n\nShow the code\nodbus &lt;- read_csv(\"data/aspatial/origin_destination_bus_202308.csv\")\n\n\n\n\n\n\n\n\nLearneR notes\n\n\n\n\n\n\nNB: Here, we use relative paths, which is helpful for Rstudio, HTML rendering etc\n\nThis works because current file (“In-class_Ex1.qmd”) is in the same subdirectory level as /data file\nIn the “Olde Style”, we would specify data directories\n\nIn “Environment” window, clicking white triangle beside ODBUS gives an .info() of the odbus dataframe; easier than using str (structure) command\n\nCan also open folder in R to review for quick inspection;\nRemember to close after use to avoid\n\n\n\n\n\n\n\nodbus$ORIGIN_PT_CODE &lt;- as.factor(odbus$ORIGIN_PT_CODE)\nodbus$DESTINATION_PT_CODE &lt;- as.factor(odbus$DESTINATION_PT_CODE)\n\n\n\n\n\n\n\nLearneR notes\n\n\n\n\n\n\nCommand changes two columns into factor datatype; categorical variable stored as integer\n\n\n\n\n\norigtrip_7_9 &lt;- odbus %&gt;% \n  filter(DAY_TYPE == \"WEEKDAY\")  %&gt;% \n  filter(TIME_PER_HOUR &gt;= 7 & TIME_PER_HOUR &lt;= 9)  %&gt;% \n  group_by(ORIGIN_PT_CODE)  %&gt;% \n  summarise(TRIPS = sum(TOTAL_TRIPS))\norigtrip_7_9\n\n# A tibble: 5,015 × 2\n   ORIGIN_PT_CODE TRIPS\n   &lt;fct&gt;          &lt;dbl&gt;\n 1 01012           1617\n 2 01013            813\n 3 01019           1620\n 4 01029           2383\n 5 01039           2727\n 6 01059           1415\n 7 01109            115\n 8 01112           7675\n 9 01113           8074\n10 01119           3913\n# ℹ 5,005 more rows\n\n\n\n\n\n\n\n\nLearneR notes\n\n\n\n\n\n\n%&gt;% Pipe function; chains together several operations in sequence to allow for multi-filter\n\nCode filters from odbus to …\n\n\nFilter by WEEKDAY\nTIME_PER_HOUR between 7 to 9 inclusive (eg 0700 to 0959)\nCreates a table, grouped by ORIGIN_PT_CODE…\n…with a fourth summary column, TRIPS, created as a sum of TOTAL_TRIPS"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#extracting-the-study-data",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#extracting-the-study-data",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "Extracting the Study Data",
    "text": "Extracting the Study Data\n\nbusstop &lt;- st_read(dsn = \"data/geospatial\", \n                   layer = \"BusStop\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `BusStop' from data source \n  `C:\\1darren\\ISSS624\\In-class_Ex\\In-class_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\nmpsz &lt;- st_read(dsn = \"data/geospatial\", \n                layer = \"MPSZ-2019\") %&gt;%\n  st_transform(crs = 3414)\n\nReading layer `MPSZ-2019' from data source \n  `C:\\1darren\\ISSS624\\In-class_Ex\\In-class_Ex1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 332 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 103.6057 ymin: 1.158699 xmax: 104.0885 ymax: 1.470775\nGeodetic CRS:  WGS 84\n\n\n\nBUSSTOP$GEOMETRY: Binary Large Object Field; defines coordinate for each bus-stop\n\nTypical cell of dataframe is either INT or CHAR\nBLOF allows to store a list; POINT(lat, long)\n\nMPSZ$GEOMETRYis instead a multipolygon; a list of 300 x 2 coordinates\nIMPORTANT: There are two different coordinates system;\n\nMost GML, KML, data.gov are in wgs84, decimal degree data format\nWe should change to svy21\nCRS: coordinate system of Singapore; cahnge to metres\n\nNow we can join odbus (bus stop code) and busstop (coordinates) and also master plan subzone (mpsz) ## Working with Geospatial Data"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#importing-geospatial-data",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#importing-geospatial-data",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "Importing Geospatial Data",
    "text": "Importing Geospatial Data"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#geospatial-data-wrangling",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#geospatial-data-wrangling",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "Geospatial Data Wrangling",
    "text": "Geospatial Data Wrangling"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#combining-bus-stop-mpsz",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#combining-bus-stop-mpsz",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "Combining Bus Stop & MPSZ",
    "text": "Combining Bus Stop & MPSZ"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#visualizing-the-geospatial-data",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#visualizing-the-geospatial-data",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "Visualizing the Geospatial Data",
    "text": "Visualizing the Geospatial Data"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#viewing-the-subzone-spatial-file",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#viewing-the-subzone-spatial-file",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "Viewing the Subzone Spatial File",
    "text": "Viewing the Subzone Spatial File"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#personal-notes",
    "href": "In-class_Ex/In-class_Ex1/In-class_Ex1.html#personal-notes",
    "title": "In-class Exercise 1: My First Date with Geospatial Data Analytics",
    "section": "Personal Notes",
    "text": "Personal Notes\n\nODBus Structure\n\n7 columns;\ntime_per_hour: hour of travel\norigin_pt_code, destination_pt_code: per bus-trip; change-bus counted as 2 trips\ntotal_trips: number of passengers moving between startpoint/endpoint, during that hour\nIssue: no explicit location, only bus stop code; can we reconcile bus-stop-code with geospatial data?\nyes:"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_GLSA.html",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_GLSA.html",
    "title": "In-class Exercise 2: Spatial",
    "section": "",
    "text": "This function calls pacman to load sf, tidyverse, tmap, knitr packages;\n\ntmap : For thematic mapping; powerful mapping package\nsf : for geospatial data handling, but also geoprocessing: buffer, point-in-polygon count, etc\n\nbatch processing over GIS packages; can handle tibble format\n\nsfdep : creates space-time cube, EHSA; replaces spdep\ntidyverse : for non-spatial data handling; commonly used R package\nknitr : generates html table\n\n\npacman::p_load(tmap, sf, sfdep, tidyverse, knitr)\n\n\n\n\n\nHunan: geospatial dataset in ESRI shapefile format\n\nuse of st_read() to import assf data.frame\n\n$geometry column is actually a list inside the df cell; that’s the power of the tibble dataframe\n“features” of simple features refers to geometric features eg point line curve etc\n\nnote projection is WGS84; see `88\n\nhunan2012: attribute format in csv format\n\nuse of read_csv() astbl_df data.frame\n\n!IMPORTANT! to retain geometry, you must left join to the sf dataframe (eg you can also hunan2012 right join hunan)\n\nwithout sf dataframe, normal tibble dataframe will drop the geometry column\n\n\n\n\nshow code\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `C:\\1darren\\ISSS624\\In-class_Ex\\In-class_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nshow code\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\nhunan_GDPPC &lt;- left_join(hunan,hunan2012)%&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\n\nshow code\n#qtm(hunan, \"GDPPC\") +\n#  tm_layout(main.title = \"GDPPC\", main.title.position = \"right\")\n\n\ntm_shape(hunan_GDPPC) +\n  tm_fill(col = \"GDPPC\", \n          style = \"pretty\",\n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC\",\n            inner.margins = c(0.1, 0.1, 0.1, 0.1),\n            outer.margins = c(0.1, 0.1, 0.1, 0.1)\n            ) + \n  tm_grid(alpha = ) \n\n\n\n\n\n\n\n\n\n\nmutate is function that creates new column from previous column datas\n\nst_contiguity creates nb neighbour matrix (QUEEN contiguity, by default)\nst_weights creates row-standardised weights (style=\"W\") from nb object\nOne-step function using sfdep; a wrapper for spdep but writes output into sf dataframe\n\n\n\n\nshow code\nwm_q &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry), \n         wt = st_weights(nb, style = \"W\"),\n         .before = 1)\n\n\n\n\n\n\nbelow is “old_style”\n\n\n\nshow code\n# moran_i = global_moran(\n#   hunan_GDPPC$GDPPC,\n#   hunan_GDPPC$nb,\n#   hunan_GDPPC$wt\n# )\n# glimpse(moran_i)\n\n\n\n\n\n\nMonte Carlo: simulation more accurate, calculate Local Moran’s I using\nunnest is needed to turn the output of local_moran into individual columns in lisa\n\nlocal_moran will create a group table; a separate list of columns that’s hard to read\n\nseveral high/low options for Moran’s I\n\n$mean is default;\n$median can be used if distribution is highly skewed (eg skew high == biased to right)\n\n\n\n\nshow code\n# \n# lisa &lt;- wm_q %&gt;%\n#   mutate(localmoran = local_moran(GDPPC, nb, wt, nsim=99), \n#          .before = 1) %&gt;%\n#   unnest(localmoran)\n# glimpse(lisa)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_GLSA.html#getting-started---import-packages",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_GLSA.html#getting-started---import-packages",
    "title": "In-class Exercise 2: Spatial",
    "section": "",
    "text": "This function calls pacman to load sf, tidyverse, tmap, knitr packages;\n\ntmap : For thematic mapping; powerful mapping package\nsf : for geospatial data handling, but also geoprocessing: buffer, point-in-polygon count, etc\n\nbatch processing over GIS packages; can handle tibble format\n\nsfdep : creates space-time cube, EHSA; replaces spdep\ntidyverse : for non-spatial data handling; commonly used R package\nknitr : generates html table\n\n\npacman::p_load(tmap, sf, sfdep, tidyverse, knitr)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_GLSA.html#loading-the-data",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_GLSA.html#loading-the-data",
    "title": "In-class Exercise 2: Spatial",
    "section": "",
    "text": "Hunan: geospatial dataset in ESRI shapefile format\n\nuse of st_read() to import assf data.frame\n\n$geometry column is actually a list inside the df cell; that’s the power of the tibble dataframe\n“features” of simple features refers to geometric features eg point line curve etc\n\nnote projection is WGS84; see `88\n\nhunan2012: attribute format in csv format\n\nuse of read_csv() astbl_df data.frame\n\n!IMPORTANT! to retain geometry, you must left join to the sf dataframe (eg you can also hunan2012 right join hunan)\n\nwithout sf dataframe, normal tibble dataframe will drop the geometry column\n\n\n\n\nshow code\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `C:\\1darren\\ISSS624\\In-class_Ex\\In-class_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nshow code\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\nhunan_GDPPC &lt;- left_join(hunan,hunan2012)%&gt;%\n  select(1:4, 7, 15)\n\n\n\n\n\n\nshow code\n#qtm(hunan, \"GDPPC\") +\n#  tm_layout(main.title = \"GDPPC\", main.title.position = \"right\")\n\n\ntm_shape(hunan_GDPPC) +\n  tm_fill(col = \"GDPPC\", \n          style = \"pretty\",\n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC\",\n            inner.margins = c(0.1, 0.1, 0.1, 0.1),\n            outer.margins = c(0.1, 0.1, 0.1, 0.1)\n            ) + \n  tm_grid(alpha = )"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_GLSA.html#deriving-queen-contiguity-weights",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_GLSA.html#deriving-queen-contiguity-weights",
    "title": "In-class Exercise 2: Spatial",
    "section": "",
    "text": "mutate is function that creates new column from previous column datas\n\nst_contiguity creates nb neighbour matrix (QUEEN contiguity, by default)\nst_weights creates row-standardised weights (style=\"W\") from nb object\nOne-step function using sfdep; a wrapper for spdep but writes output into sf dataframe\n\n\n\n\nshow code\nwm_q &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry), \n         wt = st_weights(nb, style = \"W\"),\n         .before = 1)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_GLSA.html#computing-global-morans-i",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_GLSA.html#computing-global-morans-i",
    "title": "In-class Exercise 2: Spatial",
    "section": "",
    "text": "below is “old_style”\n\n\n\nshow code\n# moran_i = global_moran(\n#   hunan_GDPPC$GDPPC,\n#   hunan_GDPPC$nb,\n#   hunan_GDPPC$wt\n# )\n# glimpse(moran_i)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_GLSA.html#computing-local-morans-i",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_GLSA.html#computing-local-morans-i",
    "title": "In-class Exercise 2: Spatial",
    "section": "",
    "text": "Monte Carlo: simulation more accurate, calculate Local Moran’s I using\nunnest is needed to turn the output of local_moran into individual columns in lisa\n\nlocal_moran will create a group table; a separate list of columns that’s hard to read\n\nseveral high/low options for Moran’s I\n\n$mean is default;\n$median can be used if distribution is highly skewed (eg skew high == biased to right)\n\n\n\n\nshow code\n# \n# lisa &lt;- wm_q %&gt;%\n#   mutate(localmoran = local_moran(GDPPC, nb, wt, nsim=99), \n#          .before = 1) %&gt;%\n#   unnest(localmoran)\n# glimpse(lisa)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_Spatial.html",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_Spatial.html",
    "title": "In-class Exercise 2: Spatial",
    "section": "",
    "text": "Hidden: In-Class Lecture notes\n\n\n\n\n\n\n“In real world, there is no concept of equality.” – Prof Kam\n### Dependency Ratio Plot\n\n-   First: check for statistical significance,\n-   Then, plot/test for null hypothesis == normally distributed\n\n-   In geospatial, we don't test for `normality`, we test for `spatial randomness` : whatever is observed in space is randomly distributed\n    -   Similar to default assumption that distributions are Normal, default assumption\n    -   Worth checking; not all\n\n-   `Dependency Ratio`: Number of non-working people / Number of working people\n    - High chance that dependency ratio is not randomly distributed;\n    - Socio-economic factors (eg private homes, \"good\" schools, workers)\n    - SES-locales can affect crime rates, electricity consumption, \n    =&gt; Concept of `Spatial Inequality`: This world is not equal\n\n\n### Tobler's First Law of Geography: \n\nEverything is related to everything else, but near things are more related than distant things\n\n\n### Spatial Weights\n\n- Spatial weights help to parse spatial dependency\n- Use of `adjacency` or `neighbours` as a frame to examine dependency\n    - Neighbourhood search: neighbours or not neighbours\n    \n- How do we qualify 'neighbourliness'?\n    - ADJACENCY - If vertex/edges touching, then neighbours\n    - DISTANCE  - If within certain distance, then neighbours\n        - binary, (1/0) neighbourhood stats\n    - INVERSE DISTANCE  - 1/distance\n        - gives higher weightage to nearer neighbours; Tobler's\n\n- In the real world, not everything is adjacency -- islands are not touching\n    - If using hex-based map, also \n    - Tobler's first law: not touching but also has direct effects\n- Use of hexagon binning: MODIFIED AREA UNIT\n    - Planning Subzone has huge irregularities e.g. large areas (deployment area), oddly shaped long areas etc\n    - Hexagons are one of most compact tessellating shapes\n    - Modern city design work uses grids -- rare, not very usual (eg Punggol) \n\n\n- QUEEN'S CONTIGUITY: \"So long as they touch\"; any point on edge is vertex\n\n- LAGGED CONTINUITY\n    - Lag 1: all adjacent neighbours\n    - Lag 2: adjacent neighbours' neighbours PLUS Lag 1;\n- All levels of lag include previous levels;\n    - Hence, see correllogram: influence gradually wanes over distance\n- EXAMPLE: Where to put Pizza Hut to minimise drive time, maximize food quality\n    - Look at real estate to identify largest expected market share, penetration\n    \n- If region is considered water or unnecessary, region can be dropped as rows\n    - Has to be excluded from consideration, otherwise will have geometric errors\n    - Will also mess with Moran's I calculations\n    \n    \n- Use of ROW-STANDARDISED WEIGHTS to avoid bias\n    - Since weights matrix is a matrix, diagonals are zero, symmetric about diagonal\n    - Also possible to calculate COLUMN-STANDARDISED METRICS with similar effect\n\n- GLOBAL Measure are for mathematical understanding;\n- LOCAL measures are more interesting\n\n- GDPPC in any regions will be unevenly spatially distributed\n    - Aside from capital, may have secondary city: second region of prosperity & development as growth centre\n    \n    \n### Spatial Dependency\n\n- When measuring rainfall, not every region has a rainfall station. How do we do this for all region?\n    - Use interpolation for uncovered regions + concept of Spatial Dependency\n    \n### Spatial Autocorrelation\n- Typical correlation compares 2 different features;\n- Spatial Autocorrelation compares one feature, of self vs neighbour\n    - Similar formula to correlation coefficient\n    \n- Spatial Autocorrelation tests (Moran's I, Geary's C) tries to reject the null hypothesis that \"things are randomly distributed in space\"\n    - Positive Spatial Autocorrelation: Pattern of clustering, e.g. high is found near to high \n    - Negative Spatial Autocorrelation does not mean randomness! Instead, it follows some ordered pattern (e.g. checkerboard -- high surrounds low, low surrounds high); searching for outliers where different from surroundings\n    - Spatial Autocorrelation suggests non-random patterns \n    \n- in Spatial Data, go directly to monte carlo simulation\n\n\n### LISA\n- Helps to identify clusters (high-high, low-low) and outliers (high-low, low-high) ==&gt; regions of positive/negative spatial autocorrelation\n    - hence quadrant 5, \"none\"\n- For all tests, we only highlight Spatial Autocorrelation AND high statistical significance\n\n- Why might region not be statistically significant?\n    - Not enough neighbours; test is uncertain\n    - Enough neighbours, but data just not stat-significant\n    \n    \n### Gi*\n- For Gi*, should always be distance-based;\n    - Moran's I, Geary's C can be contiguity or distance-based; but Getis-Ord should always be distance-based\n    - Evolved from work on dengue transmissions; hence distance is more valuable\n- Gi does not count itself, Gi* counts itself\n\n\n### Mann-Kendall \n- Useful for time-series data;\n    - Hunan GDPPC is single-instance, but most effects happen over space and time; thus, it's useful to consider time \n- A \"sign\" statistic:\n    - compares value at time j vs at time k;\n    - outputs either +1 if decreasing, -1 if increasing, or 0 if no change\n    - does not consider output, only test for monotonicity and \"interprets the relationship\"\n    - cannot have breaks in timeseries; eg if COVID, add datapoint of 0 to keep it going\n- Note: this is nonspatial, only statistical\n\n\n### Emerging Hotspot EHSA\n- Space & time analysis improvement over Mann-Kendall\n- Instead using Gi* instead of value\n    - Compares Gi* over time -- increasing or decreasing\n    \n- Need to transform data into SPACETIME CUBE\n    - x, y dimensions of attribute a\n    - z-dimension of time \n    - `sfdep` allows construction of spacetime cube"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_Spatial.html#in-class-lecture-notes",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_Spatial.html#in-class-lecture-notes",
    "title": "In-class Exercise 2: Spatial",
    "section": "",
    "text": "“In real world, there is no concept of equality.” – Prof Kam\n### Dependency Ratio Plot\n\n-   First: check for statistical significance,\n-   Then, plot/test for null hypothesis == normally distributed\n\n-   In geospatial, we don't test for `normality`, we test for `spatial randomness` : whatever is observed in space is randomly distributed\n    -   Similar to default assumption that distributions are Normal, default assumption\n    -   Worth checking; not all\n\n-   `Dependency Ratio`: Number of non-working people / Number of working people\n    - High chance that dependency ratio is not randomly distributed;\n    - Socio-economic factors (eg private homes, \"good\" schools, workers)\n    - SES-locales can affect crime rates, electricity consumption, \n    =&gt; Concept of `Spatial Inequality`: This world is not equal\n\n\n### Tobler's First Law of Geography: \n\nEverything is related to everything else, but near things are more related than distant things\n\n\n### Spatial Weights\n\n- Spatial weights help to parse spatial dependency\n- Use of `adjacency` or `neighbours` as a frame to examine dependency\n    - Neighbourhood search: neighbours or not neighbours\n    \n- How do we qualify 'neighbourliness'?\n    - ADJACENCY - If vertex/edges touching, then neighbours\n    - DISTANCE  - If within certain distance, then neighbours\n        - binary, (1/0) neighbourhood stats\n    - INVERSE DISTANCE  - 1/distance\n        - gives higher weightage to nearer neighbours; Tobler's\n\n- In the real world, not everything is adjacency -- islands are not touching\n    - If using hex-based map, also \n    - Tobler's first law: not touching but also has direct effects\n- Use of hexagon binning: MODIFIED AREA UNIT\n    - Planning Subzone has huge irregularities e.g. large areas (deployment area), oddly shaped long areas etc\n    - Hexagons are one of most compact tessellating shapes\n    - Modern city design work uses grids -- rare, not very usual (eg Punggol) \n\n\n- QUEEN'S CONTIGUITY: \"So long as they touch\"; any point on edge is vertex\n\n- LAGGED CONTINUITY\n    - Lag 1: all adjacent neighbours\n    - Lag 2: adjacent neighbours' neighbours PLUS Lag 1;\n- All levels of lag include previous levels;\n    - Hence, see correllogram: influence gradually wanes over distance\n- EXAMPLE: Where to put Pizza Hut to minimise drive time, maximize food quality\n    - Look at real estate to identify largest expected market share, penetration\n    \n- If region is considered water or unnecessary, region can be dropped as rows\n    - Has to be excluded from consideration, otherwise will have geometric errors\n    - Will also mess with Moran's I calculations\n    \n    \n- Use of ROW-STANDARDISED WEIGHTS to avoid bias\n    - Since weights matrix is a matrix, diagonals are zero, symmetric about diagonal\n    - Also possible to calculate COLUMN-STANDARDISED METRICS with similar effect\n\n- GLOBAL Measure are for mathematical understanding;\n- LOCAL measures are more interesting\n\n- GDPPC in any regions will be unevenly spatially distributed\n    - Aside from capital, may have secondary city: second region of prosperity & development as growth centre\n    \n    \n### Spatial Dependency\n\n- When measuring rainfall, not every region has a rainfall station. How do we do this for all region?\n    - Use interpolation for uncovered regions + concept of Spatial Dependency\n    \n### Spatial Autocorrelation\n- Typical correlation compares 2 different features;\n- Spatial Autocorrelation compares one feature, of self vs neighbour\n    - Similar formula to correlation coefficient\n    \n- Spatial Autocorrelation tests (Moran's I, Geary's C) tries to reject the null hypothesis that \"things are randomly distributed in space\"\n    - Positive Spatial Autocorrelation: Pattern of clustering, e.g. high is found near to high \n    - Negative Spatial Autocorrelation does not mean randomness! Instead, it follows some ordered pattern (e.g. checkerboard -- high surrounds low, low surrounds high); searching for outliers where different from surroundings\n    - Spatial Autocorrelation suggests non-random patterns \n    \n- in Spatial Data, go directly to monte carlo simulation\n\n\n### LISA\n- Helps to identify clusters (high-high, low-low) and outliers (high-low, low-high) ==&gt; regions of positive/negative spatial autocorrelation\n    - hence quadrant 5, \"none\"\n- For all tests, we only highlight Spatial Autocorrelation AND high statistical significance\n\n- Why might region not be statistically significant?\n    - Not enough neighbours; test is uncertain\n    - Enough neighbours, but data just not stat-significant\n    \n    \n### Gi*\n- For Gi*, should always be distance-based;\n    - Moran's I, Geary's C can be contiguity or distance-based; but Getis-Ord should always be distance-based\n    - Evolved from work on dengue transmissions; hence distance is more valuable\n- Gi does not count itself, Gi* counts itself\n\n\n### Mann-Kendall \n- Useful for time-series data;\n    - Hunan GDPPC is single-instance, but most effects happen over space and time; thus, it's useful to consider time \n- A \"sign\" statistic:\n    - compares value at time j vs at time k;\n    - outputs either +1 if decreasing, -1 if increasing, or 0 if no change\n    - does not consider output, only test for monotonicity and \"interprets the relationship\"\n    - cannot have breaks in timeseries; eg if COVID, add datapoint of 0 to keep it going\n- Note: this is nonspatial, only statistical\n\n\n### Emerging Hotspot EHSA\n- Space & time analysis improvement over Mann-Kendall\n- Instead using Gi* instead of value\n    - Compares Gi* over time -- increasing or decreasing\n    \n- Need to transform data into SPACETIME CUBE\n    - x, y dimensions of attribute a\n    - z-dimension of time \n    - `sfdep` allows construction of spacetime cube"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_Spatial.html#overview",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_Spatial.html#overview",
    "title": "In-class Exercise 2: Spatial",
    "section": "Overview",
    "text": "Overview\n\nSee also In-Class Exercise 2: GLSA and In-Class Exercise 2: EHSA"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_Spatial.html#getting-started---import-packages",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_Spatial.html#getting-started---import-packages",
    "title": "In-class Exercise 2: Spatial",
    "section": "Getting Started - Import packages",
    "text": "Getting Started - Import packages\nThis function calls pacman to load sf, tidyverse, tmap, knitr packages;\n\ntmap : For thematic mapping; powerful mapping package\nsf : for geospatial data handling, but also geoprocessing: buffer, point-in-polygon count, etc\n\nbatch processing over GIS packages; can handle tibble format\n\nsfdep : creates space-time cube, EHSA; replaces spdep\ntidyverse : for non-spatial data handling; commonly used R package\nknitr : generates html table\n\n\npacman::p_load(tmap, sf, sfdep, tidyverse, knitr)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_Spatial.html#loading-the-data",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_Spatial.html#loading-the-data",
    "title": "In-class Exercise 2: Spatial",
    "section": "Loading the data",
    "text": "Loading the data\n\nHunan: geospatial dataset in ESRI shapefile format\n\nuse of st_read() to import assf data.frame\n\n$geometry column is actually a list inside the df cell; that’s the power of the tibble dataframe\n“features” of simple features refers to geometric features eg point line curve etc\n\nnote projection is WGS84; see `88\n\nhunan2012: attribute format in csv format\n\nuse of read_csv() astbl_df data.frame\n\n!IMPORTANT! to retain geometry, you must left join to the sf dataframe (eg you can also hunan2012 right join hunan)\n\nwithout sf dataframe, normal tibble dataframe will drop the geometry column\n\n\n\n\nshow code\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `C:\\1darren\\ISSS624\\In-class_Ex\\In-class_Ex2\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nshow code\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\nhunan_GDPPC &lt;- left_join(hunan,hunan2012)%&gt;%\n  select(1:4, 7, 15)\n\n\n\nPlot a chloropleth of GDPPC\n\n\nshow code\n#qtm(hunan, \"GDPPC\") +\n#  tm_layout(main.title = \"GDPPC\", main.title.position = \"right\")\n\n\ntm_shape(hunan_GDPPC) +\n  tm_fill(col = \"GDPPC\", \n          style = \"pretty\",\n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC\",\n            inner.margins = c(0.1, 0.1, 0.1, 0.1),\n            outer.margins = c(0.1, 0.1, 0.1, 0.1)\n            ) + \n  tm_grid(alpha = )"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex2/In-class_Ex2_Spatial.html#deriving-queen-contiguity-weights",
    "href": "In-class_Ex/In-class_Ex2/In-class_Ex2_Spatial.html#deriving-queen-contiguity-weights",
    "title": "In-class Exercise 2: Spatial",
    "section": "Deriving QUEEN contiguity weights",
    "text": "Deriving QUEEN contiguity weights\n\nmutate is function that creates new column from previous column datas\n\nst_contiguity creates nb neighbour matrix (QUEEN contiguity, by default)\nst_weights creates row-standardised weights (style=\"W\") from nb object\nOne-step function using sfdep; a wrapper for spdep but writes output into sf dataframe\n\n\n\n\nshow code\nwm_q &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry), \n         wt = st_weights(nb, style = \"W\"),\n         .before = 1)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISSS624",
    "section": "",
    "text": "Welcome to ISSS624 Geospatial Analytics Applications, by 1Darren!\nIn this website, I am going to share with you my learning journey of Geospatial Analytics (under instruction from Prof Kam).\nBecause Rstudio regularly crashes with Quarto when I try to commit, here is a link menu to my exercises:\n\n\n\n\n\n\nHands-on Exercises:\n\n\n\n\nHands-on Exercise 1\nHands-on Exercise 2 parts 1 & 2 - Spatial Weights & Global Measures of Spatial Autocorrelation\n\nHands-on Exercise 2 parts 3 - Local Measures of Spatial Autocorrelation – broken up because Rstudio kept crashing\n\nHands-on Exercise 3\n\n\n\n\n\n\n\n\n\nTake-Home Exercises\n\n\n\n\nTake-home Exercise 1\n\n\n\n\n\n\n\n\n\nIn-class Exercises:\n\n\n\n\nWarning, these are incomplete, Prof Kam rushes through too quickly\nIn-class Exercise 1\nIn-class Exercise 2 Spatial Weights\nIn-class Exercise 2 GLSA\nIn-class Exercise 2 EHSA"
  },
  {
    "objectID": "Take-home_Ex01/take_home_ex_01.html",
    "href": "Take-home_Ex01/take_home_ex_01.html",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "On this page, I try to pass Prof Kam’s class"
  },
  {
    "objectID": "Take-home_Ex01/take_home_ex_01.html#load-packages",
    "href": "Take-home_Ex01/take_home_ex_01.html#load-packages",
    "title": "Take-home Exercise 1",
    "section": "0.1 Load Packages",
    "text": "0.1 Load Packages\nThis function calls pacman to load sf, tidyverse, tmap, knitr packages;\n\ntmap : For thematic mapping; powerful mapping package\nsf : for geospatial data handling, but also geoprocessing: buffer, point-in-polygon count, etc\n\nbatch processing over GIS packages; can handle tibble format\n\nsfdep : creates space-time cube, EHSA; replaces spdep\ntidyverse : for non-spatial data handling; commonly used R package and contains dplyr for dataframe manipulation\nmapview : interactive plotting & manipulation\nRColorBrewer : custom colour palettes for manipulation\n\n\npacman::p_load(tmap, sf, sfdep, tidyverse, mapview, RColorBrewer)"
  },
  {
    "objectID": "Take-home_Ex01/take_home_ex_01.html#import-data",
    "href": "Take-home_Ex01/take_home_ex_01.html#import-data",
    "title": "Take-home Exercise 1",
    "section": "0.2 Import Data",
    "text": "0.2 Import Data\n\n0.2.1 Load Bus Stop Location\n\nimport bus stop geospatial data into bus_stop_sf\n\nconvert to svy21 dataframe projection\n\n\n\nshow code\nraw_bus_stop_sf &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"BusStop\") %&gt;%\n  st_transform(crs = 3414)\n\n\nReading layer `BusStop' from data source \n  `C:\\1darren\\ISSS624\\Take-home_Ex01\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 5161 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21\n\n\nshow code\n# bus_stop_sf$BUS_STOP_N  &lt;- as.factor(bus_stop_sf$BUS_STOP_N )\nmapview_test_points = mapview(raw_bus_stop_sf, cex = 3, alpha = .5, popup = NULL)\nmapview_test_points\n\n\n\n\n\n\nHow many rows are repeated?\n\n\ncat(\"Total number of rows in raw_bus_stop_sf\\t\\t: \", paste0(length(raw_bus_stop_sf$BUS_STOP_N)))\n\nTotal number of rows in raw_bus_stop_sf     :  5161\n\ncat(\"\\nTotal unique bus stop IDs in raw_bus_stop_sf\\t: \", paste0(length(unique(raw_bus_stop_sf$BUS_STOP_N))))\n\n\nTotal unique bus stop IDs in raw_bus_stop_sf    :  5145\n\ncat(\"\\nRepeated bus stops\\t\\t\\t\\t:   \", paste0(length(raw_bus_stop_sf$BUS_STOP_N) - length(unique(raw_bus_stop_sf$BUS_STOP_N))))\n\n\nRepeated bus stops              :    16\n\n\n\nIt appears there are 16 datapoints that are specifically repeated; let’s identify the bus stop numbers with repeated rows:\n\nfirst we use filter() with a pipe mark(using or condition) to identify repeated numbers\nwe sort using arrange()\nthen, using group_by() and row_number() we add row numbers based on $BUS_STOP_N\nadding to a new column using mutate()\n\n\n\n\nshow code\nrepeated_df &lt;- raw_bus_stop_sf %&gt;%\n  filter(duplicated(BUS_STOP_N) | duplicated(BUS_STOP_N, fromLast = TRUE)) %&gt;% \n  arrange(BUS_STOP_N) %&gt;%\n  group_by(BUS_STOP_N) %&gt;%\n  mutate(RowNumber = row_number())\n# repeated_df$BUS_STOP_N  &lt;- as.factor(repeated_df$BUS_STOP_N )\nrepeated_df\n\n\nSimple feature collection with 32 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 13488.02 ymin: 32594.17 xmax: 44144.57 ymax: 47934\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 32 × 5\n# Groups:   BUS_STOP_N [16]\n   BUS_STOP_N BUS_ROOF_N LOC_DESC                       geometry RowNumber\n * &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;                       &lt;POINT [m]&gt;     &lt;int&gt;\n 1 11009      B04        Ghim Moh Ter        (23101.34 32594.17)         1\n 2 11009      B04-TMNL   GHIM MOH TER        (23100.58 32604.36)         2\n 3 22501      B02        Blk 662A             (13489.09 35536.4)         1\n 4 22501      B02        BLK 662A            (13488.02 35537.88)         2\n 5 43709      B06        BLK 644              (18963.42 36762.8)         1\n 6 43709      B06        BLK 644             (18952.02 36751.83)         2\n 7 47201      UNK        &lt;NA&gt;                (22616.75 47793.68)         1\n 8 47201      NIL        W'LANDS NTH STN        (22632.92 47934)         2\n 9 51071      B21        MACRITCHIE RESERVO… (28311.27 36036.92)         1\n10 51071      B21        MACRITCHIE RESERVO… (28282.54 36033.93)         2\n# ℹ 22 more rows\n\n\n\nLet’s examine these bus stops on the map;\n\nwe use mapview() to display these repeated bus stops on the map\nwe use zcol=$BUS_STOP_N to give each bus stop an individual colour\nwe used colorRampPalette to expand the palette to 16 different colours\n\n\n\n\nshow code\ncust_palette = colorRampPalette(brewer.pal(11, \"Spectral\"))\n\nmapview_repeated = mapview(repeated_df, zcol=\"BUS_STOP_N\", cex = 3, alpha = .5, popup = NULL, col.regions=cust_palette)\nmapview_repeated\n\n\n\n\n\n\n\nAt this stage, there are 32 entries for 16 bus stops, each with repeated locations\nFirst, we perform some manual cleaning;\n\n\n\n\ndrop_second_stop = c(\"43709\", \"51071\", \"53041\", \"52059\", \"58031\", \"68091\", \"68099\", \"97079\")\nrows_to_retain_df &lt;- repeated_df %&gt;%\n  filter(\n    case_when(\n      BUS_STOP_N == \"11009\" & grepl(\"[a-z]\", LOC_DESC) ~ FALSE,\n      BUS_STOP_N == \"22501\" & grepl(\"[a-z]\", LOC_DESC) ~ FALSE,\n      BUS_STOP_N == \"77329\" & grepl(\"[a-z]\", LOC_DESC) ~ FALSE,\n      BUS_STOP_N == \"82221\" & grepl(\"[a-z]\", LOC_DESC) ~ FALSE,\n      BUS_STOP_N == \"62251\" & grepl(\"[a-z]\", LOC_DESC) ~ FALSE,\n      BUS_STOP_N == \"96319\" & grepl(\"[a-z]\", LOC_DESC) ~ FALSE,\n\n      BUS_STOP_N == \"67421\" & BUS_ROOF_N == \"NIL\" ~ FALSE,\n      BUS_STOP_N %in% drop_second_stop & RowNumber == 2 ~ FALSE,\n      BUS_STOP_N == \"47201\" & is.na(LOC_DESC) ~ FALSE,\n      TRUE ~ TRUE\n    )\n  )\n\nrows_to_retain_df$LOC_DESC  = toupper(rows_to_retain_df$LOC_DESC)\n\nprint(\"Printing rows to retain:\")\n\n[1] \"Printing rows to retain:\"\n\nrows_to_retain_df\n\nSimple feature collection with 16 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 13488.02 ymin: 32604.36 xmax: 44144.57 ymax: 47934\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 16 × 5\n# Groups:   BUS_STOP_N [16]\n   BUS_STOP_N BUS_ROOF_N LOC_DESC                       geometry RowNumber\n * &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;                       &lt;POINT [m]&gt;     &lt;int&gt;\n 1 11009      B04-TMNL   GHIM MOH TER        (23100.58 32604.36)         2\n 2 22501      B02        BLK 662A            (13488.02 35537.88)         2\n 3 43709      B06        BLK 644              (18963.42 36762.8)         1\n 4 47201      NIL        W'LANDS NTH STN        (22632.92 47934)         2\n 5 51071      B21        MACRITCHIE RESERVO… (28311.27 36036.92)         1\n 6 52059      B03        OPP BLK 65           (30770.3 34460.06)         1\n 7 53041      B05        UPP THOMSON ROAD     (28105.8 37246.76)         1\n 8 58031      UNK        OPP CANBERRA DR      (27089.69 47570.9)         1\n 9 62251      B03        BEF BLK 471B        (35500.36 39943.34)         2\n10 67421      B01        CHENG LIM STN EXIT… (34548.54 42052.15)         1\n11 68091      B01        AFT BAKER ST        (32164.11 42695.98)         1\n12 68099      B02        BEF BAKER ST         (32154.9 42742.82)         1\n13 77329      B01        BEF PASIR RIS ST 53 (40765.35 39452.18)         1\n14 82221      B01        BLK 3A               (35323.6 33257.05)         1\n15 96319      NIL        YUSEN LOGISTICS     (42187.23 34995.78)         2\n16 97079      B14        OPP ST. JOHN'S CRES (44144.57 38980.25)         1\n\n\nConfirming rows to retain:\n\n\ncat(\"Number of unique bus stop IDs in `rows_to_retain_df`: \", paste0(length(unique(rows_to_retain_df$BUS_STOP_N))))\n\nNumber of unique bus stop IDs in `rows_to_retain_df`:  16\n\ncat(\"\\nNumber of unique bus stop IDs in `repeated_df`: \",paste0(length(unique(repeated_df$BUS_STOP_N))))\n\n\nNumber of unique bus stop IDs in `repeated_df`:  16\n\n\n\nNow we repeat the steps on raw_bus_stop_sf\n\n\nbus_stop_sf &lt;- raw_bus_stop_sf %&gt;%\n  arrange(BUS_STOP_N) %&gt;%\n  group_by(BUS_STOP_N) %&gt;%\n  mutate(RowNumber = row_number()) %&gt;%\n  filter(\n    case_when(\n      BUS_STOP_N == \"11009\" & grepl(\"[a-z]\", LOC_DESC) ~ FALSE,\n      BUS_STOP_N == \"22501\" & grepl(\"[a-z]\", LOC_DESC) ~ FALSE,\n      BUS_STOP_N == \"77329\" & grepl(\"[a-z]\", LOC_DESC) ~ FALSE,\n      BUS_STOP_N == \"82221\" & grepl(\"[a-z]\", LOC_DESC) ~ FALSE,\n      BUS_STOP_N == \"62251\" & grepl(\"[a-z]\", LOC_DESC) ~ FALSE,\n      BUS_STOP_N == \"96319\" & grepl(\"[a-z]\", LOC_DESC) ~ FALSE,\n      \n      BUS_STOP_N == \"67421\" & BUS_ROOF_N == \"NIL\" ~ FALSE,\n      BUS_STOP_N %in% drop_second_stop & RowNumber == 2 ~ FALSE,\n      BUS_STOP_N == \"47201\" & is.na(LOC_DESC) ~ FALSE,\n      TRUE ~ TRUE\n    )\n  )\nbus_stop_sf\n\nSimple feature collection with 5145 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3970.122 ymin: 26482.1 xmax: 48284.56 ymax: 52983.82\nProjected CRS: SVY21 / Singapore TM\n# A tibble: 5,145 × 5\n# Groups:   BUS_STOP_N [5,145]\n   BUS_STOP_N BUS_ROOF_N LOC_DESC                       geometry RowNumber\n * &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;                       &lt;POINT [m]&gt;     &lt;int&gt;\n 1 01012      B03        HOTEL GRAND PACIFIC  (30140.8 31031.95)         1\n 2 01013      B05        ST JOSEPH'S CH      (30218.75 31126.49)         1\n 3 01019      B04        BRAS BASAH CPLX     (30187.77 31034.55)         1\n 4 01029      B07        OPP NATL LIB        (30345.83 31007.64)         1\n 5 01039      B09        BUGIS CUBE          (30471.08 31175.63)         1\n 6 01059      B08        BUGIS STN EXIT A    (30570.37 31494.55)         1\n 7 01109      TMNL       QUEEN ST TER        (30603.56 31759.58)         1\n 8 01112      B07        OPP BUGIS STN EXIT… (30434.46 31386.45)         1\n 9 01113      B09        BUGIS STN EXIT B    (30542.33 31509.91)         1\n10 01119      B06        AFT BUGIS STN EXIT… (30422.02 31320.02)         1\n# ℹ 5,135 more rows\n\n\n\nCleaning worked for bus_stop_sf\n\n\ncat(\"`raw_bus_stop_sf`:\")\n\n`raw_bus_stop_sf`:\n\ncat(\"\\n&gt;&gt; Number of total rows \\t\\t: \", paste0(nrow(raw_bus_stop_sf)))\n\n\n&gt;&gt; Number of total rows         :  5161\n\ncat(\"\\n&gt;&gt; Number of unique bus stop IDs\\t: \", paste0(length(unique(raw_bus_stop_sf$BUS_STOP_N))))\n\n\n&gt;&gt; Number of unique bus stop IDs    :  5145\n\ncat(\"\\n\\n`bus_stop_sf`:\")\n\n\n\n`bus_stop_sf`:\n\ncat(\"\\n&gt;&gt; Number of total rows \\t\\t: \", paste0(nrow(bus_stop_sf)))\n\n\n&gt;&gt; Number of total rows         :  5145\n\ncat(\"\\n&gt;&gt; Number of unique bus stop IDs\\t: \", paste0(length(unique(bus_stop_sf$BUS_STOP_N))))\n\n\n&gt;&gt; Number of unique bus stop IDs    :  5145\n\n\n\n\n0.2.2 Generate Hexes\n\n\nshow code\n# area_honeycomb_grid = st_make_grid(bus_stop, units::as_units(216506, \"m^2\"), what = \"polygons\", square = FALSE)\nraw_hex_grid = st_make_grid(bus_stop_sf, units::as_units(500, \"m\"), what = \"polygons\", square = FALSE) %&gt;%\n  st_transform(crs = 3414)\n# To sf and add grid ID\nraw_hexagon_sf = st_sf(raw_hex_grid) %&gt;%\n  # add grid ID\n  mutate(grid_id = 1:length(lengths(raw_hex_grid))) %&gt;%\n  st_transform(crs = 3414)\n\n\n# count number of points in each grid\n# https://gis.stackexchange.com/questions/323698/counting-points-in-polygons-with-sf-package-of-r\nraw_hexagon_sf$n_bus_stops = lengths(st_intersects(raw_hexagon_sf, bus_stop_sf))\n\n# remove grid without value of 0 (i.e. no points in side that grid)\nhexagon_sf = filter(raw_hexagon_sf, n_bus_stops &gt; 0)\nhexagon_sf\n\n\nSimple feature collection with 1524 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 3720.122 ymin: 26193.43 xmax: 48720.12 ymax: 53184.55\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n                     raw_hex_grid grid_id n_bus_stops\n1  POLYGON ((3970.122 27925.48...      34           1\n2  POLYGON ((4220.122 28358.49...      65           1\n3  POLYGON ((4470.122 30523.55...      99           1\n4  POLYGON ((4720.122 28358.49...     127           1\n5  POLYGON ((4720.122 30090.54...     129           2\n6  POLYGON ((4720.122 30956.57...     130           1\n7  POLYGON ((4720.122 31822.59...     131           1\n8  POLYGON ((4970.122 28791.5,...     159           1\n9  POLYGON ((4970.122 29657.53...     160           1\n10 POLYGON ((4970.122 30523.55...     161           2\n\n\n\n\nshow code\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nshow code\nmap_hexagon_sf = tm_shape(hexagon_sf) +\n  tm_fill(\n    col = \"n_bus_stops\",\n    palette = \"-viridis\",\n    style = \"cont\",\n    \n    breaks = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12),\n    title = \"Number of bus_stops\",\n    id = \"grid_id\",\n    showNA = FALSE,\n    alpha = 0.6,\n    popup.vars = c(\n      \"Number of Bus Stops: \" = \"n_bus_stops\"\n    ),\n    popup.format = list(\n      n_bus_stops = list(format = \"f\", digits = 0)\n    )\n  ) +\n  tm_borders(col = \"grey40\", lwd = 0.7)\n\nmap_hexagon_sf\n\n\n\n\n\n\nCount number of hexes\nCheck biggest IDs\n\n\n\nshow code\ncat(paste(\"Total number of raw hexes is\\t\\t: \", nrow(raw_hexagon_sf), \"\\n\"))\n\n\nTotal number of raw hexes is        :  5580 \n\n\nshow code\ncat(paste(\"Total number of hexes (n_bus_stop &gt; 1) is\\t: \", nrow(hexagon_sf)), \"\\n\")\n\n\nTotal number of hexes (n_bus_stop &gt; 1) is   :  1524 \n\n\nshow code\ncat(\"\\nPrinting map_hexagon_sf:\\n &gt;&gt; \")\n\n\n\nPrinting map_hexagon_sf:\n &gt;&gt; \n\n\nshow code\nhexagon_sf[hexagon_sf$n_bus_stops &gt; 10, ]\n\n\nSimple feature collection with 2 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 17470.12 ymin: 38317.78 xmax: 42720.12 ymax: 40194.17\nProjected CRS: SVY21 / Singapore TM\n                       raw_hex_grid grid_id n_bus_stops\n304  POLYGON ((17720.12 39616.82...    1752          11\n1466 POLYGON ((42470.12 38317.78...    4820          11\n\n\n\n\n0.3.1 Load Bus Trip Data\n\n\nshow code\nodbus_2308 &lt;- read_csv(\"data/aspatial/origin_destination_bus_202308.csv\")\n\n\nRows: 5709512 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): YEAR_MONTH, DAY_TYPE, PT_TYPE, ORIGIN_PT_CODE, DESTINATION_PT_CODE\ndbl (2): TIME_PER_HOUR, TOTAL_TRIPS\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nshow code\n# Drop pt_type\nodbus_2308 &lt;- select(odbus_2308, -PT_TYPE)\n# alternative way in read_csv df &lt;- read_csv(\"data/aspatial/origin_destination_bus_202308.csv\", col_types = \"ccdcffd\")\n\nodbus_2308$ORIGIN_PT_CODE &lt;- as.factor(odbus_2308$ORIGIN_PT_CODE)\nodbus_2308$DESTINATION_PT_CODE &lt;- as.factor(odbus_2308$DESTINATION_PT_CODE) \nglimpse(odbus_2308)\n\n\nRows: 5,709,512\nColumns: 6\n$ YEAR_MONTH          &lt;chr&gt; \"2023-08\", \"2023-08\", \"2023-08\", \"2023-08\", \"2023-…\n$ DAY_TYPE            &lt;chr&gt; \"WEEKDAY\", \"WEEKENDS/HOLIDAY\", \"WEEKENDS/HOLIDAY\",…\n$ TIME_PER_HOUR       &lt;dbl&gt; 16, 16, 14, 14, 17, 17, 17, 17, 7, 17, 14, 10, 10,…\n$ ORIGIN_PT_CODE      &lt;fct&gt; 04168, 04168, 80119, 80119, 44069, 44069, 20281, 2…\n$ DESTINATION_PT_CODE &lt;fct&gt; 10051, 10051, 90079, 90079, 17229, 17229, 20141, 2…\n$ TOTAL_TRIPS         &lt;dbl&gt; 7, 2, 3, 10, 5, 4, 3, 22, 3, 3, 7, 1, 3, 1, 3, 1, …\n\n\n\nsanity check number of distinct bus stops over months\n\n\nodbus_2309 &lt;- read_csv(\"data/aspatial/origin_destination_bus_202309.csv\")\n\nRows: 5714196 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): YEAR_MONTH, DAY_TYPE, PT_TYPE, ORIGIN_PT_CODE, DESTINATION_PT_CODE\ndbl (2): TIME_PER_HOUR, TOTAL_TRIPS\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nodbus_2310 &lt;- read_csv(\"data/aspatial/origin_destination_bus_202310.csv\")\n\nRows: 5694297 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): YEAR_MONTH, DAY_TYPE, PT_TYPE, ORIGIN_PT_CODE, DESTINATION_PT_CODE\ndbl (2): TIME_PER_HOUR, TOTAL_TRIPS\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Drop pt_type\nodbus_2309 &lt;- select(odbus_2309, -PT_TYPE)\nodbus_2310 &lt;- select(odbus_2310, -PT_TYPE)\n# alternative way in read_csv df &lt;- read_csv(\"data/aspatial/origin_destination_bus_202308.csv\", col_types = \"ccdcffd\")\n\nodbus_2309$ORIGIN_PT_CODE &lt;- as.factor(odbus_2309$ORIGIN_PT_CODE)\nodbus_2310$ORIGIN_PT_CODE &lt;- as.factor(odbus_2310$ORIGIN_PT_CODE)\n\n\n\ncat(\"Confirm distinct origin bus stops 23-08: \\n&gt;&gt; \", paste(length(unique(odbus_2308$ORIGIN_PT_CODE))))\n\nConfirm distinct origin bus stops 23-08: \n&gt;&gt;  5067\n\ncat(\"Confirm distinct origin bus stops 23-09: \\n&gt;&gt; \", paste(length(unique(odbus_2309$ORIGIN_PT_CODE))))\n\nConfirm distinct origin bus stops 23-09: \n&gt;&gt;  5067\n\ncat(\"Confirm distinct origin bus stops 23-10: \\n&gt;&gt; \", paste(length(unique(odbus_2310$ORIGIN_PT_CODE))))\n\nConfirm distinct origin bus stops 23-10: \n&gt;&gt;  5073\n\n\nFirst we split out weekday and then perform mutate to group by\n\n# odbus_2310_weekday = odbus_2310 %&gt;%\n#  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n#  mutate(PEAK = case_when(\n#    TIME_PER_HOUR &gt;= 6 &  TIME_PER_HOUR &lt;= 9 ~ \"MORNING PEAK\",\n#    TIME_PER_HOUR &gt;= 17 &  TIME_PER_HOUR &lt;= 20 ~ \"AFTEROON PEAK\",\n#    TRUE ~ \"Unknown\"\n#  ))\n#cat(\"Confirm $DAY_TYPE only has `WEEKDAY` value: \\n&gt;&gt; \", paste(unique(odbus_2310_weekday$DAY_TYPE)))\n#odbus_2310_weekday\n\n\n\nodbus_filtered &lt;- odbus_2310 %&gt;%\n  mutate(PEAK = case_when(\n    DAY_TYPE == \"WEEKDAY\" & TIME_PER_HOUR &gt;= 6 &  TIME_PER_HOUR &lt;= 9 ~ \"WEEKDAY MORNING\",\n    DAY_TYPE == \"WEEKDAY\" & TIME_PER_HOUR &gt;= 17 &  TIME_PER_HOUR &lt;= 20 ~ \"WEEKDAY AFTERNOON\",\n    DAY_TYPE == \"WEEKENDS/HOLIDAY\" & TIME_PER_HOUR &gt;= 11 &  TIME_PER_HOUR &lt;= 14 ~ \"WEEKEND MORNING\",\n    DAY_TYPE == \"WEEKENDS/HOLIDAY\" & TIME_PER_HOUR &gt;= 16 &  TIME_PER_HOUR &lt;= 19 ~ \"WEEKEND EVENING\",\n    TRUE ~ \"Unknown\"\n  )) %&gt;%\n  filter(\n    case_when(\n      PEAK == \"Unknown\" ~ FALSE,\n      TRUE ~ TRUE\n    )) %&gt;%\n  group_by(ORIGIN_PT_CODE, PEAK) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS))\n\n`summarise()` has grouped output by 'ORIGIN_PT_CODE'. You can override using\nthe `.groups` argument.\n\nodbus_filtered &lt;- odbus_filtered %&gt;%\n  pivot_wider(names_from = PEAK, values_from = TRIPS, values_fill = 0)\n\n\nwrite_rds(odbus_filtered, \"data/rds/odbus_filtered.rds\")\nodbus_filtered\n\n# A tibble: 5,072 × 5\n# Groups:   ORIGIN_PT_CODE [5,072]\n   ORIGIN_PT_CODE `WEEKDAY AFTERNOON` `WEEKDAY MORNING` `WEEKEND EVENING`\n   &lt;fct&gt;                        &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n 1 01012                         8000              1770              3061\n 2 01013                         7038               841              2770\n 3 01019                         3398              1530              1685\n 4 01029                         9089              2483              4063\n 5 01039                        12095              2919              7263\n 6 01059                         2212              1734              1118\n 7 01109                          276               200               101\n 8 01112                        43513              8593             21405\n 9 01113                        25769              7749             11556\n10 01119                         8386              3687              6027\n# ℹ 5,062 more rows\n# ℹ 1 more variable: `WEEKEND MORNING` &lt;dbl&gt;\n\n\nSanity check, number of distinct bus stops?\n\ncat(\"Confirm distinct origin bus stops: \\n&gt;&gt; \", paste(length(unique(odbus_2310$ORIGIN_PT_CODE))))\n\nConfirm distinct origin bus stops: \n&gt;&gt;  5073\n\ncat(\"\\nConfirm distinct destination stops: \\n&gt;&gt; \", paste(length(unique(odbus_2310$DESTINATION_PT_CODE ))))\n\n\nConfirm distinct destination stops: \n&gt;&gt;  5077\n\ncat(\"\\nConfirm distinct bus stops in `bus_stop_sf`: \\n&gt;&gt; \", paste(length(unique(bus_stop_sf$BUS_STOP_N))))\n\n\nConfirm distinct bus stops in `bus_stop_sf`: \n&gt;&gt;  5145\n\n# odbus_2310_copy &lt;- odbus_2310 %&gt;%\n#   rename(BUS_STOP_N = ORIGIN_PT_CODE)\n# \n# non_hexagon_sf &lt;- anti_join(bus_stop_sf, odbus_2310_copy, by = \"BUS_STOP_N\")\n# \n# cat(\"\\nConfirm distinct bus stops in `non_hexagon_sf`: \\n&gt;&gt; \", paste(length(unique(non_hexagon_sf$BUS_STOP_N  ))))\n# \n# mapview_non_hexag = mapview(non_hexagon_sf, cex = 3, alpha = .5, popup = NULL)\n# mapview_non_hexag\n# \n# distinct_odbus &lt;- distinct(select(odbus_2310, ORIGIN_PT_CODE)) #5073\n# distinct_busstop &lt;- distinct(select(bus_stop_sf, BUS_STOP_N)%&gt;%\n#   st_drop_geometry()) #5145\n# \n# anti_join(distinct_odbus, distinct_busstop, by=c(\"ORIGIN_PT_CODE\" =\"BUS_STOP_N\")) # 60\n# anti_join(distinct_busstop, distinct_odbus, by=c(\"BUS_STOP_N\" = \"ORIGIN_PT_CODE\")) #132\n\n\nTest to reproduce 01013 bus stop with 841 trips on weekday mornings\n\n\n\nshow code\nsubset_df &lt;- odbus_2310 %&gt;%\n  filter(ORIGIN_PT_CODE == '01013') %&gt;%\n  filter(DAY_TYPE == \"WEEKDAY\") %&gt;%\n  filter(TIME_PER_HOUR &gt;= 6) %&gt;%\n  filter(TIME_PER_HOUR &lt;= 9) %&gt;%\n  group_by(ORIGIN_PT_CODE, DAY_TYPE, TIME_PER_HOUR, YEAR_MONTH) %&gt;%\n  summarise(TRIPS = sum(TOTAL_TRIPS), .groups = 'keep')\nsubset_df\n\n\n# A tibble: 4 × 5\n# Groups:   ORIGIN_PT_CODE, DAY_TYPE, TIME_PER_HOUR, YEAR_MONTH [4]\n  ORIGIN_PT_CODE DAY_TYPE TIME_PER_HOUR YEAR_MONTH TRIPS\n  &lt;fct&gt;          &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n1 01013          WEEKDAY              6 2023-10      180\n2 01013          WEEKDAY              7 2023-10      138\n3 01013          WEEKDAY              8 2023-10      254\n4 01013          WEEKDAY              9 2023-10      269\n\n\nshow code\n180 + 138 + 254 + 269\n\n\n[1] 841"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex1/data/geospatial/MPSZ-2019.html",
    "href": "In-class_Ex/In-class_Ex1/data/geospatial/MPSZ-2019.html",
    "title": "ISSS624",
    "section": "",
    "text": "&lt;!DOCTYPE qgis PUBLIC ‘http://mrcc.com/qgis.dtd’ ‘SYSTEM’&gt;     dataset\n\n\n        0 0     false"
  },
  {
    "objectID": "Take-home_Ex01/take_home_ex_01.html#create-bus_stop_hexgrid_id",
    "href": "Take-home_Ex01/take_home_ex_01.html#create-bus_stop_hexgrid_id",
    "title": "Take-home Exercise 1",
    "section": "Create bus_stop_hexgrid_id",
    "text": "Create bus_stop_hexgrid_id\n\nCombine hexagon_sf with bus_stop_sf\n\n\n\nshow code\nbus_stop_hexgrid_id &lt;- st_intersection(bus_stop_sf, hexagon_sf) %&gt;%\n  select(grid_id, BUS_STOP_N) %&gt;%\n  st_drop_geometry()\n\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\nshow code\ncat(\"\\nNumber of bus stops\\t:\", length(unique(bus_stop_hexgrid_id$BUS_STOP_N)))\n\n\n\nNumber of bus stops : 5145\n\n\nshow code\ncat(\"\\nNumber of hexgrids\\t:\", length(unique(bus_stop_hexgrid_id$grid_id)))\n\n\n\nNumber of hexgrids  : 1524\n\n\nshow code\nbus_stop_hexgrid_id\n\n\n# A tibble: 5,145 × 2\n   grid_id BUS_STOP_N\n *   &lt;int&gt; &lt;chr&gt;     \n 1      34 25059     \n 2      65 25751     \n 3      99 26379     \n 4     127 25761     \n 5     129 25719     \n 6     129 26389     \n 7     130 26369     \n 8     131 26299     \n 9     159 25741     \n10     160 25711     \n# ℹ 5,135 more rows"
  },
  {
    "objectID": "Take-home_Ex01/take_home_ex_01.html#combine-bus_stop_hexgrid_id-with-trip-details",
    "href": "Take-home_Ex01/take_home_ex_01.html#combine-bus_stop_hexgrid_id-with-trip-details",
    "title": "Take-home Exercise 1",
    "section": "Combine bus_stop_hexgrid_id with trip details",
    "text": "Combine bus_stop_hexgrid_id with trip details\n\nCombine bus_stop_hexgrid_id with odbus_filtered\n\n\n\nshow code\ngrid_trips_df &lt;- left_join(odbus_filtered, bus_stop_hexgrid_id, \n            by = c(\"ORIGIN_PT_CODE\" = \"BUS_STOP_N\")) %&gt;%\n  select(grid_id, \n         `WEEKDAY MORNING`,\n         `WEEKDAY AFTERNOON`,\n         `WEEKEND MORNING`,\n         `WEEKEND EVENING`)  %&gt;%\n  group_by(grid_id) %&gt;%\n  summarise(\n    WEEKDAY_MORNING_TRIPS = sum(`WEEKDAY MORNING`), \n    WEEKDAY_AFTERNOON_TRIPS = sum(`WEEKDAY AFTERNOON`), \n    WEEKEND_MORNING_TRIPS = sum(`WEEKEND MORNING`), \n    WEEKEND_EVENING_TRIPS = sum(`WEEKEND EVENING`)\n    )\n\n\nAdding missing grouping variables: `ORIGIN_PT_CODE`\n\n\nshow code\ngrid_trips_df\n\n\n# A tibble: 1,505 × 5\n   grid_id WEEKDAY_MORNING_TRIPS WEEKDAY_AFTERNOON_TRIPS WEEKEND_MORNING_TRIPS\n     &lt;int&gt;                 &lt;dbl&gt;                   &lt;dbl&gt;                 &lt;dbl&gt;\n 1      34                   103                     390                     0\n 2      65                    52                     114                    26\n 3      99                    78                     291                    52\n 4     127                   185                    1905                   187\n 5     129                  1116                    2899                   455\n 6     130                    53                     241                    76\n 7     131                    60                     368                    45\n 8     159                    64                     296                    21\n 9     160                   251                     297                    39\n10     161                  1034                    2472                   691\n# ℹ 1,495 more rows\n# ℹ 1 more variable: WEEKEND_EVENING_TRIPS &lt;dbl&gt;"
  },
  {
    "objectID": "Take-home_Ex01/take_home_ex_01.html#left-join-back-to-hexagon_df",
    "href": "Take-home_Ex01/take_home_ex_01.html#left-join-back-to-hexagon_df",
    "title": "Take-home Exercise 1",
    "section": "Left join back to hexagon_df",
    "text": "Left join back to hexagon_df\n\n\nshow code\nhexagon_sf_2 &lt;- left_join(hexagon_sf, grid_trips_df, \n            by = 'grid_id' ) %&gt;%\n  mutate(\n    WEEKDAY_MORNING_TRIPS = ifelse(is.na(WEEKDAY_MORNING_TRIPS), 0, WEEKDAY_MORNING_TRIPS),\n    WEEKDAY_AFTERNOON_TRIPS = ifelse(is.na(WEEKDAY_AFTERNOON_TRIPS), 0, WEEKDAY_AFTERNOON_TRIPS),\n    WEEKEND_MORNING_TRIPS = ifelse(is.na(WEEKEND_MORNING_TRIPS), 0, WEEKEND_MORNING_TRIPS),\n    WEEKEND_EVENING_TRIPS = ifelse(is.na(WEEKEND_EVENING_TRIPS), 0, WEEKEND_EVENING_TRIPS),\n         )\n\nhexagon_sf_2\n\n\nSimple feature collection with 1524 features and 6 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 3720.122 ymin: 26193.43 xmax: 48720.12 ymax: 53184.55\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n   grid_id n_bus_stops WEEKDAY_MORNING_TRIPS WEEKDAY_AFTERNOON_TRIPS\n1       34           1                   103                     390\n2       65           1                    52                     114\n3       99           1                    78                     291\n4      127           1                   185                    1905\n5      129           2                  1116                    2899\n6      130           1                    53                     241\n7      131           1                    60                     368\n8      159           1                    64                     296\n9      160           1                   251                     297\n10     161           2                  1034                    2472\n   WEEKEND_MORNING_TRIPS WEEKEND_EVENING_TRIPS                   raw_hex_grid\n1                      0                    56 POLYGON ((3970.122 27925.48...\n2                     26                    14 POLYGON ((4220.122 28358.49...\n3                     52                   100 POLYGON ((4470.122 30523.55...\n4                    187                   346 POLYGON ((4720.122 28358.49...\n5                    455                   634 POLYGON ((4720.122 30090.54...\n6                     76                    55 POLYGON ((4720.122 30956.57...\n7                     45                    49 POLYGON ((4720.122 31822.59...\n8                     21                    53 POLYGON ((4970.122 28791.5,...\n9                     39                   132 POLYGON ((4970.122 29657.53...\n10                   691                  1030 POLYGON ((4970.122 30523.55...\n\n\n\nNow to try to do data analysis\nIn my defense, I emailed prof kam for DAL notes but he ignroed me for 6 months\n\n\n\nshow code\nsummary(hexagon_sf_2$WEEKDAY_MORNING_TRIPS)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      0     909    7532   16838   23245  386450 \n\n\nshow code\nhist(hexagon_sf_2$WEEKDAY_MORNING_TRIPS, \n     main = \"Histogram Example\", \n     xlab = \"WEEKDAY_MORNING_TRIPS\", \n     col = \"lightblue\", \n     border = \"black\")\n\n\n\n\n\nshow code\n# Load the ggplot2 package\nlibrary(ggplot2)\n\n\ntrip_col_names &lt;- c(\"WEEKDAY_MORNING_TRIPS\", \"WEEKDAY_AFTERNOON_TRIPS\", \"WEEKEND_MORNING_TRIPS\", \"WEEKEND_EVENING_TRIPS\")\n\npar(mfrow = c(2, 2))  # Set up a 2x2 layout\ncustom_breaks &lt;- seq(0, 550000, by = 50000)\n\nfor (col in trip_col_names) {\n  hist(hexagon_sf_2[[col]], main = col, col = \"lightblue\", border = \"black\",\n       breaks = custom_breaks)\n}\n\n\n\n\n\nshow code\ncolnames(hexagon_sf_2)\n\n\n[1] \"grid_id\"                 \"n_bus_stops\"            \n[3] \"WEEKDAY_MORNING_TRIPS\"   \"WEEKDAY_AFTERNOON_TRIPS\"\n[5] \"WEEKEND_MORNING_TRIPS\"   \"WEEKEND_EVENING_TRIPS\"  \n[7] \"raw_hex_grid\""
  },
  {
    "objectID": "Take-home_Ex01/take_home_ex_01.html#map-hexes-on-singapore",
    "href": "Take-home_Ex01/take_home_ex_01.html#map-hexes-on-singapore",
    "title": "Take-home Exercise 1",
    "section": "Map hexes on SIngapore",
    "text": "Map hexes on SIngapore\n\n\nshow code\nsarb_sf &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Subzone_Census2010\") %&gt;%\n  st_transform(crs = 3414)\n\n\nReading layer `Subzone_Census2010' from data source \n  `C:\\1darren\\ISSS624\\Take-home_Ex01\\data\\geospatial' using driver `ESRI Shapefile'\nSimple feature collection with 311 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2637.869 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nshow code\ntmap_options(check.and.fix = TRUE)\nqtm(sarb_sf)\n\n\nWarning: The shape sarb_sf is invalid. See sf::st_is_valid\n\n\n\n\n\nshow code\n## Additional data from: Data.gov.sg, https://beta.data.gov.sg/datasets/d_02cba6aeeed323b5f6c723527757c0bc/view\n\n\n\nweekday_morning_visweekday_afternoon_visweekend_morning_visweekend_evenings_vis\n\n\n\n\nshow code\ntm_shape(sarb_sf) +\n  tm_polygons() + \n  tm_shape(hexagon_sf_2) + \n  tm_fill(\"WEEKDAY_MORNING_TRIPS\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Number of trips\")\n\n\nWarning: The shape sarb_sf is invalid. See sf::st_is_valid\n\n\n\n\n\n\n\n\n\nshow code\ntm_shape(sarb_sf) +\n  tm_polygons() + \n  tm_shape(hexagon_sf_2) + \n  tm_fill(\"WEEKDAY_AFTERNOON_TRIPS\", \n          style = \"quantile\", \n          palette = \"Greens\",\n          title = \"Dependency ratio\")\n\n\nWarning: The shape sarb_sf is invalid. See sf::st_is_valid\n\n\n\n\n\n\n\n\ntm_shape(sarb_sf) +\n  tm_polygons() + \n  tm_shape(hexagon_sf_2) + \n  tm_fill(\"WEEKEND_MORNING_TRIPS\", \n          style = \"quantile\", \n          palette = \"Reds\",\n          title = \"Dependency ratio\")\n\nWarning: The shape sarb_sf is invalid. See sf::st_is_valid\n\n\n\n\n\n\n\n\n\nshow code\ntm_shape(sarb_sf) +\n  tm_polygons() + \n  tm_shape(hexagon_sf_2) + \n  tm_fill(\"WEEKEND_EVENING_TRIPS\", \n          style = \"quantile\", \n          palette = \"Oranges\",\n          title = \"Dependency ratio\")\n\n\nWarning: The shape sarb_sf is invalid. See sf::st_is_valid\n\n\n\n\n\n\n\n\n\n\nshow code\ntm_shape(sarb_sf) +\n  tm_polygons() + \n  tm_shape(hexagon_sf_2) + \n  tm_fill(\"WEEKDAY_MORNING_TRIPS\", \n          style = \"cont\", \n          palette = \"viridis\",\n          breaks = custom_breaks,\n          title = \"Dependency ratio\")\n\n\nWarning: The shape sarb_sf is invalid. See sf::st_is_valid\n\n\n\n\n\nshow code\ntm_shape(sarb_sf) +\n  tm_polygons() + \n  tm_shape(hexagon_sf_2) + \n  tm_fill(\"WEEKDAY_AFTERNOON_TRIPS\", \n          style = \"cont\", \n          palette = \"viridis\",\n          breaks = custom_breaks,\n          title = \"Dependency ratio\")\n\n\nWarning: The shape sarb_sf is invalid. See sf::st_is_valid\n\n\n\n\n\nshow code\ntm_shape(sarb_sf) +\n  tm_polygons() + \n  tm_shape(hexagon_sf_2) + \n  tm_fill(\"WEEKEND_MORNING_TRIPS\", \n          style = \"cont\", \n          palette = \"viridis\",\n          breaks = custom_breaks,\n          title = \"Dependency ratio\")\n\n\nWarning: The shape sarb_sf is invalid. See sf::st_is_valid\n\n\n\n\n\nshow code\ntm_shape(sarb_sf) +\n  tm_polygons() + \n  tm_shape(hexagon_sf_2) + \n  tm_fill(\"WEEKEND_EVENING_TRIPS\", \n          style = \"cont\", \n          palette = \"viridis\",\n          breaks = custom_breaks,\n          title = \"Dependency ratio\")\n\n\nWarning: The shape sarb_sf is invalid. See sf::st_is_valid"
  },
  {
    "objectID": "Take-home_Ex01/take_home_ex_01.html#plot-weekday-morning",
    "href": "Take-home_Ex01/take_home_ex_01.html#plot-weekday-morning",
    "title": "Take-home Exercise 1",
    "section": "Plot Weekday Morning",
    "text": "Plot Weekday Morning\n\n\nshow code\nmapview(hexagon_sf_2, zcol=\"WEEKDAY_MORNING_TRIPS\", cex = 3, alpha = .5, popup = NULL)"
  },
  {
    "objectID": "Take-home_Ex01/take_home_ex_01.html#plot-weekday-afternoons",
    "href": "Take-home_Ex01/take_home_ex_01.html#plot-weekday-afternoons",
    "title": "Take-home Exercise 1",
    "section": "Plot Weekday Afternoons",
    "text": "Plot Weekday Afternoons\n\n\nshow code\nmapview(hexagon_sf_2, zcol=\"WEEKDAY_AFTERNOON_TRIPS\", cex = 3, alpha = .5, popup = NULL)"
  },
  {
    "objectID": "Take-home_Ex01/take_home_ex_01.html#plot-weekend-morning",
    "href": "Take-home_Ex01/take_home_ex_01.html#plot-weekend-morning",
    "title": "Take-home Exercise 1",
    "section": "Plot Weekend Morning",
    "text": "Plot Weekend Morning\n\n\nshow code\nmapview(hexagon_sf_2, zcol=\"WEEKEND_MORNING_TRIPS\", cex = 3, alpha = .5, popup = NULL)"
  },
  {
    "objectID": "Take-home_Ex01/take_home_ex_01.html#plot-weekend-afternoons",
    "href": "Take-home_Ex01/take_home_ex_01.html#plot-weekend-afternoons",
    "title": "Take-home Exercise 1",
    "section": "Plot Weekend Afternoons",
    "text": "Plot Weekend Afternoons\n\n\nshow code\nmapview(hexagon_sf_2, zcol=\"WEEKEND_EVENING_TRIPS\", cex = 3, alpha = .5, popup = NULL)\n\n\n\n\n\n\n\n\n\n\n\n\nQuiz: What statistical conclusion can you draw from the output above?\n\n\n\n\nUsing both fixed- and adaptive-distance neigbours generates similar results; values are largely in agreement\nknn=8 feels like it’s not very logical, but maybe there’s not too much difference"
  },
  {
    "objectID": "Take-home_Ex01/take_home_ex_01.html#create-queen-contiguity-weight-matrix-hex",
    "href": "Take-home_Ex01/take_home_ex_01.html#create-queen-contiguity-weight-matrix-hex",
    "title": "Take-home Exercise 1",
    "section": "Create Queen contiguity weight matrix hex",
    "text": "Create Queen contiguity weight matrix hex\n\n\nshow code\nwm_hex &lt;- st_contiguity(hexagon_sf_2, queen=TRUE)\nsummary(wm_hex)\n\n\nNeighbour list object:\nNumber of regions: 1524 \nNumber of nonzero links: 6880 \nPercentage nonzero weights: 0.2962228 \nAverage number of links: 4.514436 \n10 regions with no links:\n307 565 730 984 1051 1419 1509 1512 1516 1524\nLink number distribution:\n\n  0   1   2   3   4   5   6 \n 10  39 112 205 291 364 503 \n39 least connected regions:\n1 7 22 38 98 169 187 195 211 218 258 259 264 267 287 457 566 611 646 700 712 736 755 788 873 1025 1026 1050 1090 1218 1468 1475 1486 1504 1505 1507 1510 1514 1523 with 1 link\n503 most connected regions:\n10 13 16 17 24 25 31 35 42 43 48 53 55 60 63 67 73 77 80 81 84 85 87 88 91 92 97 102 107 111 117 121 127 133 140 141 143 148 149 150 154 155 156 157 163 164 165 173 174 175 183 184 185 191 192 193 194 200 201 202 205 206 207 208 216 229 239 243 244 246 257 266 271 278 279 283 284 291 292 298 300 301 302 304 310 311 313 314 317 322 325 326 328 338 339 340 341 344 353 356 364 369 391 392 401 403 404 408 415 419 424 426 432 438 439 440 442 445 453 454 455 464 470 471 472 473 477 481 484 485 489 493 497 498 500 506 507 511 517 518 521 522 527 533 538 543 547 552 553 554 556 560 562 568 572 577 578 580 581 585 594 595 598 602 603 608 609 613 619 623 628 630 637 640 641 642 652 653 654 658 659 661 662 663 673 674 675 681 684 685 686 691 692 694 695 704 705 708 709 710 717 720 721 728 731 732 733 744 745 759 761 762 764 775 778 779 780 781 786 787 791 792 793 796 797 798 799 803 804 810 811 814 815 816 817 823 824 827 828 829 834 835 836 845 847 848 850 851 852 854 855 856 857 858 864 867 869 870 871 875 876 880 881 882 884 885 886 888 889 891 892 895 897 900 903 906 909 910 914 918 923 925 930 931 932 934 935 939 941 947 948 949 950 951 952 958 962 963 966 967 972 973 975 976 977 981 988 989 990 991 992 994 1000 1001 1002 1003 1008 1015 1016 1017 1018 1028 1029 1030 1032 1033 1040 1041 1042 1046 1054 1055 1058 1060 1061 1066 1067 1068 1070 1071 1072 1073 1080 1082 1083 1084 1087 1093 1097 1104 1105 1106 1109 1110 1114 1115 1121 1124 1125 1126 1132 1137 1138 1139 1140 1145 1146 1148 1149 1150 1151 1152 1154 1160 1161 1162 1166 1167 1168 1170 1173 1174 1175 1176 1180 1181 1182 1183 1184 1188 1190 1194 1195 1196 1197 1198 1205 1206 1207 1208 1209 1210 1211 1214 1215 1221 1222 1223 1224 1225 1231 1237 1238 1239 1243 1248 1249 1255 1257 1258 1259 1265 1269 1270 1275 1276 1277 1281 1285 1287 1293 1303 1305 1306 1307 1308 1320 1322 1328 1329 1330 1331 1333 1334 1335 1338 1339 1340 1341 1347 1348 1349 1356 1357 1359 1360 1365 1369 1370 1372 1373 1375 1376 1381 1384 1385 1386 1388 1392 1395 1397 1399 1402 1410 1412 1416 1421 1422 1424 1428 1429 1430 1431 1432 1437 1438 1439 1440 1444 1445 1446 1450 1451 1453 1455 1457 1460 1461 1463 1464 1465 1466 1473 with 6 links"
  },
  {
    "objectID": "Take-home_Ex01/take_home_ex_01.html#create-queen-contiguity",
    "href": "Take-home_Ex01/take_home_ex_01.html#create-queen-contiguity",
    "title": "Take-home Exercise 1",
    "section": "Create Queen contiguity",
    "text": "Create Queen contiguity\n\n\nshow code\nhex_no_nb &lt;- c(307, 565, 730, 984, 1051, 1419, 1509, 1512, 1516, 1524)\nhexagon_sf_2_drop_nonb &lt;- hexagon_sf_2 %&gt;%\n  mutate(RowNumber = row_number()) %&gt;%\n  subset( !(RowNumber  %in% hex_no_nb))\n\nvis_excluded_hexes &lt;- tm_shape(sarb_sf) +\n  tm_polygons() + \n  tm_shape(hexagon_sf_2) + \n  tm_fill(\"red\") + \n  tm_shape(hexagon_sf_2_drop_nonb) + \n  tm_fill(\"green\")\nvis_excluded_hexes\n\n\nWarning: The shape sarb_sf is invalid. See sf::st_is_valid"
  },
  {
    "objectID": "Take-home_Ex01/take_home_ex_01.html#drop-jb-bus-stop",
    "href": "Take-home_Ex01/take_home_ex_01.html#drop-jb-bus-stop",
    "title": "Take-home Exercise 1",
    "section": "Drop JB Bus Stop",
    "text": "Drop JB Bus Stop\n\nFind Northernmost point: JB Bus stop\n\n\nshow code\ncoordinates &lt;- st_coordinates(raw_bus_stop_sf$geometry)\n\n# Find the index of the northernmost point\nnorthernmost_index &lt;- which.max(coordinates[, 2])\n\n# Extract the northernmost point\nnorthernmost_point &lt;- raw_bus_stop_sf[northernmost_index, ]\nnorthernmost_point # 46239\n\n\nSimple feature collection with 1 feature and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 17969.14 ymin: 52983.82 xmax: 17969.14 ymax: 52983.82\nProjected CRS: SVY21 / Singapore TM\n     BUS_STOP_N BUS_ROOF_N   LOC_DESC                  geometry\n1452      46239         NA LARKIN TER POINT (17969.14 52983.82)\n\n\n\n\nIdentify the hex to drop\n\n\nshow code\njb_grid_id &lt;- pull(bus_stop_hexgrid_id[bus_stop_hexgrid_id$BUS_STOP_N == 46239, 'grid_id']) #1767\nhexagon_sf_2_drop_jb &lt;- \n  subset(hexagon_sf_2, !(grid_id == jb_grid_id))\n\n\n\n\nNow we figure out distance; first find centroid\n\n\nshow code\nlongitude &lt;- map_dbl(hexagon_sf_2_drop_jb$raw_hex_grid, ~st_centroid(.x)[[1]])\nlatitude &lt;- map_dbl(hexagon_sf_2_drop_jb$raw_hex_grid, ~st_centroid(.x)[[2]])\ncoords &lt;- cbind(longitude, latitude)\n# cat(\"Printing first 6 rows of `coords`:\\n\")\n# head(coords)\n\n\nk1_nn_obj &lt;- st_knn(coords, k = 1)\nk1dists &lt;- unlist(st_nb_dists(coords, k1_nn_obj))\nsummary(k1dists)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  500.0   500.0   500.0   503.4   500.0  2291.3 \n\n\n\n2km is a huge distance – we would enmesh hexes to about 4 hexes away\nbus stops are\n\n\n\nshow code\ncheck_dist &lt;- st_nb_dists(coords, k1_nn_obj)\nwhich.max(check_dist) # 1523\n\n\n[1] 1523\n\n\nshow code\ntm_shape(sarb_sf) +\n  tm_polygons() + \n  tm_shape(hexagon_sf_2_drop_jb) + \n  tm_fill(\"cyan\") + \n  tm_shape(hexagon_sf_2_drop_jb[1523,]) + \n  tm_fill(\"orange\")\n\n\nWarning: The shape sarb_sf is invalid. See sf::st_is_valid\n\n\n\n\n\n\n\nFind Northernmost point: Changi Naval Base Point\n\n\nshow code\ncoordinates &lt;- st_coordinates(raw_bus_stop_sf$geometry)\n\n\n\n# Find the index of the northernmost point\neasternmost_index &lt;- which.max(coordinates[, 1])\n\n# Extract the northernmost point\neasternmost_point &lt;- raw_bus_stop_sf[easternmost_index, ]\neasternmost_point # 96439        \n\n\nSimple feature collection with 1 feature and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 48284.56 ymin: 33326.57 xmax: 48284.56 ymax: 33326.57\nProjected CRS: SVY21 / Singapore TM\n     BUS_STOP_N BUS_ROOF_N          LOC_DESC                  geometry\n3063      96439        B04 CHANGI NAVAL BASE POINT (48284.56 33326.57)\n\n\n\n\nIdentify the hex to drop\n\n\nshow code\nnaval_base_grid_id &lt;- pull(bus_stop_hexgrid_id[bus_stop_hexgrid_id$BUS_STOP_N == 96439, 'grid_id']) #1767\nhexagon_sf_2_drop_changi &lt;- \n  subset(hexagon_sf_2_drop_jb, !(grid_id == naval_base_grid_id))\n\nlongitude_2 &lt;- map_dbl(hexagon_sf_2_drop_changi$raw_hex_grid, ~st_centroid(.x)[[1]])\nlatitude_2 &lt;- map_dbl(hexagon_sf_2_drop_changi$raw_hex_grid, ~st_centroid(.x)[[2]])\ncoords_2 &lt;- cbind(longitude_2, latitude_2)\nk1_nn_obj_2 &lt;- st_knn(coords_2, k = 1)\nk1dists_2 &lt;- unlist(st_nb_dists(coords_2, k1_nn_obj_2))\nsummary(k1dists_2)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  500.0   500.0   500.0   502.2   500.0  1000.0 \n\n\n\nA much more manageable 1km; two hexes away. Let’s work with this.\n\n\n\nNow we figure out distance; first find centroid\n\n\nshow code\nhex_1km_nb &lt;- st_dist_band(coords_2, lower = 0, upper = 1000.1)\nhead(hex_1km_nb, 5)\n\n\n[[1]]\n[1] 2 4\n\n[[2]]\n[1] 1 4 8\n\n[[3]]\n[1]  5  6  9 10 13 14 17\n\n[[4]]\n[1]  1  2  8 12 22\n\n[[5]]\n [1]  3  6  9 10 12 13 14 16 17 24\n\n\nshow code\nhex_1km_wt &lt;- st_weights(hex_1km_nb)\n\nhead(hex_1km_wt, 5)\n\n\n[[1]]\n[1] 0.5 0.5\n\n[[2]]\n[1] 0.3333333 0.3333333 0.3333333\n\n[[3]]\n[1] 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571\n\n[[4]]\n[1] 0.2 0.2 0.2 0.2 0.2\n\n[[5]]\n [1] 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1\n\n\n\n\nCalculate Local Moran’s I\n\n\nshow code\nlocalMI_day_morn &lt;- local_moran(hexagon_sf_2_drop_changi$WEEKDAY_MORNING_TRIPS , hex_1km_nb, hex_1km_wt)\n\nhexagon_sf_local_moran &lt;- cbind(hexagon_sf_2_drop_changi,localMI_day_morn) \n\n\n\n\nIdentify the hex to drop\n\n\nshow code\nlocal_moran_stats &lt;- tm_shape(sarb_sf) +\n  tm_polygons() +\n  tm_shape(hexagon_sf_local_moran) +\n  tm_fill(col = \"ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"Local Moran Statistics\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran Statistics\")\n\nlocal_moran_p &lt;- tm_shape(sarb_sf) +\n  tm_polygons() +\n  tm_shape(hexagon_sf_local_moran) +\n  tm_fill(col = \"p_ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Greens\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran's I p-values\")\n\ntmap_arrange(local_moran_stats, \n             local_moran_p, \n             asp=1, \n             ncol=2)\n\n\nWarning: The shape sarb_sf is invalid. See sf::st_is_valid\n\n\nVariable(s) \"ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\nWarning: The shape sarb_sf is invalid. See sf::st_is_valid\n\n\n\n\n\n\nBut we only want to see the ones with p &lt; 0.05, so we need to do some cleaning\n\n\n\nshow code\nhexagon_sf_local_moran_sig_only &lt;- hexagon_sf_local_moran %&gt;%\n  subset( !(p_ii  &lt; 0.05))\n\ntm_shape(sarb_sf) +\n  tm_polygons() +\n  tm_shape(hexagon_sf_local_moran) +\n  tm_fill(\"darkgrey\") +\n  tm_shape(hexagon_sf_local_moran_sig_only) +\n  tm_fill(col = \"ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"Local Moran Statistics\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran Statistics\")\n\n\nWarning: The shape sarb_sf is invalid. See sf::st_is_valid\n\n\nVariable(s) \"ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\nMoran Scatterplot\n\n\nshow code\n# Create lag\n\n\nhexagon_sf_local_moran &lt;- hexagon_sf_local_moran %&gt;%\n  mutate(trips_lag = st_lag(hexagon_sf_local_moran$WEEKDAY_MORNING_TRIPS, hex_1km_nb, hex_1km_wt))\n\nggplot(hexagon_sf_local_moran, aes(WEEKDAY_MORNING_TRIPS, trips_lag)) +\n  geom_point() +\n  geom_vline(xintercept = mean(hexagon_sf_local_moran$WEEKDAY_MORNING_TRIPS), linetype = \"dashed\", color = \"red\", size = 1.2) +\n  geom_hline(yintercept = mean(hexagon_sf_local_moran$trips_lag), linetype = \"dashed\", color = \"red\", size = 1.5) +\n  labs(title = \"Scatterplot Example\", x = \"X-axis\", y = \"Y-axis\")\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\nPlot Local Moran’s\n\n\nshow code\nhexa_sig &lt;- hexagon_sf_local_moran  %&gt;%\n  filter(p_ii_sim &lt; 0.05)\n\n\ntmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nshow code\ntm_shape(sarb_sf) +\n  tm_polygons() + \ntm_shape(hexagon_sf_local_moran) +\n  tm_fill(\"mean\") +\n  tm_borders(alpha = 0.5)\n\n\nWarning: The shape sarb_sf is invalid. See sf::st_is_valid"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html",
    "title": "In-class Exercise 3: Spatial",
    "section": "",
    "text": "Hidden: In-Class Lecture notes\n\n\n\n\n\n\n“In real world, there is no concept of equality.” – Prof Kam\n\n### Take-home Exercise notes:\n- drop_NA() outside of region \n- do not save map as object class; will take very long\n    - e.g. do not do map_name &lt;- tmap_(...)\n    - instead just call and render directly: tm_shape(...)    \n\n### Dependency Ratio Plot\n\n-   First: check for statistical significance,\n-   Then, plot/test for null hypothesis == normally distributed\n\n-   In geospatial, we don't test for `normality`, we test for `spatial randomness` : whatever is observed in space is randomly distributed\n    -   Similar to default assumption that distributions are Normal, default assumption\n    -   Worth checking; not all\n\n-   `Dependency Ratio`: Number of non-working people / Number of working people\n    - High chance that dependency ratio is not randomly distributed;\n    - Socio-economic factors (eg private homes, \"good\" schools, workers)\n    - SES-locales can affect crime rates, electricity consumption, \n    =&gt; Concept of `Spatial Inequality`: This world is not equal\n\n\n### Tobler's First Law of Geography: \n\nEverything is related to everything else, but near things are more related than distant things\n\n\n### Spatial Weights\n\n- Spatial weights help to parse spatial dependency\n- Use of `adjacency` or `neighbours` as a frame to examine dependency\n    - Neighbourhood search: neighbours or not neighbours\n    \n- How do we qualify 'neighbourliness'?\n    - ADJACENCY - If vertex/edges touching, then neighbours\n    - DISTANCE  - If within certain distance, then neighbours\n        - binary, (1/0) neighbourhood stats\n    - INVERSE DISTANCE  - 1/distance\n        - gives higher weightage to nearer neighbours; Tobler's\n\n- In the real world, not everything is adjacency -- islands are not touching\n    - If using hex-based map, also \n    - Tobler's first law: not touching but also has direct effects\n- Use of hexagon binning: MODIFIED AREA UNIT\n    - Planning Subzone has huge irregularities e.g. large areas (deployment area), oddly shaped long areas etc\n    - Hexagons are one of most compact tessellating shapes\n    - Modern city design work uses grids -- rare, not very usual (eg Punggol) \n\n\n- QUEEN'S CONTIGUITY: \"So long as they touch\"; any point on edge is vertex\n\n- LAGGED CONTINUITY\n    - Lag 1: all adjacent neighbours\n    - Lag 2: adjacent neighbours' neighbours PLUS Lag 1;\n- All levels of lag include previous levels;\n    - Hence, see correllogram: influence gradually wanes over distance\n- EXAMPLE: Where to put Pizza Hut to minimise drive time, maximize food quality\n    - Look at real estate to identify largest expected market share, penetration\n    \n- If region is considered water or unnecessary, region can be dropped as rows\n    - Has to be excluded from consideration, otherwise will have geometric errors\n    - Will also mess with Moran's I calculations\n    \n    \n- Use of ROW-STANDARDISED WEIGHTS to avoid bias\n    - Since weights matrix is a matrix, diagonals are zero, symmetric about diagonal\n    - Also possible to calculate COLUMN-STANDARDISED METRICS with similar effect\n\n- GLOBAL Measure are for mathematical understanding;\n- LOCAL measures are more interesting\n\n- GDPPC in any regions will be unevenly spatially distributed\n    - Aside from capital, may have secondary city: second region of prosperity & development as growth centre\n    \n    \n### Spatial Dependency\n\n- When measuring rainfall, not every region has a rainfall station. How do we do this for all region?\n    - Use interpolation for uncovered regions + concept of Spatial Dependency\n    \n### Spatial Autocorrelation\n- Typical correlation compares 2 different features;\n- Spatial Autocorrelation compares one feature, of self vs neighbour\n    - Similar formula to correlation coefficient\n    \n- Spatial Autocorrelation tests (Moran's I, Geary's C) tries to reject the null hypothesis that \"things are randomly distributed in space\"\n    - Positive Spatial Autocorrelation: Pattern of clustering, e.g. high is found near to high \n    - Negative Spatial Autocorrelation does not mean randomness! Instead, it follows some ordered pattern (e.g. checkerboard -- high surrounds low, low surrounds high); searching for outliers where different from surroundings\n    - Spatial Autocorrelation suggests non-random patterns \n    \n- in Spatial Data, go directly to monte carlo simulation\n\n\n### LISA\n- Helps to identify clusters (high-high, low-low) and outliers (high-low, low-high) ==&gt; regions of positive/negative spatial autocorrelation\n    - hence quadrant 5, \"none\"\n- For all tests, we only highlight Spatial Autocorrelation AND high statistical significance\n\n- Why might region not be statistically significant?\n    - Not enough neighbours; test is uncertain\n    - Enough neighbours, but data just not stat-significant\n    \n    \n### Gi*\n- For Gi*, should always be distance-based;\n    - Moran's I, Geary's C can be contiguity or distance-based; but Getis-Ord should always be distance-based\n    - Evolved from work on dengue transmissions; hence distance is more valuable\n- Gi does not count itself, Gi* counts itself\n\n\n### Mann-Kendall \n- Useful for time-series data;\n    - Hunan GDPPC is single-instance, but most effects happen over space and time; thus, it's useful to consider time \n- A \"sign\" statistic:\n    - compares value at time j vs at time k;\n    - outputs either +1 if decreasing, -1 if increasing, or 0 if no change\n    - does not consider output, only test for monotonicity and \"interprets the relationship\"\n    - cannot have breaks in timeseries; eg if COVID, add datapoint of 0 to keep it going\n- Note: this is nonspatial, only statistical\n\n\n### Emerging Hotspot EHSA\n- Space & time analysis improvement over Mann-Kendall\n- Instead using Gi* instead of value\n    - Compares Gi* over time -- increasing or decreasing\n    \n- Need to transform data into SPACETIME CUBE\n    - x, y dimensions of attribute a\n    - z-dimension of time \n    - `sfdep` allows construction of spacetime cube"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#in-class-lecture-notes",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#in-class-lecture-notes",
    "title": "In-class Exercise 3: Spatial",
    "section": "",
    "text": "“In real world, there is no concept of equality.” – Prof Kam\n\n### Take-home Exercise notes:\n- drop_NA() outside of region \n- do not save map as object class; will take very long\n    - e.g. do not do map_name &lt;- tmap_(...)\n    - instead just call and render directly: tm_shape(...)    \n\n### Dependency Ratio Plot\n\n-   First: check for statistical significance,\n-   Then, plot/test for null hypothesis == normally distributed\n\n-   In geospatial, we don't test for `normality`, we test for `spatial randomness` : whatever is observed in space is randomly distributed\n    -   Similar to default assumption that distributions are Normal, default assumption\n    -   Worth checking; not all\n\n-   `Dependency Ratio`: Number of non-working people / Number of working people\n    - High chance that dependency ratio is not randomly distributed;\n    - Socio-economic factors (eg private homes, \"good\" schools, workers)\n    - SES-locales can affect crime rates, electricity consumption, \n    =&gt; Concept of `Spatial Inequality`: This world is not equal\n\n\n### Tobler's First Law of Geography: \n\nEverything is related to everything else, but near things are more related than distant things\n\n\n### Spatial Weights\n\n- Spatial weights help to parse spatial dependency\n- Use of `adjacency` or `neighbours` as a frame to examine dependency\n    - Neighbourhood search: neighbours or not neighbours\n    \n- How do we qualify 'neighbourliness'?\n    - ADJACENCY - If vertex/edges touching, then neighbours\n    - DISTANCE  - If within certain distance, then neighbours\n        - binary, (1/0) neighbourhood stats\n    - INVERSE DISTANCE  - 1/distance\n        - gives higher weightage to nearer neighbours; Tobler's\n\n- In the real world, not everything is adjacency -- islands are not touching\n    - If using hex-based map, also \n    - Tobler's first law: not touching but also has direct effects\n- Use of hexagon binning: MODIFIED AREA UNIT\n    - Planning Subzone has huge irregularities e.g. large areas (deployment area), oddly shaped long areas etc\n    - Hexagons are one of most compact tessellating shapes\n    - Modern city design work uses grids -- rare, not very usual (eg Punggol) \n\n\n- QUEEN'S CONTIGUITY: \"So long as they touch\"; any point on edge is vertex\n\n- LAGGED CONTINUITY\n    - Lag 1: all adjacent neighbours\n    - Lag 2: adjacent neighbours' neighbours PLUS Lag 1;\n- All levels of lag include previous levels;\n    - Hence, see correllogram: influence gradually wanes over distance\n- EXAMPLE: Where to put Pizza Hut to minimise drive time, maximize food quality\n    - Look at real estate to identify largest expected market share, penetration\n    \n- If region is considered water or unnecessary, region can be dropped as rows\n    - Has to be excluded from consideration, otherwise will have geometric errors\n    - Will also mess with Moran's I calculations\n    \n    \n- Use of ROW-STANDARDISED WEIGHTS to avoid bias\n    - Since weights matrix is a matrix, diagonals are zero, symmetric about diagonal\n    - Also possible to calculate COLUMN-STANDARDISED METRICS with similar effect\n\n- GLOBAL Measure are for mathematical understanding;\n- LOCAL measures are more interesting\n\n- GDPPC in any regions will be unevenly spatially distributed\n    - Aside from capital, may have secondary city: second region of prosperity & development as growth centre\n    \n    \n### Spatial Dependency\n\n- When measuring rainfall, not every region has a rainfall station. How do we do this for all region?\n    - Use interpolation for uncovered regions + concept of Spatial Dependency\n    \n### Spatial Autocorrelation\n- Typical correlation compares 2 different features;\n- Spatial Autocorrelation compares one feature, of self vs neighbour\n    - Similar formula to correlation coefficient\n    \n- Spatial Autocorrelation tests (Moran's I, Geary's C) tries to reject the null hypothesis that \"things are randomly distributed in space\"\n    - Positive Spatial Autocorrelation: Pattern of clustering, e.g. high is found near to high \n    - Negative Spatial Autocorrelation does not mean randomness! Instead, it follows some ordered pattern (e.g. checkerboard -- high surrounds low, low surrounds high); searching for outliers where different from surroundings\n    - Spatial Autocorrelation suggests non-random patterns \n    \n- in Spatial Data, go directly to monte carlo simulation\n\n\n### LISA\n- Helps to identify clusters (high-high, low-low) and outliers (high-low, low-high) ==&gt; regions of positive/negative spatial autocorrelation\n    - hence quadrant 5, \"none\"\n- For all tests, we only highlight Spatial Autocorrelation AND high statistical significance\n\n- Why might region not be statistically significant?\n    - Not enough neighbours; test is uncertain\n    - Enough neighbours, but data just not stat-significant\n    \n    \n### Gi*\n- For Gi*, should always be distance-based;\n    - Moran's I, Geary's C can be contiguity or distance-based; but Getis-Ord should always be distance-based\n    - Evolved from work on dengue transmissions; hence distance is more valuable\n- Gi does not count itself, Gi* counts itself\n\n\n### Mann-Kendall \n- Useful for time-series data;\n    - Hunan GDPPC is single-instance, but most effects happen over space and time; thus, it's useful to consider time \n- A \"sign\" statistic:\n    - compares value at time j vs at time k;\n    - outputs either +1 if decreasing, -1 if increasing, or 0 if no change\n    - does not consider output, only test for monotonicity and \"interprets the relationship\"\n    - cannot have breaks in timeseries; eg if COVID, add datapoint of 0 to keep it going\n- Note: this is nonspatial, only statistical\n\n\n### Emerging Hotspot EHSA\n- Space & time analysis improvement over Mann-Kendall\n- Instead using Gi* instead of value\n    - Compares Gi* over time -- increasing or decreasing\n    \n- Need to transform data into SPACETIME CUBE\n    - x, y dimensions of attribute a\n    - z-dimension of time \n    - `sfdep` allows construction of spacetime cube"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#overview",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#overview",
    "title": "In-class Exercise 3: Spatial",
    "section": "Overview",
    "text": "Overview\n\nSee also In-Class Exercise 2: GLSA and In-Class Exercise 2: EHSA"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#getting-started---import-packages",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#getting-started---import-packages",
    "title": "In-class Exercise 3: Spatial",
    "section": "Getting Started - Import packages",
    "text": "Getting Started - Import packages\nThis function calls pacman to load sf, tidyverse, tmap, knitr packages;\n\ntmap : For thematic mapping; powerful mapping package\nsf : for geospatial data handling, but also geoprocessing: buffer, point-in-polygon count, etc\n\nbatch processing over GIS packages; can handle tibble format\n\nsfdep : creates space-time cube, EHSA; replaces spdep\ntidyverse : for non-spatial data handling; commonly used R package\nknitr : generates html table\n\n\npacman::p_load(tmap, sf, sfdep, tidyverse, knitr)"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#loading-the-data",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#loading-the-data",
    "title": "In-class Exercise 3: Spatial",
    "section": "Loading the data",
    "text": "Loading the data\n\nHunan: geospatial dataset in ESRI shapefile format\n\nuse of st_read() to import assf data.frame\n\n$geometry column is actually a list inside the df cell; that’s the power of the tibble dataframe\n“features” of simple features refers to geometric features eg point line curve etc\n\nnote projection is WGS84; see `88\n\nhunan2012: attribute format in csv format\n\nuse of read_csv() astbl_df data.frame\n\n!IMPORTANT! to retain geometry, you must left join to the sf dataframe (eg you can also hunan2012 right join hunan)\n\nwithout sf dataframe, normal tibble dataframe will drop the geometry column\n\n\n\n\nshow code\nhunan &lt;- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\n\nReading layer `Hunan' from data source \n  `C:\\1darren\\ISSS624\\In-class_Ex\\In-class_Ex3\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\n\nshow code\nhunan2012 &lt;- read_csv(\"data/aspatial/Hunan_2012.csv\")\nhunan_GDPPC &lt;- left_join(hunan,hunan2012)%&gt;%\n  select(1:4, 7, 15)\n\n\n\nPlot a chloropleth of GDPPC\n\n\nshow code\n#qtm(hunan, \"GDPPC\") +\n#  tm_layout(main.title = \"GDPPC\", main.title.position = \"right\")\n\n\ntm_shape(hunan_GDPPC) +\n  tm_fill(col = \"GDPPC\", \n          style = \"pretty\",\n          palette = \"Blues\",\n          title = \"GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC\",\n            inner.margins = c(0.1, 0.1, 0.1, 0.1),\n            outer.margins = c(0.1, 0.1, 0.1, 0.1)\n            ) + \n  tm_grid(alpha = )"
  },
  {
    "objectID": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#deriving-queen-contiguity-weights",
    "href": "In-class_Ex/In-class_Ex3/In-class_Ex3.html#deriving-queen-contiguity-weights",
    "title": "In-class Exercise 3: Spatial",
    "section": "Deriving QUEEN contiguity weights",
    "text": "Deriving QUEEN contiguity weights\n\nmutate is function that creates new column from previous column datas\n\nst_contiguity creates nb neighbour matrix (QUEEN contiguity, by default)\nst_weights creates row-standardised weights (style=\"W\") from nb object\nOne-step function using sfdep; a wrapper for spdep but writes output into sf dataframe\n\n\n\n\nshow code\nwm_q &lt;- hunan_GDPPC %&gt;%\n  mutate(nb = st_contiguity(geometry), \n         wt = st_weights(nb, style = \"W\"),\n         .before = 1)"
  }
]