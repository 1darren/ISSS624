---
title: "Take-home Exercise 1"
editor: source
---

# 0. Goals & Objectives.

On this page, we define Exploratory Spatial Data Analysis (ESDA) to understand spatial and spatio-temporal mobility patterns of public bus passengers in Singapore.

More detail about the task from: <https://isss624-ay2023-24nov.netlify.app/take-home_ex01>

::: callout-important
## Important Note

Due to the nature of EDA and Data Analysis, parts of this page have been Collapsed or placed behind tabs, to avoid excessive scrolling.

For easier reading, this study is also presented in point-form.
:::

## 0.1 Motivation

Public transport is a key concern for residents in land-scarce, population-dense Singapore. With [COEs](https://onemotoring.lta.gov.sg/content/onemotoring/home/buying/upfront-vehicle-costs/certificate-of-entitlement--coe-.html) reaching record highs and [authorities announcing the termination of bus services](https://www.todayonline.com/singapore/bus-service-167-30-minute-breaks-commuters-lta-2314221), there is no better time to understand the public transport network and systems in Singapore.

By understanding and modelling patterns of public transport consumption - including specifically Local Indicators of Spatial Association (LISA), insights can be provided to both public and private sector to formulate more informed decisions that for the benefit of the public, and not just for profit.

# 1. Geospatial Data Wrangling

This study was performed in R, written in R Studio, and published using Quarto.

## 1.1 Import Packages

This function calls `pacman` to load sf, tidyverse, tmap, knitr packages;

-   `tmap` : For thematic mapping; powerful mapping package;
-   `sf` : for geospatial data handling, but also geoprocessing: buffer, point-in-polygon count, etc;
-   `sfdep` : useful functions for creating weight matrix, LISA calculations etc;
-   `tidyverse` : for non-spatial data handling; commonly used R package and contains `dplyr` for dataframe manipulation and `ggplot` for data visualization;
-   `DT`: for displaying datatables;
-   `leaflet`: for custom layer controls over `tmap` visualisations.

```{r}
pacman::p_load(tmap, sf, sfdep, tidyverse, DT, leaflet)
```

## 1.2 Import Data

### 1.2.1 Load Geospatial Bus Stop Data

-   First, we load `BusStop` shapefile data from [LTA Datamall](https://datamall.lta.gov.sg/content/datamall/en/dynamic-data.html);
-   `st_read()` is used to import the ESRI Shapefile data into an `sf` dataframe.

```{r}
#| code-fold: true
#| code-summary: "show code"
raw_bus_stop_sf <- st_read(dsn = "data/geospatial", 
                 layer = "BusStop") 
head(raw_bus_stop_sf)
```

-   We check the coordinate reference system with `st_crs()`;

::: {.callout-warning collapse="true"}
## EXPAND To See: Full `st_crs`Readout

```{r}
#| code-fold: true
#| code-summary: "show code"

st_crs(raw_bus_stop_sf)
```
:::

-   We see that although the projection is SVY21, the CRS (coordinate reference system) is EPSG 9001, which is incorrect;
-   We thus use `st_set_crs()` to set it to 3414, which is the [EPSG code for SVY21](https://epsg.io/3414);
    -   we then use `st_crs()` to confirm it is the correct EPSG code.

::: {.callout-warning collapse="true"}
## EXPAND To See: `st_crs` Readout after `st_transform`

```{r}
#| code-fold: true
#| code-summary: "show code"

raw_bus_stop_sf <- st_set_crs(raw_bus_stop_sf, 3414)

st_crs(raw_bus_stop_sf)
```
:::

-   We now do a quick plot to quickly visualize the bus stops;
    -   `qtm()` is a wrapper to do a quick, simple plot using `tmap` without defining arguments;
    -   we use `tmap_mode("plot")` to set the map as a static image, rather than as an interactive map, in order to save processing time.

```{r}
#| code-fold: true
#| code-summary: "show code"

tmap_mode("plot")
qtm(raw_bus_stop_sf)

```

### 1.2.2 Visualizing Within Singapore's Administrative National Boundary

-   This image is hard to read; we can vaguely discern Singapore's Southern coastline, but it can be hard to visualize.
-   I have sourced and downloaded supplemental data about Singapore's Administrative National Boundary ("SANB") from [igismap.com](https://map.igismap.com/gis-data/singapore/administrative_national_boundary) as a base layer for visualization;
    -   We set `tmap_mode("view")` to allow us to scroll and confirm the SARB boundaries;
    -   As before, we use `st_read()` to load the shapefile data, and `st_transform()` to ensure the projection is correct;
        -   We use `tm_shape() + tm_polygons()` to map a grey layer of the SARB boundaries;
        -   On top of which, we layer `tm_shape() + tm_dots()` to show the bus stops.

```{r}
#| code-fold: true
#| code-summary: "show code"

sanb_sf <- st_read(dsn = "data/geospatial", 
                 layer = "singapore_administrative_national_boundary") %>%
  st_transform(crs = 3414)

tmap_mode("view")
tm_shape(sanb_sf) +
  tm_polygons() + 
  tm_shape(raw_bus_stop_sf) + 
  tm_dots()

## Additional data from: Data.gov.sg, https://beta.data.gov.sg/datasets/d_02cba6aeeed323b5f6c723527757c0bc/view
```

::: callout-note
## Does The Map Look Skewed To The Right?

That's because the Singapore National Administrative Boundaries map includes [Pedra Branca](https://en.wikipedia.org/wiki/Pedra_Branca,_Singapore), the much-disputed outlying island and easternmost point of Singapore.

It's an amusing artifact here, but will not be involved in further analysis later.
:::

-   We note there are a number of bus stops outside Singapore's boundaries; Specifically, three bus stops in a cluster just outside the Causeway, and one further North.
-   We perform several steps to isolate & check the data;
    -   we use `st_filter()` to find bus stops within Singapore's Administrative National Boundaries, and create `sg_bus_stop_sf` for future use.
    -   to check what bus stops have been dropped, we use `anti_join()` to compare `raw_bus_stop_sf` with `sg_bus_stop_sf`. We use `st_drop_geometry` on both `sf` dataframes to only compare the non-geometry columns.

```{r}
#| code-fold: true
#| code-summary: "show code"
sg_bus_stop_sf <- st_filter(raw_bus_stop_sf, sanb_sf)
anti_join(st_drop_geometry(raw_bus_stop_sf), st_drop_geometry(sg_bus_stop_sf))

```

-   We see there are in fact 5 bus stops outside of Singapore (including the defunct [Kotaraya II Terminal](https://landtransportguru.net/kotaraya-ii-bus-terminal/)) that have been removed, which means our data cleaning was correct.

## 1.3 Geospatial Data Cleaning

### 1.3.1 Removing Duplicate Bus Stops

-   But, do we need to do more data cleaning?
-   We use `length()` to find the total number of raw values in the `$BUS_STOP_N` column of `sg_bus_stop_sf;`
    -   We then compare this to `length(unique())` to find the unique values;
-   And, indeed, we find there are 16 bus stops that are repeated;

```{r}
cat("Total number of rows in sg_bus_stop_sf\t\t: ", paste0(length(sg_bus_stop_sf$BUS_STOP_N)))
cat("\nTotal unique bus stop IDs in sg_bus_stop_sf\t: ", paste0(length(unique(sg_bus_stop_sf$BUS_STOP_N))))
cat("\nRepeated bus stops\t\t\t\t:   ", paste0(length(raw_bus_stop_sf$BUS_STOP_N) - length(unique(raw_bus_stop_sf$BUS_STOP_N))))

```

-   It appears there are 16 datapoints that are specifically repeated; let's identify the bus stop numbers with repeated rows:
    -   First we use `filter()` with a pipe mark (using `or` condition) to identify repeated bus stop numbers (i.e. `$BUS_STOP_N`);
    -   We sort them in ascending order using `arrange()`; then, using `group_by()` and `row_number()` we add row numbers based on `$BUS_STOP_N` to a new column using `mutate()`.

```{r}
#| code-fold: true
#| code-summary: "show code"
repeated_df <- sg_bus_stop_sf %>%
  filter(duplicated(BUS_STOP_N) | duplicated(BUS_STOP_N, fromLast = TRUE)) %>% 
  arrange(BUS_STOP_N) %>%
  group_by(BUS_STOP_N) %>%
  mutate(RowNumber = row_number())

datatable(repeated_df)
```

-   From examination, there are 32 bus stops sharing 16 bus stop numbers -- 16 pairs of bus stops sharing the same number.
-   Let's examine these bus stop pairs on the map;
    -   we use `mapview()` to display these repeated bus stops on the map;
    -   we use `col="BUS_STOP_N"` with `palette="Spectral` to give each pair of bus stops an individual colour.

```{r}
#| code-fold: true
#| code-summary: "show code"

tmap_mode("view")
tm_shape(sanb_sf) +
  tm_polygons() + 
  tm_shape(repeated_df) + 
  tm_dots(col = "BUS_STOP_N", palette = "Spectral")
```

-   After confirming with Prof Kam, we will simply drop the second instance of the rows.
    -   we use `duplicated()` to identify rows with repeated values of `$BUS_STOP_N`; duplicated rows will return `TRUE` while all other rows will return `FALSE`
    -   We use `!` to invert the values, so only the unduplicated rows will return `TRUE`.
    -   We then use square brackets `[]` to index `sg_bus_stop_sf` based on the rows, and return only the unduplicated rows;

```{r}
#| code-fold: true
#| code-summary: "show code"
sg_bus_stop_sf <- sg_bus_stop_sf[!duplicated(sg_bus_stop_sf$BUS_STOP_N), ]
head(sg_bus_stop_sf)
```

::: {.callout-warning collapse="true"}
## EXPAND to see: Alternative Manual Cleaning Steps

I was unable to take Prof Kam's Data Analytics Lab, but I know of his fervour and attention to detail. I believe in informed choices, and so I performed manual cleaning with the following steps:

-   Remove the rows with lowercase names, as most `$LOC_DESC` are in strict uppercase

-   Remove the rows with `NA` `$LOC_DESC`

-   Remove the row with `NIL` `$LOC_DESC`

-   For remaining rows, drop second entry

-   Retain remaining rows

After this, we just run the same steps on `sg_bus_stop_sf` or perform an anti-join.

However, after clarification with Prof Kam, we just drop the second entry. My code is shown below for the gratification of those who may enjoy it.

```{r}
#| code-fold: true
#| code-summary: "show code"

drop_second_stop = c("43709", "51071", "53041", "52059", "58031", "68091", "68099", "97079")
rows_to_retain_df <- repeated_df %>%
  filter(
    case_when(
      BUS_STOP_N == "11009" & grepl("[a-z]", LOC_DESC) ~ FALSE,
      BUS_STOP_N == "22501" & grepl("[a-z]", LOC_DESC) ~ FALSE,
      BUS_STOP_N == "77329" & grepl("[a-z]", LOC_DESC) ~ FALSE,
      BUS_STOP_N == "82221" & grepl("[a-z]", LOC_DESC) ~ FALSE,
      BUS_STOP_N == "62251" & grepl("[a-z]", LOC_DESC) ~ FALSE,
      BUS_STOP_N == "96319" & grepl("[a-z]", LOC_DESC) ~ FALSE,

      BUS_STOP_N == "47201" & is.na(LOC_DESC) ~ FALSE,

      BUS_STOP_N == "67421" & BUS_ROOF_N == "NIL" ~ FALSE,
      BUS_STOP_N %in% drop_second_stop & RowNumber == 2 ~ FALSE,

      TRUE ~ TRUE
    )
  )

rows_to_retain_df$LOC_DESC  = toupper(rows_to_retain_df$LOC_DESC)

print("Printing rows to retain:")
rows_to_retain_df
```

-   If we run the steps from above, we can see that there are no repeated bus stops.

```{r}
cat("Total number of rows in sg_bus_stop_sf\t\t: ", paste0(length(sg_bus_stop_sf$BUS_STOP_N)))
cat("\nTotal unique bus stop IDs in sg_bus_stop_sf\t: ", paste0(length(unique(sg_bus_stop_sf$BUS_STOP_N))))
cat("\nRepeated bus stops\t\t\t\t:   ", paste0(length(sg_bus_stop_sf$BUS_STOP_N) - length(unique(sg_bus_stop_sf$BUS_STOP_N))))

```
:::

## 1.4 Generating Hexagon Maps

-   We use `st_make_grid()` with `square = FALSE` to create the `hexagon` layer as defined in the study, which we name `raw_hex_grid`;
    -   We pass `cellsize = 500` to create the hexagons of necessary size. Prof Kam defined the apothem as 250m, which means the distance between opposite edges is double that, or 500m;
        -   I used `units::as_units` to pass 500 metres into the argument. I am still uncertain whether a length of 500m needs to be reprojected, or whether we need to do any further transformation.
-   We use `st_sf()` to convert `raw_hex_grid` into an `sf` dataframe;
    -   `mutate()` is used here to create a `grid_id` column;
    -   We just use `st_transform()` in case we need to reproject the coordinate system, just in case.
-   However, trying to visualize this right now just gives us a map full of hexagons:

```{r}
#| code-fold: true
#| code-summary: "show code"


raw_hex_grid = st_make_grid(sg_bus_stop_sf, cellsize = units::as_units(500, "m"), what = "polygons", square = FALSE) %>%
  st_transform(crs = 3414)
# To sf and add grid ID
raw_hex_grid <- st_sf(raw_hex_grid) %>%
  # add grid ID
  mutate(grid_id = 1:length(lengths(raw_hex_grid))) %>%
  st_transform(crs = 3414)

tmap_mode("plot")
qtm(raw_hex_grid)

```

-   What we will need is to isolate only the hexagons with bus stops in them.
    -   We use `lengths(st_intersects())` to count the number of bus stops in each hexagon, and add that to a new column, `$n_bus_stops` ;
    -   We then create `hexagon_sf` by filtering for only hexagons with non-zero bus stops in each.
-   We then plot these using the usual `tmap()` functions;
    -   `tm_basemap()`is used to create a "basemap" layer underneath to orient our hexagons.
    -   We used [OneMapSG](https://www.onemap.gov.sg/) as our comprehensive map of Singapore; if you zoom in, you can actually count the number of bus stops in red on the map.

```{r}
#| code-fold: true
#| code-summary: "show code"


# Count number of points in each grid, code snippet referenced from: 
# https://gis.stackexchange.com/questions/323698/counting-points-in-polygons-with-sf-package-of-r

raw_hex_grid$n_bus_stops = lengths(st_intersects(raw_hex_grid, sg_bus_stop_sf))

# remove grid without value of 0 (i.e. no points inside that grid)
hexagon_sf = filter(raw_hex_grid, n_bus_stops > 0)
# head(hexagon_sf)

tmap_mode("view")
tm_basemap(providers$OneMapSG.Grey) + 
  tm_shape(hexagon_sf) +
  tm_fill(
    col = "n_bus_stops",
    palette = "-plasma",
    style = "cont",
    
    breaks = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12),
    title = "Number of bus_stops",
    id = "grid_id",
    showNA = FALSE,
    alpha = 0.6,
    popup.vars = c(
      "Number of Bus Stops: " = "n_bus_stops"
    ),
    popup.format = list(
      n_bus_stops = list(format = "f", digits = 0)
    )
  ) +
  tm_borders(col = "grey40", lwd = 0.7)


```

-   There seem to be 2 regions of deep purple, centred over an area near, but not exactly over Pasir Ris and Choa Chu Kang MRTs.
-   We perform some simple stats to count the total number of filtered hexagons, and to see the maximum number of bus stops in a hexagon.

```{r}
#| code-fold: true
#| code-summary: "show code"
cat(paste("Total number of raw hexagons is\t\t\t: ", nrow(raw_hex_grid), "\n"))
cat(paste("Total number of hexagons (n_bus_stop > 1) is\t: ", nrow(hexagon_sf)), "\n")

cat("\nPrinting map_hexagon_sf:\n >> ")
hexagon_sf[hexagon_sf$n_bus_stops > 10, ]
```

-   For the next step, we'll need to manage the aspatial bus trips dataset, which is what we'll work on now.

## 1.5 Aspatial Data Wrangling: Bus trip dataset

### 1.5.1 Import Bus O/D Dataset

-   For our purposes, we will focus only on 2023-October *Passenger Volume by Origin Destination Bus Stops*, downloaded from [LTA DataMall](https://datamall.lta.gov.sg/content/datamall/en.html);

-   We use `read_csv()` to load the data from the .csv file;

-   We use `select()` with a `-` sign to remove two columns redundant for our analysis:

    -   `$PT_TYPE` column indicates the type of public transport, however, every value is "BUS"
    -   `$YEAR_MONTH` column similarly has "2023-10" for every value, which we are aware of
    -   With this in mind, we drop these two columns to save space.

-   Finally, we do `as.factor()` to convert two columns (`$ORIGIN_PT_CODE` and `$DESTINATION_PT_CODE`)from character to factor, for easier analysis.

```{r}
#| code-fold: true
#| code-summary: "show code"

odbus_2310 <- read_csv("data/aspatial/origin_destination_bus_202310.csv")
odbus_2310 <- select(odbus_2310, -PT_TYPE, -YEAR_MONTH)
odbus_2310$ORIGIN_PT_CODE <- as.factor(odbus_2310$ORIGIN_PT_CODE)
odbus_2310$DESTINATION_PT_CODE <- as.factor(odbus_2310$DESTINATION_PT_CODE)

str(odbus_2310)
```

-   This is a huge `tibble` dataframe with over 5 million rows.

### 1.5.2 Filter For Peaks

-   We now perform a multi-step filter process;
    1.  We combine `mutate()` with `case_when()` to create a new column, `$PEAK`, based on the study criteria:
        a.  "WEEKDAY_MORNING_PEAK" if `$DAY_TYPE` is "WEEKDAY" and bus tap-on time (e.g. `$TIME_PER_HOUR`) is between 6 am and 9 am, inclusive;
        b.  "WEEKDAY_AFTERNOON_PEAK" if `$DAY_TYPE` is "WEEKDAY" and bus tap-on time (e.g. `$TIME_PER_HOUR`) is between 5 pm and 8pm, inclusive;
        c.  "WEEKEND_MORNING_PEAK" if `$DAY_TYPE` is "WEEKENDS/HOLIDAY" and bus tap-on time (e.g. `$TIME_PER_HOUR`) is between 11 am and 2pm, inclusive;
        d.  "WEEKDAY_AFTERNOON_PEAK" if `$DAY_TYPE` is "WEEKENDS/HOLIDAY" and bus tap-on time (e.g. `$TIME_PER_HOUR`) is between 4 pm and 7pm, inclusive;
        e.  Remaining values are assigned "Unknown" value.
    2.  We then use `filter()` to eliminate those with "Unknown" `$PEAK` values, i.e. rows outside the period of interest for the study
    3.  We use `group_by()` to group the values by `$ORIGIN_PT_CODE` and `$PEAK`, and use `summarise()` to aggregate the sum of `$TOTAL_TRIPS` as a new column, `$TRIPS`.
    4.  Finally, we use `pivot_wider()` to pivot our long table into wide format, where each column represents one of the peak periods of interest in our study, and the values are filled from `$TRIPS`. We use the `values_fill` argument to fill in a value of "0" if the bus stop has a missing value.
    5.  We use `write_rds()` to save the output dataframe, `odbus_filtered`, as an RDS object for future reference/load.

::: callout-note
-   Note that we convert the values for `$TIME_PER_HOUR` to 24-hour clock, e.g. "5pm" is "17" hundred hours, "8pm" is "20" hundred hours,.
-   We truncate "Weekend/Public Holiday" to "WEEKEND" for easier reading/reference.
:::

```{r}




odbus_filtered <- odbus_2310 %>%
  mutate(PEAK = case_when(
    DAY_TYPE == "WEEKDAY" & TIME_PER_HOUR >= 6 &  TIME_PER_HOUR <= 9 ~ "WEEKDAY_MORNING_PEAK",
    DAY_TYPE == "WEEKDAY" & TIME_PER_HOUR >= 17 &  TIME_PER_HOUR <= 20 ~ "WEEKDAY_AFTERNOON_PEAK",
    DAY_TYPE == "WEEKENDS/HOLIDAY" & TIME_PER_HOUR >= 11 &  TIME_PER_HOUR <= 14 ~ "WEEKEND_MORNING_PEAK",
    DAY_TYPE == "WEEKENDS/HOLIDAY" & TIME_PER_HOUR >= 16 &  TIME_PER_HOUR <= 19 ~ "WEEKEND_EVENING_PEAK",
    TRUE ~ "Unknown"
  )) %>%
  filter(
    case_when(
      PEAK == "Unknown" ~ FALSE,
      TRUE ~ TRUE
    )) %>%
  group_by(ORIGIN_PT_CODE, PEAK) %>%
  summarise(TRIPS = sum(TOTAL_TRIPS)) %>%
  pivot_wider(names_from = PEAK, values_from = TRIPS, values_fill = 0)


write_rds(odbus_filtered, "data/rds/odbus_filtered.rds")
head(odbus_filtered)
```

## 1.6 Combine Bus Trip Data With `hexagon_sf` Dataframe

-   For our study purposes, we need to have the number of bus trips originating from each hexagon. In order to achieve this, we must combine our three current dataframes:
    -   `hexagon_sf`, an `sf` dataframe with `$grid_id` column (primary key) and the specific polygon geometry of each hexagon;
    -   `sg_bus_stop_sf`, an `sf` dataframe with the `$BUS_STOP_N` (primary key) and the point geometry of each bus stop;
    -   `odbus_filtered`, a `tibble` dataframe with the `$ORIGIN_PT_CODE` (primary key) column and the trip details for each of the four peak periods of interest.

### 1.6.1 Identify Hexagon `grid_id` For Each Bus Stop

-   First, we combine `hexagon_sf` with `sg_bus_stop_sf` ;
    -   We use `st_intersection` to combine the dataframes such that each row of `sg_bus_stop_sf` has an associated `grid_id`;
    -   We use `select()` to filter the resultant `bus_stop_hexgrid_id` dataframe to only `$grid_id` and `$BUS_STOP_N` columns, and use `st_drop_geometry()` to convert into a simple dataframe with just two columns:

```{r}
#| code-fold: true
#| code-summary: "show code"

bus_stop_hexgrid_id <- st_intersection(sg_bus_stop_sf, hexagon_sf) %>%
  select(grid_id, BUS_STOP_N) %>%
  st_drop_geometry()

cat("\nNumber of bus stops\t:", length(unique(bus_stop_hexgrid_id$BUS_STOP_N)))
cat("\nNumber of hexgrids\t:", length(unique(bus_stop_hexgrid_id$grid_id)))

head(bus_stop_hexgrid_id)
```

### 1.6.2 Identify Bus Trip Details For Each Hexagon `grid_id`

-   Here, we again use multiple steps to generate bus trip details for each hexagon `grid_id`;
    1.  We use `left_join()` to add the `grid_id` to each row of `odbus_filtered`, since each row has a unique single bus stop ID (i.e. `$BUS_STOP_N`);
    2.  We use `select()` to retain only the `grid_id` and the four peak-trips columns;
    3.  We combine `group_by()` and `summarise()` to aggregate the trips for each peak by `grid_id`.
-   Finally, we use `head()` to preview the `grid_trips_df` tibble dataframe.

```{r}
#| code-fold: true
#| code-summary: "show code"

colnames(odbus_filtered)

grid_trips_df <- left_join(odbus_filtered, bus_stop_hexgrid_id, 
            by = c("ORIGIN_PT_CODE" = "BUS_STOP_N")) %>%
  select(grid_id, 
         WEEKDAY_MORNING_PEAK,
         WEEKDAY_AFTERNOON_PEAK,
         WEEKEND_MORNING_PEAK,
         WEEKEND_EVENING_PEAK)  %>%
  group_by(grid_id) %>%
  summarise(
    WEEKDAY_MORNING_TRIPS = sum(WEEKDAY_MORNING_PEAK), 
    WEEKDAY_AFTERNOON_TRIPS = sum(WEEKDAY_AFTERNOON_PEAK), 
    WEEKEND_MORNING_TRIPS = sum(WEEKEND_MORNING_PEAK), 
    WEEKEND_EVENING_TRIPS = sum(WEEKEND_EVENING_PEAK)
    )
head(grid_trips_df)
```

### 1.6.3 Combine Bus Trip Details Back Into `hexagon_sf`

-   Finally, it's time to recombine bus trip data back into `hexagon_sf`;
    -   We use `left_join` on `$grid_id` to add trip data back into hexagon_sf;
    -   We add a failsafe `mutate()` to replace any "NA" values for the columns.

```{r}
#| code-fold: true
#| code-summary: "show code"

hexagon_sf <- left_join(hexagon_sf, grid_trips_df, 
            by = 'grid_id' ) %>%
  mutate(
    WEEKDAY_MORNING_TRIPS = ifelse(is.na(WEEKDAY_MORNING_TRIPS), 0, WEEKDAY_MORNING_TRIPS),
    WEEKDAY_AFTERNOON_TRIPS = ifelse(is.na(WEEKDAY_AFTERNOON_TRIPS), 0, WEEKDAY_AFTERNOON_TRIPS),
    WEEKEND_MORNING_TRIPS = ifelse(is.na(WEEKEND_MORNING_TRIPS), 0, WEEKEND_MORNING_TRIPS),
    WEEKEND_EVENING_TRIPS = ifelse(is.na(WEEKEND_EVENING_TRIPS), 0, WEEKEND_EVENING_TRIPS),
         )

head(hexagon_sf)

```

## 1.7 Exploratory Data Analysis Of Bus Trips, Across Peak Periods, By Hexagons

-   For `ggplot`, we need data in long format, so we can use `gather()` on `grid_trips_df` from Section 1.6.2 to pivot this;
-   We then pipe this into a `geom_boxplot()` for an exploratory look:

```{r}
#| code-fold: true
#| code-summary: "show code"

gather(grid_trips_df, key = "Peak", value = "Trips", -grid_id) %>%
  ggplot( aes(x=Peak, y=Trips, fill=Peak)) +
  geom_boxplot() + 
  ggtitle("Boxplot: Trips over peak periods, 2023-Oct data") +
    xlab("") + 
    theme(
      legend.position="none"
      ) +
  coord_flip()
```

::: callout-tip
We also observe that number of trips for Weekday Morning & Weekday Afternoon seems to be larger than Weekend Morning and Weekend Evening peak trips. This is also confirmed by the figure in the next section.

This means that we will have to consider Weekday and Weekend peaks on different scales.
:::

-   This is an exceptionally ugly plot, but it shows an important point: there is some serious right skew in our dataset;
-   Clearly, there are some hexagons with exceptionally high trips compared to the rest of the hexagons But, could this be because some hexagons have up to 11 bus stops, while others have 1 or 2?
-   We do a quick scatter plot based on `$n_bus_stops` to verify this:

```{r}
#| code-fold: true
#| code-summary: "show code"
hexagon_sf %>% 
  st_drop_geometry() %>% 
  pivot_longer(cols = starts_with("WEEK"),
               names_to = "PEAK", values_to = "TRIPS") %>%
  ggplot( aes(x=TRIPS, y=n_bus_stops, color=PEAK, shape=PEAK)) +
  geom_point(size=2) +
  ggtitle("Scatterplot: Trips over peak periods by number of bus stops per hexagon, 2023-Oct data") +
    theme(legend.position=c(.85, .15),
          legend.background = element_rect(fill = "transparent"),
          legend.key.size = unit(0.5, "cm"),  
          legend.text = element_text(size = 6),
          legend.title = element_text(size = 8)
    ) 
```

-   Surprising results from our plot! If we consider those with \> 100,000 trips as outliers, most of them come from hexagons with between 4-8 bus stops;
-   There is some correlation between number of bus stops and high numbers of trips, but a stronger factor is peak time; Weekday Morning peak trips, followed by Weekday Afternoon peak trips, contribute to the largest outliers.

::: callout-tip
I note that these visualizations only scrape the surface of understanding the data. However, this is not the focus of our study; we do these quick visualizations only to provide better context for our study.
:::

# 2 Geovisualisation And Geocommunication

## 2.1 Passenger Trips By Origin At Hexagon Level

::: callout-important
## **Q:** With reference to the time intervals provided in the table below, compute the passenger trips generated by origin at the hexagon level.
:::

-   We use `datatable()` to display `hexagon_sf` as an interactive table. NB: In order to view the position of `grid_id`, we'll need to look at the next section.

```{r}
#| code-fold: true
#| code-summary: "show code"

datatable(hexagon_sf)
```

## 2.2 Visualising The Geographical Distribution Of Passenger Trips By Origin At Hexagon Level

::: callout-important
## **Q:** Display the geographical distribution of the passenger trips by using appropriate geovisualisation methods.
:::

The below code block is a little more complicated, but can be thought of as combining a `tm` object with custom `tm_leaflet()` settings:

**Creating the `tm` object:**

-   We start with `tmap_mode("view")` to create an interactive base layer of Singapore;
-   To create the "WEEKDAY_MORNING_TRIPS" layer, we perform the following:
    -   We use `tm_shape()` and `tm_borders()` to visualize the hexagons in `hexagon_sf` ;
    -   We use `tm_fill` and pass the argument `col = "WEEKDAY_MORNING_TRIPS"` to colour each hexagon based on the number of trips in the `$WEEKDAY_MORNING_TRIPS` column;
    -   Since we are largely comparing the patterns, we use `style = "jenks"` to make use of the [Jenks natural breaks classification method](https://en.wikipedia.org/wiki/Jenks_natural_breaks_optimization) to group the number of trips in different hexagons. This results in more visually distinct clusters that allow us to better spot patterns and distribution.
    -   This also means that each layer will be on a different scale; we therefore use different `palettes` so the user understand the scale breaks on each layer is different. For consistency, we also match each layer with the colourscheme in the visualisations in section 1.6.4.
    -   For each feature, we add the `group="WEEKDAY_MORNING_TRIPS"` argument for leaflet control later.
-   We repeat this for the other three layers as well.

**Creating the `tm_leaflet` controls:**

-   We use `addLayersControl()` to specify the checkbox-menu of `overlayGroups`, according to the four defined peaks. Additional arguments allow us to ensure the menu is positioned at the top-left and expanded, for visibility and ease of use.

```{r}
#| code-fold: true
#| code-summary: "show code"
tmap_mode("view")
tm <- tm_shape(hexagon_sf, group="WEEKDAY_MORNING_TRIPS") +
 tm_borders(group="WEEKDAY_MORNING_TRIPS") + 
 tm_fill(col = "WEEKDAY_MORNING_TRIPS", 
         palette = "Greens",
         style = "jenks",
         group="WEEKDAY_MORNING_TRIPS") +
  
 tm_shape(hexagon_sf, group="WEEKDAY_AFTERNOON_TRIPS") +
 tm_borders(group="WEEKDAY_AFTERNOON_TRIPS") + 
 tm_fill(col = "WEEKDAY_AFTERNOON_TRIPS", 
         palette = "Reds",
         style = "jenks",
         group="WEEKDAY_AFTERNOON_TRIPS") +
  
 tm_shape(hexagon_sf, group="WEEKEND_MORNING_TRIPS") +
 tm_borders(group="WEEKEND_MORNING_TRIPS") + 
 tm_fill(col = "WEEKEND_MORNING_TRIPS",
         palette = "Blues",
         style = "jenks",
         group="WEEKEND_MORNING_TRIPS") +
  
 tm_shape(hexagon_sf, group="WEEKEND_EVENING_TRIPS") +
 tm_borders(group="WEEKEND_EVENING_TRIPS") + 
 tm_fill(col = "WEEKEND_EVENING_TRIPS",
         palette = "Purples",
         style = "jenks",
         group="WEEKEND_EVENING_TRIPS")
  
tm %>% 
  tmap_leaflet() %>%
  hideGroup(c("WEEKDAY_AFTERNOON_TRIPS", "WEEKEND_MORNING_TRIPS", "WEEKEND_EVENING_TRIPS")) %>%
  addLayersControl(
    overlayGroups = c("WEEKDAY_MORNING_TRIPS", "WEEKDAY_AFTERNOON_TRIPS",
                      "WEEKEND_MORNING_TRIPS", "WEEKEND_EVENING_TRIPS"),
    options = layersControlOptions(collapsed = FALSE),
    position = "topleft"
  )

## Code referenced from: https://stackoverflow.com/questions/53094379/in-r-tmap-how-do-i-control-layer-visibility-in-interactive-mode
```

## 2.3 Observed Spatial Patterns

::: callout-important
## **Q:** Describe the spatial patterns revealed by the geovisualisation
:::

There are three major patterns observable from the geospatial visualization. First, we note that **a minority of hexagons are responsible for a majority of trips**. Most hexagons fall within the first quintile of trips made. However, across all peak periods, we can observe contiguous regions of deeply-coloured hexagons (by switching between layers in the above visualisation) that represent regions with high numbers of bus trips. This suggests the presence of transport hubs where commuters congregate to take bus trips during peak hours -- possibly bus interchanges, or MRT stations.

Secondly, we observe that these regions of high bus trips seem to roughly correspond to regions of residential new towns and MRT lines (see image below). This makes sense, as commuters are likely to leave their homes in the mornings or take buses from MRT stations. We also see a darker region in the WEEKDAY_AFTERNOON_TRIPS map corresponding to the Central Business District, as commuters leave their workplace back to home.

[![Image: Planning map of residential new towns and MRT lines in Singapore. Sourced from: "The developmental state in the global hegemony of neoliberalism: A new strategy for public housing in Singapore", sourced from \[Cities: The International Journal of Urban Planning and Policy, Vol 29 Issue 6, Dec 2012\](https://www.researchgate.net/publication/257097062_The_developmental_state_in_the_global_hegemony_of_neoliberalism_A_new_strategy_for_public_housing_in_Singapore)](Planning-of-new-towns-in-Singapore.png){fig-alt="Image showing planning of new towns in Singapore"}](https://www.researchgate.net/figure/Planning-of-new-towns-in-Singapore_fig1_257097062)

Finally, however, we observe that there is incomplete information. There are more trips on Weekday Afternoons compared to Weekday Mornings, and more trips on Weekdays than Weekends. This is likely due to other forms of transport (e.g. private cars, MRTs, bicycles).

::: {.callout-caution collapse="true"}
## EXPAND To See: What If We Plot The Maps On The Same Scale?

-   We can also visualize the map of all four on the same scale, for side-by-side comparison;
-   However, as has been noted, the difference in scales between Weekday (left column) and Weekend (right column) trips means that there's not much visual information value; the Weekend plot looks mostly flat.
-   We will not go into detail about the code or spatial patterns, aside from noting that **the hexagons with the highest number of trips are the same on weekdays and weekends, across morning and afternoon/evening peaks.**

```{r}
#| code-fold: true
#| code-summary: "show code"


tmap_mode("plot")

custom_breaks <- seq(0, 550000, by = 50000)

wd_morn <- tm_shape(hexagon_sf) +
  tm_fill("WEEKDAY_MORNING_TRIPS",
          style = "cont",
          palette = "-plasma",
          breaks = custom_breaks,
          title = "Trips") + 
  tm_borders() +
  tm_layout(title = "WEEKDAY_MORNING", title.size = 0.8, legend.show = FALSE)
wd_aft <- tm_shape(hexagon_sf) +
  tm_fill("WEEKDAY_AFTERNOON_TRIPS",
          style = "cont",
          palette = "-plasma",
          breaks = custom_breaks,
          title = "Trips") + 
  tm_borders() +
  tm_layout(title = "WEEKDAY_AFTERNOON", title.size = 0.8, legend.show = FALSE)
we_morn <- tm_shape(hexagon_sf) +
  tm_fill("WEEKEND_MORNING_TRIPS",
          style = "cont",
          palette = "-plasma",
          breaks = custom_breaks,
          title = "Trips") + 
  tm_borders() +
  tm_layout(title = "WEEKEND_MORNING", title.size = 0.8, legend.show = FALSE)
we_eve <- tm_shape(hexagon_sf) +
  tm_fill("WEEKEND_EVENING_TRIPS",
          style = "cont",
          palette = "-plasma",
          breaks = custom_breaks,
          title = "Trips") + 
  tm_borders() +
  tm_layout(title = "WEEKEND_EVENING", title.size = 0.8, legend.text.size = 0.5)

tmap_arrange(wd_morn, we_morn, wd_aft,  we_eve)

```
:::

# 3. Geospatial Analysis: Local Indicators of Spatial Association (LISA) Analysis

## 3.1 Visualizing Queen Contiguity (And Why It Won't Work)

-   To perform LISA analysis, we will need an `nb` matrix.
-   However, using QUEEN contiguity will not work, as there are a number of hexagons without neighbours (i.e. no adjacent/touching hexagons);
-   This is visible if we call `st_contiguity()` on our `hexagon_sf` with the argument "queen=TRUE" for QUEEN contiguity;
    -   We see "9 regions with no links", i.e. 9 hexagons will have no neighbours.

```{r}
#| code-fold: true
#| code-summary: "show code"
wm_hex <- st_contiguity(hexagon_sf, queen=TRUE)
summary(wm_hex)

```

-   We can perform a quick visualization of the hexagons with no neighbours;

-   We plot all hexagons, in green, from `hexagon_sf`, as an underlying layer;

-   On top of this layer, we plot all hexagons with no neighbours, `hexagon_sf_nonb` in red;

    -   Using the output from above, we create a list of hexagons with no neighbours, `hex_no_nb` ;
    -   We create `hexagon_sf_nonb` by adding a `$RowNumber` column using `mutate()` and using `row_number()` to label each row;
    -   Finally, we use `subset()` to select only the hexagons with no neighbours.

    ```{r}
    #| code-fold: true
    #| code-summary: "show code"
    hex_no_nb <- c(561, 726, 980, 1047, 1415, 1505, 1508, 1512, 1520)


    hexagon_sf_nonb <- hexagon_sf %>%
      mutate(RowNumber = row_number()) %>%
      subset((RowNumber  %in% hex_no_nb))

    tm_shape(hexagon_sf) + 
      tm_fill("green") + 
      tm_borders() +
      tm_shape(hexagon_sf_nonb) + 
      tm_fill("red")
    ```

-   We note that the Easternmost red outlier hexagon is likely the bus stop outside Changi Naval Base:

```{r}
#| code-fold: true
#| code-summary: "show code"
coordinates <- st_coordinates(raw_bus_stop_sf$geometry)

# Find the index of the easternmost point
easternmost_index <- which.max(coordinates[, 1])

# Extract the easternmost bus-stop
easternmost_point <- raw_bus_stop_sf[easternmost_index, ]
easternmost_point # 96439
```

## 3.2 Creating Distance-Based `nb` Neighbours Matrix

### 3.2.1 Identifying Coordinates Of centroid

-   Creating Distance-based neighbour weights matrix relies on point geometry;
-   That means, we need to calculate the centroids of each hexagon, by creating `centroid_coords`:
    -   This combines two lists, `longitude` and `latitude`, which we extract using `st_centroid` and indexing either the first or second value using `[[]]`
    -   `map_dbl()` ensures we obtain `longitude` and `latitude` as double integers

```{r}
#| code-fold: true
#| code-summary: "show code"

longitude <- map_dbl(hexagon_sf$raw_hex_grid, ~st_centroid(.x)[[1]])
latitude <- map_dbl(hexagon_sf$raw_hex_grid, ~st_centroid(.x)[[2]])
centroid_coords <- cbind(longitude, latitude)
cat("Printing first 6 rows of `centroid_coords`:\n")
head(centroid_coords)
```

### 3.2.2 Identifying The Minimum DIstance So Each Hexagon Has At Least One Neighbour

-   We use `st_knn()` with argument "k=1" to create a neighbour list, `k1_nn_obj`, such that each hexagon centroid has 1 neighbour (using the [k-nearest-neighbour algorithm](https://sfdep.josiahparry.com/reference/st_knn));
-   We use `st_nb_dists()` to create a list of distances between each hexagon centroid and its nearest neighbours;
-   We performing `unlist()` on the object to convert it into a vector of numeric values, so that we can run `summary()` to see summary statistics on it.

```{r}
#| code-fold: true
#| code-summary: "show code"

k1_nn_obj <- st_knn(centroid_coords, k = 1)
k1dists <- unlist(st_nb_dists(centroid_coords, k1_nn_obj))
summary(k1dists)
```

-   We see that the furthest distance between a hexagon and its nearest neighbour is about 2.3km away; that will be the upper limit value needed to create distance-based contiguity.

::: callout-tip
Personally, I would drop the outlier (the hexagon with the Changi Naval Base bus stop). My reasoning is thus:

-   Logically speaking, we are enmeshing regions up to more than four hexagons away; this may lea to greater-than-average lag than is necessary.

-   As a commuter, walking 2.3km to the next nearest bus-stop hexagon is impractical and a clear outlier situation.

-   In personal testing, removing Changi Naval Base drops the maximum nearest-neighbour distance to 1km, which is more reasonable for both analysis and real-life considerations, but I note this here for future work and as commentary.
:::

## 3.3 Creating A Distance-Based Weights Natrix

-   To create a neighbours list, we use `st_dist_band()` on the `centroid_coords` of our hexagons, and pass "upper = 2300" to set the upper limit for the neighbour threshold (rounding up to 2.3km from our previous result);
-   We then use `st_weights()` to create the weights matrix; by default, the function creates row-standardised weights, but we could also have specified "style ="W"" for completeness.

```{r}
#| code-fold: true
#| code-summary: "show code"
hex_2km_nb <- st_dist_band(centroid_coords, lower = 0, upper = 2300)
hex_2km_wt <- st_weights(hex_2km_nb)

head(hex_2km_wt, 5)
  
```

## 3.4 Calculate Local Moran's I

-   For each peak period, we perform a calculation using `local_moran()`, feeding in the `nb` neighbours list and `wt` weight matrix from the previous section;
    -   We use `select()` to isolate the columns that we need, specifically the Local Moran's I statistics `$ii`, the p-value `$p_ii`, and the LISA `$mean`(here, we use mean over median or pysal columns);
    -   For ease of use later, we rename these columns with `rename()` ;
    -   We repeat for each of the four peak periods.
-   Finally, we use a `cbind()` to append the columns back to our `hexagon_sf` :

```{r}
#| code-fold: true
#| code-summary: "show code"


localMI_day_morn <- local_moran(hexagon_sf$WEEKDAY_MORNING_TRIPS, hex_2km_nb, hex_2km_wt) %>%
  select(ii, p_ii, mean) %>%
  rename(DAY_MORN_LMI = ii,
         DAY_MORN_p = p_ii,
         DAY_MORN_mean = mean)
localMI_day_aft <- local_moran(hexagon_sf$WEEKDAY_AFTERNOON_TRIPS, hex_2km_nb, hex_2km_wt) %>%
  select(ii, p_ii, mean) %>%
  rename(DAY_AFT_LMI = ii,
         DAY_AFT_p = p_ii,
         DAY_AFT_mean = mean)
localMI_end_morn <- local_moran(hexagon_sf$WEEKEND_MORNING_TRIPS , hex_2km_nb, hex_2km_wt) %>%
  select(ii, p_ii, mean) %>%
  rename(END_MORN_LMI = ii,
         END_MORN_p = p_ii,
         END_MORN_mean = mean)
localMI_end_eve <- local_moran(hexagon_sf$WEEKEND_EVENING_TRIPS, hex_2km_nb, hex_2km_wt) %>%
  select(ii, p_ii, mean) %>%
  rename(END_EVE_LMI = ii,
         END_EVE_p = p_ii,
         END_EVE_mean = mean)

hexagon_sf_local_moran <- cbind(hexagon_sf,localMI_day_morn, localMI_day_aft, localMI_end_morn, localMI_end_eve)
```

::: callout-important
## **Q:** Compute LISA of the passengers trips generate by origin at hexagon level.
:::

```{r}
#| code-fold: true
#| code-summary: "show code"

datatable(hexagon_sf_local_moran)
```

## 3.5 Removing Statistically Non-Significant Values Before Plotting the LISA Map

-   To plot the LISA map, we will use the peak-specific `$mean` column; this uses the mean of values to classify regions into either outliers (High-Low or Low-High) or clusters (High-High or Low-Low).
-   However, we also only want to see categories if they are statistically significant, i.e. if the `$p_ii` value is \>= 0.05, we should coerce the LISA category to "Insignificant", so we need to do some cleaning;
-   Once again, we use our trusty friend `mutate()` to modify existing columns;
    -   We pass the column `$DAY_MORN_mean` and define a `case_when()` statement that replaces the value with "Insignificant" if the `$DAY_MORN_p` value is greater than 0.05 (i.e. p-value is higher than 0.05);
    -   We repeat this for the other peak periods:

```{r}
#| code-fold: true
#| code-summary: "show code"


df <- hexagon_sf_local_moran %>% 
  mutate(
    DAY_MORN_mean = case_when(
                      DAY_MORN_p  >= 0.05 ~ "Insignificant",
                      TRUE               ~ DAY_MORN_mean
                    ),
    DAY_AFT_mean = case_when(
                      DAY_AFT_p   >= 0.05  ~ "Insignificant",
                      TRUE               ~ DAY_AFT_mean
                    ),
    END_MORN_mean  = case_when(
                      END_MORN_p  >= 0.05 ~ "Insignificant",
                      TRUE               ~ END_MORN_mean 
                    ),
    END_EVE_mean = case_when(
                      END_EVE_p   >= 0.05  ~ "Insignificant",
                      TRUE               ~ END_EVE_mean
                    )
    )
datatable(head(df))
```

## 3.6 Plotting The LISA Map

-   Now, we perform a similar plot to the visualisation above in Section 2.2;
    -   We use `tmap_leaflet()` and `addLayersControl()` to allow for toggling between the various peak periods;
    -   We pass `group=` argument for each `tmap` function to group them into layer groups;
    -   We use `tm_shape()` to plot the hexagons, `tm_borders()` to draw the borders and `tm_fill()` to colour them in.
-   The difference here is we define a custom palette, `colors`, sourced from Prof Kam's book.

```{r}
#| code-fold: true
#| code-summary: "show code"
tmap_mode("view")

colors <- c("#2c7bb6", "#abd9e9", "#ffffff", "#fdae61", "#d7191c")
# Custom colors palette sourced from: https://r4gdsa.netlify.app/chap10#plotting-lisa-map

tm2 <- tm_shape(df, group="DAY_MORN") +
 tm_borders(group="DAY_MORN") + 
 tm_fill(col = "DAY_MORN_mean", 
         alpha = 0.8,
         palette = colors,
         group="DAY_MORN") + 
  
      tm_shape(df, group="DAY_AFT") +
 tm_borders(group="DAY_AFT") + 
 tm_fill(col = "DAY_AFT_mean", 
         alpha = 0.8,
         palette = colors,
         group="DAY_AFT") +

      tm_shape(df, group="END_MORN") +
 tm_borders(group="END_MORN") + 
 tm_fill(col = "END_MORN_mean", 
         alpha = 0.8,
         palette = colors,
         group="END_MORN") + 
  
      tm_shape(df, group="END_EVE") +
 tm_borders(group="END_EVE") + 
 tm_fill(col = "END_EVE_mean", 
         alpha = 0.8,
         palette = colors,
         group="END_EVE") 
   
tm2 %>% 
  tmap_leaflet() %>%
  hideGroup(c("DAY_AFT", "END_MORN", "END_EVE")) %>%
  addLayersControl(
    overlayGroups = c("DAY_MORN", "DAY_AFT",
                      "END_MORN", "END_EVE"),
    options = layersControlOptions(collapsed = FALSE),
    position = "topleft"
  )   
```

::: callout-important
## **Q:** With reference to the analysis results, draw statistical conclusions (not more than 200 words per visual).
:::

-   The Westmost region (Tuas, Boon Lay) areas are hugely underserved by buses, across all peak periods.
-   Travel patterns on Weekends are more homogeneous (regions are similar across Weekend Morning and Weekend Evening peaks); however, there is a stark difference between Weekday Morning and Weekday Afternoon peaks. This likely reflects people leaving homes to commute / people leaving places of work, study, or commerce to return home.
-   Across all peak periods, there are consistent areas of high demand; these include:
    -   A region in the West, stretching across Jurong West-Jurong East-Bukit Batok-Choa Chu Kang neighbourhoods;
    -   A cluster in the North, around the Woodlands region;
    -   A cluster in the East, around the Bedok-Tampines region;
-   There are also regions of peak-specific demand;
    -   In the Northeast, there is a region of High-High peak around the Sengkang-Punggol region during weekday and weeknd mornings, that are not there during weekday evenings/weekend afternoons. This may represent residents leaving homes to commute to school, work, or recreation, but returning outside peak hours or not using buses.
    -   There is Central region around the Chinatown-CBD-Bugis-Rochor stretch that shows High-High and Low-High peaks of traffic demand on all peaks aside from Weekday Mornings; it is likely that these are not residential areas, but places of commerce, shopping, or work.
-   To reiterate previous conclusions, most hexagons do not show significant trends and may be disregarded in favour of analysis of major hotspots; and there is incomplete information in our dataset.

### 3.6.1 Plotting The LISA Maps Side-By-Side

-   For better illustration of the regions and conclusions, we can plot the maps altogether, using similar methods as before.

```{r}
#| code-fold: true
#| code-summary: "show code"

tmap_mode("plot")
wd_morn_lisa <- tm_shape(df) +
  tm_fill("DAY_MORN_mean",
          palette = colors) + 
  tm_borders() +
  tm_layout(title = "WEEKDAY_MORNING LISA", title.size = 0.8, legend.show = FALSE)

wd_aft_lisa <- tm_shape(df) +
  tm_fill("DAY_AFT_mean",
          palette = colors) + 
  tm_borders() +
  tm_layout(title = "WEEKDAY_AFTERNOON LISA", title.size = 0.8, legend.show = FALSE)

we_morn_lisa <- tm_shape(df) +
  tm_fill("END_MORN_mean",
          palette = colors) + 
  tm_borders() +
  tm_layout(title = "WEEKEND_MORNING LISA", title.size = 0.8, legend.show = FALSE)

we_eve_lisa <- tm_shape(df) +
  tm_fill("END_EVE_mean",
          palette = colors,
          title="LISA") + 
  tm_borders() +
  tm_layout(title = "WEEKEND_EVENING LISA", title.size = 0.8, 
            legend.text.size = 0.4, legend.position = c("right", "bottom"))

tmap_arrange(wd_morn_lisa, we_morn_lisa, wd_aft_lisa,  we_eve_lisa)

```

## 3.7 Plotting the Local Moran's I statistics

-   For completeness, we have also plotted the map of Local Moran's I statistic for brief discussion;
-   We simply wish to observe that there appear to be persistent hotspot hexagons with outlier values (i.e. very positive or very negative) that persist across all peaks.

```{r}
#| code-fold: true
#| code-summary: "show code"

tmap_mode("plot")
wd_morn_lmi <- tm_shape(df) +
  tm_fill("DAY_MORN_LMI",
          palette = "RdBu",
          midpoint = 0,
          title="Local Moran's I") + 
  tm_borders() +
  tm_layout(title = "WEEKDAY_MORNING", title.size = 0.8, 
            legend.text.size = 0.4, legend.position = c("left", "top"),
            title.position = c("right", "top"))

wd_aft_lmi <- tm_shape(df) +
  tm_fill("DAY_AFT_LMI",
          palette = "RdBu",
          midpoint = 0) + 
  tm_borders() +
  tm_layout(title = "WEEKDAY_AFTERNOON", title.size = 0.8, legend.show = FALSE,
            title.position = c("right", "top"))

we_morn_lmi <- tm_shape(df) +
  tm_fill("END_MORN_LMI",
          palette = "RdBu",
          midpoint = 0) + 
  tm_borders() +
  tm_layout(title = "WEEKEND_MORNING", title.size = 0.8, legend.show = FALSE,
            title.position = c("right", "top"))

we_eve_lmi <- tm_shape(df) +
  tm_fill("END_EVE_LMI",
          palette = "RdBu",
          midpoint = 0) + 
  tm_borders() +
  tm_layout(title = "WEEKEND_EVENING", title.size = 0.8, legend.show = FALSE,
            title.position = c("right", "top"))

tmap_arrange(wd_morn_lmi, we_morn_lmi, wd_aft_lmi,  we_eve_lmi)

```

# 4 Summary Of Observations & Future Work

-   From the geospatial visualisations, we have identified regions of high traffic based on bus stop origin trip data.
    -   Considering only passenger trips (Section 2), there are certain regions where many trips originate. These are likely areas of transit from other transporation types, e.g. MRT Stations or taxi stands.
    -   Considering LISA (Section 3), there appear to be different traffic demands at different peak times. In particular, we saw a difference in traffic demand regions during Weekday Morning and Weekday Afternoon peaks.
-   For further work, we can consider the following:
    -   **Integrating data from other sources of public transport** including MRT data. This will provide a more holistic picture.
    -   **Incorporate Emerging Hotspot Analysis** across the months and across the hours of the day, in order to get a clearer picture of travel patterns
    -   **Mapping regions of residence/commerce/study** in order to provide context for major areas of high traffic.
